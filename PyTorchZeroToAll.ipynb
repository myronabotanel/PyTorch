{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdb62d32-e15b-4952-b2e1-1fe613a9c7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= 0.0\n",
      "\t 1.0 2.0 0.0 4.0\n",
      "\t 2.0 4.0 0.0 16.0\n",
      "\t 3.0 6.0 0.0 36.0\n",
      "MSE= 18.666666666666668\n",
      "w= 0.1\n",
      "\t 1.0 2.0 0.1 3.61\n",
      "\t 2.0 4.0 0.2 14.44\n",
      "\t 3.0 6.0 0.30000000000000004 32.49\n",
      "MSE= 16.846666666666668\n",
      "w= 0.2\n",
      "\t 1.0 2.0 0.2 3.24\n",
      "\t 2.0 4.0 0.4 12.96\n",
      "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
      "MSE= 15.120000000000003\n",
      "w= 0.30000000000000004\n",
      "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
      "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
      "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
      "MSE= 13.486666666666665\n",
      "w= 0.4\n",
      "\t 1.0 2.0 0.4 2.5600000000000005\n",
      "\t 2.0 4.0 0.8 10.240000000000002\n",
      "\t 3.0 6.0 1.2000000000000002 23.04\n",
      "MSE= 11.946666666666667\n",
      "w= 0.5\n",
      "\t 1.0 2.0 0.5 2.25\n",
      "\t 2.0 4.0 1.0 9.0\n",
      "\t 3.0 6.0 1.5 20.25\n",
      "MSE= 10.5\n",
      "w= 0.6000000000000001\n",
      "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
      "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
      "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
      "MSE= 9.146666666666663\n",
      "w= 0.7000000000000001\n",
      "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
      "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
      "\t 3.0 6.0 2.1 15.209999999999999\n",
      "MSE= 7.886666666666666\n",
      "w= 0.8\n",
      "\t 1.0 2.0 0.8 1.44\n",
      "\t 2.0 4.0 1.6 5.76\n",
      "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
      "MSE= 6.719999999999999\n",
      "w= 0.9\n",
      "\t 1.0 2.0 0.9 1.2100000000000002\n",
      "\t 2.0 4.0 1.8 4.840000000000001\n",
      "\t 3.0 6.0 2.7 10.889999999999999\n",
      "MSE= 5.646666666666666\n",
      "w= 1.0\n",
      "\t 1.0 2.0 1.0 1.0\n",
      "\t 2.0 4.0 2.0 4.0\n",
      "\t 3.0 6.0 3.0 9.0\n",
      "MSE= 4.666666666666667\n",
      "w= 1.1\n",
      "\t 1.0 2.0 1.1 0.8099999999999998\n",
      "\t 2.0 4.0 2.2 3.2399999999999993\n",
      "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
      "MSE= 3.779999999999999\n",
      "w= 1.2000000000000002\n",
      "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
      "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
      "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
      "MSE= 2.986666666666665\n",
      "w= 1.3\n",
      "\t 1.0 2.0 1.3 0.48999999999999994\n",
      "\t 2.0 4.0 2.6 1.9599999999999997\n",
      "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
      "MSE= 2.2866666666666657\n",
      "w= 1.4000000000000001\n",
      "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
      "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
      "\t 3.0 6.0 4.2 3.2399999999999993\n",
      "MSE= 1.6799999999999995\n",
      "w= 1.5\n",
      "\t 1.0 2.0 1.5 0.25\n",
      "\t 2.0 4.0 3.0 1.0\n",
      "\t 3.0 6.0 4.5 2.25\n",
      "MSE= 1.1666666666666667\n",
      "w= 1.6\n",
      "\t 1.0 2.0 1.6 0.15999999999999992\n",
      "\t 2.0 4.0 3.2 0.6399999999999997\n",
      "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
      "MSE= 0.746666666666666\n",
      "w= 1.7000000000000002\n",
      "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
      "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
      "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
      "MSE= 0.4199999999999995\n",
      "w= 1.8\n",
      "\t 1.0 2.0 1.8 0.03999999999999998\n",
      "\t 2.0 4.0 3.6 0.15999999999999992\n",
      "\t 3.0 6.0 5.4 0.3599999999999996\n",
      "MSE= 0.1866666666666665\n",
      "w= 1.9000000000000001\n",
      "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
      "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
      "\t 3.0 6.0 5.7 0.0899999999999999\n",
      "MSE= 0.046666666666666586\n",
      "w= 2.0\n",
      "\t 1.0 2.0 2.0 0.0\n",
      "\t 2.0 4.0 4.0 0.0\n",
      "\t 3.0 6.0 6.0 0.0\n",
      "MSE= 0.0\n",
      "w= 2.1\n",
      "\t 1.0 2.0 2.1 0.010000000000000018\n",
      "\t 2.0 4.0 4.2 0.04000000000000007\n",
      "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
      "MSE= 0.046666666666666835\n",
      "w= 2.2\n",
      "\t 1.0 2.0 2.2 0.04000000000000007\n",
      "\t 2.0 4.0 4.4 0.16000000000000028\n",
      "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
      "MSE= 0.18666666666666698\n",
      "w= 2.3000000000000003\n",
      "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
      "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
      "\t 3.0 6.0 6.9 0.8100000000000006\n",
      "MSE= 0.42000000000000054\n",
      "w= 2.4000000000000004\n",
      "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
      "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
      "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
      "MSE= 0.7466666666666679\n",
      "w= 2.5\n",
      "\t 1.0 2.0 2.5 0.25\n",
      "\t 2.0 4.0 5.0 1.0\n",
      "\t 3.0 6.0 7.5 2.25\n",
      "MSE= 1.1666666666666667\n",
      "w= 2.6\n",
      "\t 1.0 2.0 2.6 0.3600000000000001\n",
      "\t 2.0 4.0 5.2 1.4400000000000004\n",
      "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
      "MSE= 1.6800000000000008\n",
      "w= 2.7\n",
      "\t 1.0 2.0 2.7 0.49000000000000027\n",
      "\t 2.0 4.0 5.4 1.960000000000001\n",
      "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
      "MSE= 2.2866666666666693\n",
      "w= 2.8000000000000003\n",
      "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
      "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
      "\t 3.0 6.0 8.4 5.760000000000002\n",
      "MSE= 2.986666666666668\n",
      "w= 2.9000000000000004\n",
      "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
      "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
      "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
      "MSE= 3.780000000000003\n",
      "w= 3.0\n",
      "\t 1.0 2.0 3.0 1.0\n",
      "\t 2.0 4.0 6.0 4.0\n",
      "\t 3.0 6.0 9.0 9.0\n",
      "MSE= 4.666666666666667\n",
      "w= 3.1\n",
      "\t 1.0 2.0 3.1 1.2100000000000002\n",
      "\t 2.0 4.0 6.2 4.840000000000001\n",
      "\t 3.0 6.0 9.3 10.890000000000004\n",
      "MSE= 5.646666666666668\n",
      "w= 3.2\n",
      "\t 1.0 2.0 3.2 1.4400000000000004\n",
      "\t 2.0 4.0 6.4 5.760000000000002\n",
      "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
      "MSE= 6.720000000000003\n",
      "w= 3.3000000000000003\n",
      "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
      "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
      "\t 3.0 6.0 9.9 15.210000000000003\n",
      "MSE= 7.886666666666668\n",
      "w= 3.4000000000000004\n",
      "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
      "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
      "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
      "MSE= 9.14666666666667\n",
      "w= 3.5\n",
      "\t 1.0 2.0 3.5 2.25\n",
      "\t 2.0 4.0 7.0 9.0\n",
      "\t 3.0 6.0 10.5 20.25\n",
      "MSE= 10.5\n",
      "w= 3.6\n",
      "\t 1.0 2.0 3.6 2.5600000000000005\n",
      "\t 2.0 4.0 7.2 10.240000000000002\n",
      "\t 3.0 6.0 10.8 23.040000000000006\n",
      "MSE= 11.94666666666667\n",
      "w= 3.7\n",
      "\t 1.0 2.0 3.7 2.8900000000000006\n",
      "\t 2.0 4.0 7.4 11.560000000000002\n",
      "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
      "MSE= 13.486666666666673\n",
      "w= 3.8000000000000003\n",
      "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
      "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
      "\t 3.0 6.0 11.4 29.160000000000004\n",
      "MSE= 15.120000000000005\n",
      "w= 3.9000000000000004\n",
      "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
      "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
      "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
      "MSE= 16.84666666666667\n",
      "w= 4.0\n",
      "\t 1.0 2.0 4.0 4.0\n",
      "\t 2.0 4.0 8.0 16.0\n",
      "\t 3.0 6.0 12.0 36.0\n",
      "MSE= 18.666666666666668\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUMVJREFUeJzt3Qd4VFXaB/B/MmkkpBDSCx0SWhIIXWmCICICKioWsJfV/XRZG7trdxd7WWGxIhYUQSmCClKkg5TQe0JIgVQgvU/me86ZzEgwiSHttv/vea65M5mJ72WSue+ce877OlgsFguIiIiIDMRR6QCIiIiIWhoTICIiIjIcJkBERERkOEyAiIiIyHCYABEREZHhMAEiIiIiw2ECRERERIbjpHQAalRZWYmzZ8/C09MTDg4OSodDRERE9SBKG+bn5yMkJASOjnWP8TABqoFIfsLDw5UOg4iIiBogJSUFYWFhdT6GCVANxMiP7R/Qy8tL6XCIiIioHvLy8uQAhu08XhcmQDWwXfYSyQ8TICIiIm2pz/QVToImIiIiw2ECRERERIbDBIiIiIgMhwkQERERGQ4TICIiIjIcJkBERERkOEyAiIiIyHCYABEREZHhMAEiIiIiw2ECRERERIbDBIiIiIgMhwkQERERGQ4ToBYWn1mAtNxipcMgIiJSRFZ+KY6l50FpTIBa0Msrj2D02xvxxfYkpUMhIiJSxKLdKbjm3c145vsDUBIToBbUt10b+XXF/rOwWCxKh0NERNTixDnw4nOiUpgAtaCrIgPg7mJC6oVi7EvJUTocIiKiFnUiIx/H0vPhbHLA2J5BUBIToBbUysWEq3sEyv0V+9OUDoeIiKhFrawa/RnezR/e7s5QEhOgFjYhKkR+XXngLMyVvAxGRETGYLFYsOKA9cP/hGjruVBJTIBa2NBufvByc0Jmfil2nT6vdDhEREQt4vDZPCRmF8LN2RGju1uvhiiJCVALc3UyYVyvYLn/Q9VQIBERkd79UHXOG9U9EB6uTkqHwwRICbahv58PpqHcXKl0OERERM2qstJin/9jmwqiNCZAChjUyRd+rV1woagcW+OzlQ6HiIioWcUlX8DZ3BK0dnXCiAh/qAETIAU4mRxxbW/rZTCuBiMiIqPU/hnTMxBuziaoARMghS+D/XI4HSXlZqXDISIiahYV5kr8eFA9q79smAApJLZdGwR7uyG/tAIbT2QpHQ4REVGz+C3xPLILyuDj7owru/hBLZgAKcTR0QHXRdkug3E1GBER6dOKqnOcWAHtbFJP2qGeSAzINhS47mgmisoqlA6HiIioSZVVVOLnQ+lyf0K09UO/WjABUlDvUG+0b+uO4nIz1h7NVDocIiKiJrUlPgu5xeXw93TFwI5toSZMgBTk4OBgr4fAy2BERKQ3K6pWOo/vHQyTowPUhAmQSi6DbTxuzZKJiIj0oKTcLFc6q231lw0TIIVFBHmiW2BrlJkrsbrqF4WIiEjr1h/LRGGZGaE+rdC3nQ/URtEEaNOmTZgwYQJCQkLk5aBly5ZV+764r6btjTfeqPVnvvDCC394fGRkJNSMl8GIiEhvVlSd066LDpbnYrVRNAEqLCxEdHQ05syZU+P309LSqm3z5s2T/4g33nhjnT+3Z8+e1Z63ZcsWqJltaHBbwjlkF5QqHQ4REVGj5JeUyxEg4XoVXv4SFG3HOm7cOLnVJigoqNrt5cuXY+TIkejUqVOdP9fJyekPz1WzDn4eiArzxoHUXLlc8M5B7ZUOiYiIqMHWHs1AaUUlOvl7oEewF9RIM3OAMjIy8OOPP+Lee+/908eePHlSXlYTidLtt9+O5OTkOh9fWlqKvLy8altL42UwIiLS2+qvCVHWKS5qpJkE6PPPP4enpyduuOGGOh83cOBAzJ8/H6tWrcLcuXORmJiIoUOHIj8/v9bnzJo1C97e3vYtPDwcLW18VVXoXafPIy23uMX//0RERE0hp6gMm6paPKmt+KEmEyAx/0eM5ri5udX5OHFJbcqUKYiKisLYsWPx008/IScnB4sWLar1OTNnzkRubq59S0lJQUsL8WmF/h3awGIBfjzADvFERKRNqw6lo6LSgu7BXugS4Am10kQCtHnzZhw/fhz33XffZT/Xx8cH3bp1Q3x8fK2PcXV1hZeXV7VNycnQK5gAERGRRq04cFb1oz+aSYA+/fRTxMbGyhVjl6ugoAAJCQkIDlb3C2FrFCcKZe5PyUHyuSKlwyEiIrosmfkl2J5wrtrcVrVSNAESycm+ffvkJoj5OmL/4knLYkLy4sWLax39GTVqFGbPnm2//cQTT2Djxo04ffo0tm3bhsmTJ8NkMmHq1KlQO9ErZUhnv2oZNBERkVb8fDAdlRYgJtwH4b7uUDNFE6Ddu3ejT58+chNmzJgh95977jn7YxYuXAiLxVJrAiNGd7Kzs+23U1NT5WMjIiJw8803o23bttixYwf8/f2hBbYhQ64GIyIirVlRde5SY+uLSzlYRHZB1YhRJ7EaTEyIbun5QGL2fP9/r0W52YI1fxuGroHqnUBGRERkcyanGFe8uh5i1fv2Z0YhyLvuRUtKn781MQfISHzcXTCsq3W0iqNARESkFSurzlkDOvgqkvxcLiZAKnTxajAO0BERkbZWf4VAC5gAqdDVPQLh5uyIxOxCHD7b8lWpiYiILseprAIcOpMHk6MDru2t/lXXAhMgFfJwdcKoyEC5z8tgRESkdiur6tdd2cUPvh4u0AImQBpYDVYp1hQSERGpkMViwQ8aWv1lwwRIpUZEBMDT1Qlnc0vwW+J5pcMhIiKqkbj0FZ9ZAFcnR4zpab16oQVMgFTKzdlkv466dG+q0uEQERHVaEnVOWp0j0B4uTlDK5gAqdjkvqH2ypol5WalwyEiIqqmwlxpn6t6Qx/rOUsrmACpmKilEOrTCvmlFVhzJEPpcIiIiKrZfDIb2QVlaOvhgmHdtNFxwYYJkIo5OjpgUh/rhLJle88oHQ4REVE1S6vOTWLys7NJWymFtqI1oMl9wuTXjSeycK6gVOlwiIiIpILSCvxyJF3uT9bY5S+BCZDKdQlojagwb1RUWlgTiIiIVOPng2koKa9EJ38PeZ7SGiZAGmDLrG1DjUREREpbWnVOEpOfHUQHVI1hAqQB4tqqKC++PzUXCVkFSodDREQGl5ZbjO2nzsn9iTHau/wlMAHSAL/WrhheNbt+aRxHgYiISFnL9p6F6NUtViuH+7pDi5gAacSkqstgy/adYWsMIiJStPXF0qrih7Z6dVrEBEgjxvQIRGtXJ6ReKMbupAtKh0NERAZ1JC0PJzIK4OLkqJnO7zVhAqSh1hjjegXJfbbGICIipSytmooxunsAvFtpp/XFpZgAaYhtqHHlAbH0kK0xiIio5VtfLK8qyWKrU6dVTIA0ZFDHtgjxdkN+SQXWH8tUOhwiIjKYrQnnkJVfijbuzvbFOVrFBEhjrTEmVk2GXsLVYERE1MKWxlmnYFwXFSLnAGmZtqM3IFu33Q3HM3G+sEzpcIiIyCAKSyuw+nCG5ld/2TAB0piugZ7oFeolW2P8eICtMYiIqGWsPpyO4nIzOvp5oE+4D7SOCZAGTaqqurmErTGIiKiFW19MitFm64tLMQHSoOtjQuDoAOxNzkFidqHS4RARkc5l5JVga3y2Zju/14QJkAYFeLphaNeq1hgcBSIioma2XHQhsAD92rdBu7babH1xKSZAGnVD1QS0ZXvPyLLkREREzWVJ1cpjW1smPWACpFFjegTBw8WE5PNF2MPWGERE1EyOpuXhWHo+XEyOuC5Ku60vLsUESKNauZhwTS/rLyIvgxERUXNZVnWOGRnpDx93F+gFEyAdXAYTrTFKK9gag4iImpa50oJl+87oovXFpZgAadigTm0R5OWG3OJy/HosS+lwiIhIZ7YnnENGXqlseipGgPSECZCGmURrjJgQuc8O8URE1NSWVJ1bxNwfVycT9IQJkMbZypGL5qg5RWyNQURETaOorAKrDqVXm3KhJ0yANC4yyAvdg71QbrbIuUBERERN4ZfDGSgqM6Odrzv6tmsDvVE0Adq0aRMmTJiAkJAQWVZ72bJl1b5/1113yfsv3q655po//blz5sxBhw4d4ObmhoEDB2Lnzp0wQoNUrgYjIqKmssTW+qKPPlpfqCoBKiwsRHR0tExYaiMSnrS0NPv2zTff1Pkzv/32W8yYMQPPP/884uLi5M8fO3YsMjMzoVcTq1pjiHpAp9kag4iIGikzrwRbTmbpqvWFqhKgcePG4ZVXXsHkyZNrfYyrqyuCgoLsW5s2dQ/Dvf3227j//vtx9913o0ePHvjggw/g7u6OefPmQa8CvH5vjfHdHk6GJiKixvk+ztr6om87H9n9XY9UPwdow4YNCAgIQEREBB5++GGcO3eu1seWlZVhz549GD16tP0+R0dHeXv79u21Pq+0tBR5eXnVNq25uV+4PQESdRuIiIgawmKxYPHuFLl/S3/ruUWPVJ0AictfX3zxBdatW4fXXnsNGzdulKNGZnPNRf+ys7Pl9wIDA6vdL26np1tnstdk1qxZ8Pb2tm/h4dp7wUf3CEAbd2ek55Vgc9WwJRER0eXak3QBp7IL4e5iwvgoa6kVPVJ1AnTrrbfi+uuvR+/evTFp0iSsXLkSu3btkqNCTWnmzJnIzc21bykp1sxXS0R9BluTusW7eRmMiIgaZlHV6M/43sFo7eoEvVJ1AnSpTp06wc/PD/Hx8TV+X3zPZDIhIyOj2v3itpg/VNc8Iy8vr2qbFk2JtY5c/XIkHecLWROIiIguT2Fphb2kys06vvyluQQoNTVVzgEKDq65G62LiwtiY2PlJTObyspKeXvw4MHQux4hXugd6i1rAi2v6t1CRERUXz8eTJO1f8TE537t9Vf7RzUJUEFBAfbt2yc3ITExUe4nJyfL7z355JPYsWMHTp8+LZOYiRMnokuXLnJZu82oUaMwe/Zs+22xBP7jjz/G559/jqNHj8qJ02K5vVgVZgQ397M2q/t2V4qcyEZERFRfi3ZZL39N6Remy9o/F1P04t7u3bsxcuTIasmLMH36dMydOxcHDhyQiUxOTo4sljhmzBi8/PLL8pKVTUJCgpz8bHPLLbcgKysLzz33nJz4HBMTg1WrVv1hYrReXR8dipd/PIpj6fk4dCYPvcO8lQ6JiIg0ICGrALuTLsi6cjf21Vfn95o4WDhM8AdiGbxYDSYmRGtxPtD/fbMXP+w/izsHtcfLk3opHQ4REWnAqz8fwwcbE3BVZADm3dUfej9/a2oOEF1eTSAxD6ikvOaSAURERDYV5kp8H5da7Ryid0yAdGhI57YI9WmFvJIKrD5ce/0jIiIiYeOJLGTll6Kth4scATICJkA65OjoICewCawJRERE9a39M7lPKFycjJEaGOMoDeimWDGDH9gSn42U80VKh0NERCqVlV+KdUetDcOnGOTyl8AESKfC2rjjis5+cp8NUomIqDbL9p5BRaUF0eE+iAjyhFEwAdIx22UwkQBVskEqERFdwmKx2C9/2erIGQUTIB0b2zMIXm5OOJNTjG0J55QOh4iIVGZfSg5OZhbAzdkRE6L12/i0JkyAdMzN2YSJMdYGqbYMn4iIyGZR1UKZa3sFw8vNGUbCBEjnbPUcVh1OR25RudLhEBGRShSXmbFi/1nDTX62YQKkc71CvRAZ5Imyikr8sJ8NUomIyOrnQ2koKK1AO193DOzoC6NhAqRzopndLf2tmf23vAxGRERVvrU1Po0Nk/XjjIYJkAFMigmFi8lRNkc9fDZX6XCIiEhhp7ML8VvieVkv7sZYY63+smECZABtPFxwdY9Auc/K0ERE9F1VfbhhXf0R4tMKRsQEyGA1gZbtO4PSCjZIJSIyKnOlxZ4AGaXxaU2YABnE0K7+CPJyQ05ROdYesZY8JyIi49l8MgvpeSXwcXfG6B7GaHxaEyZABmFydJD9wQTWBCIiMi7bVIhJMaFwdTLBqJgAGYgtAdp0Mgtnc4qVDoeIiFrY+cIy/HIkHUa//CUwATKQDn4estaDxQJ8zwapRESGbHxabrbIGnE9QrxgZEyADMZWE2gxG6QSERm28ektBh/9EZgAGcy4XsFo7eqE5PNF2JHIBqlEREYhasEdS8+Hi5Mjro+29ok0MiZABtPKxWTv+LtwJydDExEZxdc7k+XXa3oGwdvdWI1Pa8IEyIBuH9jO3gcmu6BU6XCIiKiZ5ZeUY/m+M9XOAUbHBMiAeoV6IzrcR06EY2VoIiJjTH4uKjOjS0BrDDBg49OaMAEyKNsngK93JnEyNBGRzic/L/gt2f7eL5pkExMgw5oQFQJPNyeknC/G5vhspcMhIqJmEpd8QU5+dnN2xA19jNn4tCZMgAw8GfrGvtY/hAU7kpQOh4iImsmCHcn2D76c/Pw7JkAGZrsMtu5YJtJyWRmaiEhvLhSWYeXBNLl/+6D2SoejKkyADKxroKecDCc6A3+7i0viiYj05vu4VJRVVKJniBeiw7yVDkdVmAAZnG0USNQEqjBXKh0OERE1y+Tn9pz8fAkmQAZ3Ta8gtPVwQXpeCdYfy1Q6HCIiaiLbE84hMbtQVv+fGGMtgEu/YwJkcK5OJkyp6gnzVdUnBSIi0r6vfrMucJncJxQerk5Kh6M6TIAItw2wXgbbdCILyeeKlA6HiIgaKTOvBL8czpD7t7Hyc42YABHatXXHsG7+1XrFEBGRdomu7xWVFsS2b4PuwV5Kh6NKTICo2mToxbtTUFphVjocIiJqILGy95uqZtfs+6XSBGjTpk2YMGECQkJC5Oz0ZcuW2b9XXl6Op59+Gr1794aHh4d8zLRp03D27Nk6f+YLL7wgf9bFW2RkZAscjbaNigxAoJcrzhWWYXXVsCkREWnPxhOZOJNTDB93Z1zbO1jpcFRL0QSosLAQ0dHRmDNnzh++V1RUhLi4ODz77LPy65IlS3D8+HFcf/31f/pze/bsibS0NPu2ZcuWZjoC/XAyOeLW/tZPCqwMTUSk/crPN/UNg5uzSelwVEvRaeHjxo2TW028vb2xZs2aavfNnj0bAwYMQHJyMtq1q31Yz8nJCUFBQU0er97dOiAcs3+Nx2+J5xGfmY8uAZ5Kh0RERJch9UIR1h+3ljTh5GcdzQHKzc2Vl7R8fHzqfNzJkyflJbNOnTrh9ttvlwlTXUpLS5GXl1dtM6Jg71byUphgK55FRETaIar6WyzAFV3aopN/a6XDUTXNJEAlJSVyTtDUqVPh5VX7jPaBAwdi/vz5WLVqFebOnYvExEQMHToU+fn5tT5n1qxZcsTJtoWHW+viGJGtV8z3e1JRXMbJ0EREWlFursTCqrZGovIz6SABEhOib775ZlnWWyQ1dRGX1KZMmYKoqCiMHTsWP/30E3JycrBo0aJanzNz5kw5umTbUlKM2xdraBc/hPu2Ql5JBVYcqHvCORERqceaIxnIyi+Fv6crru4RqHQ4queoleQnKSlJzgmqa/SnJuJyWbdu3RAfH1/rY1xdXeXPvXgzKkdHB9w2wPrJgZfBiIi0Y0FV5edb+oXD2aT607viHLWQ/Ig5PWvXrkXbtm0v+2cUFBQgISEBwcFcClhfU/qFwdnkgP0pOTh0JlfpcIiI6E+cyirA1vhzEP1OxYIWUnkCJJKTffv2yU0Q83XEvpi0LJKfm266Cbt378aCBQtgNpuRnp4ut7KyMvvPGDVqlFwdZvPEE09g48aNOH36NLZt24bJkyfDZDLJuUNUP36tXXFNL2vCyFEgIiL1+6aqiv/IiACEtXFXOhxNUDQBEslNnz595CbMmDFD7j/33HM4c+YMfvjhB6SmpiImJkaO4Ng2kdjYiNGd7Oxs+23xeJHsREREyNEjMWq0Y8cO+PtbWz1Q/diqhy7fdwb5JeVKh0NERLUoKTdj8Z5Uuc/KzxqpAzRixAg5sbk2dX3PRoz0XGzhwoVNEpvRDezoi87+HkjIKsSyfWdxZ9XqMCIiUpefD6Uhp6gcId5uGBFhLWVCGp8DRMoR9ZZsyyhFZej6JKNERKRc5eepA9rB5OigdDiawQSIanWjLKPuiGPp+YhLzlE6HCIiusSx9DzsTroAJ0cH3NKfk58vBxMgqpW3uzMmRIXI/a/YH4yISHVs781jegYiwMtN6XA0hQkQ1emOqrk/Kw+cRWZ+idLhEBFRldziciyJOyP372Dl58vGBIjqFB3ug77tfFButtivMxMRkfIW7UpBUZkZEYGeGNz58uvkGR0TIPpTd1/R0V5ltLSC/cGIiJRmrrTg8+3WVdB3X9FBLlyhy8MEiP7UNb2CEOTlhuyCMvx4IE3pcIiIDG/t0QykXiiGj7szJsaEKh2OJjEBoj8lesrcOdh6ffmzrae5JJ6ISGGfbU20L31v5WJSOhxNYgJE9SL+yFydHHHwTC72JF1QOhwiIsM6mpaHHafOy5o/LFLbcEyAqF58PVwwqWqYVYwCERGRsqM/YnpCiE8rpcPRLCZAVG93XdFBfl11OB1nc4qVDoeIyHDOFZTK9kTCPVXvydQwTICo3roHe2FQJ1+5+uBLFkYkImpxC3eloKyiEr1DvdG3XRulw9E0JkDUoCXx3+xMRnEZl8QTEbWUcnMlvtxu/fDJpe+NxwSILsvo7oEI920lOw8v22etQEpERM3v50PpSM8rgV9rV4yPClY6HM1jAkSXRaw6mD64g30iHpfEExG17OTnOwaJVblc+t5YTIDosk3pFw53FxNOZBRgW8I5pcMhItK9fSk52JucA2eTA25n368mwQSILpt3K2fc2Des2icSIiJqPrb32glRIfD3dFU6HF1gAkSNWhK/7lgmks4VKh0OEZFuZeSV2NsQ2RaiUOMxAaIG6ezfGsO7+UNMAfp8G5fEExE1lwU7klBRaUG/9m3QO8xb6XB0gwkQNZhYhiks3p2CgtIKpcMhItKdknIzFvyWLPc5+tO0mABRgw3r6o9O/h7IL63Ad7tTlA6HiEh3Vuw/i3OFZQj2dsPYnoFKh6MrTICowRwdHXDXEOso0Ofbk1BZySXxRERNRZQZsfVevHNweziZeMpuSvzXpEYRq8E83ZyQmF2IjSeylA6HiEg3dp2+gCNpeXBzdsTU/u2UDkd3mABRo3i4OuGWfuFyfx6XxBMRNfnS98l9QtHGw0XpcHSHCRA12vQhHeDoAGw+mY34zHylwyEi0rzUC0VYfThd7t81hJOfmwMTIGq0cF932SNMsF2vJiKihhNNT8W0yiu6tEVEkKfS4egSEyBqErblmUviziC3qFzpcIiINKuorALf7Kxa+s7Rn2bDBIiaxKBOvogM8kRxuRkLd1n/cImI6PKJD5J5JRVo5+uOkZEBSoejW0yAqEk4ODjYCyN+sT0JFeZKpUMiItIcUU5k/rbT9vmVJjHBkpoFEyBqMhNjQtHWwwVncorx40Fr3xoiIqq/9ccyEZ9ZAE9XJ0zpZ206Tc2DCRA1GTdnk70w4gcbT8kiXkREVH8fbkqQX28b1A5ebs5Kh6NrTICoSYlqpe4uJhxNy8Omk9lKh0NEpBl7ks7L4ocuJkfcw75fzY4JEDUpH3cX3FpVsfTDjdZPMkRE9OfEyLmt8GGgl5vS4egeEyBqcvcO7QgnRwdsSziHA6k5SodDRKR6oojsmiMZcHAA7h/WSelwDEHRBGjTpk2YMGECQkJC5CqiZcuWVfu+mEPy3HPPITg4GK1atcLo0aNx8uTJP/25c+bMQYcOHeDm5oaBAwdi586dzXgUdKlQn1a4PjpE7n9Y9YmGiIhq99Em63vl1d0D0SWgtdLhGIKiCVBhYSGio6NlwlKT119/Hf/973/xwQcf4LfffoOHhwfGjh2LkpKSWn/mt99+ixkzZuD5559HXFyc/PniOZmZmc14JHSpB4ZbP8H8fCgNp7MLlQ6HiEi1MvJKsHTvGbn/4PDOSodjGIomQOPGjcMrr7yCyZMn/+F7YvTn3Xffxb/+9S9MnDgRUVFR+OKLL3D27Nk/jBRd7O2338b999+Pu+++Gz169JDJk7u7O+bNm9fMR0MXiwzywsgIf1nK/ePNHAUiIqrNvC2JKDdbMKCDL2Lbt1E6HMNQ7RygxMREpKeny8teNt7e3vKS1vbt22t8TllZGfbs2VPtOY6OjvJ2bc8RSktLkZeXV22jxrN9klm8JxVZ+aVKh0NEpDp5JeVY8Ju1ev6DVSPnZPAESCQ/QmCgtcmmjbht+96lsrOzYTabL+s5wqxZs2RyZdvCw8Ob5BiMbmBHX8SE+6CsohKfV1U2JSKi3y3YkYyC0gp0C2yNkRFse6H6BCglJQWpqan222KS8eOPP46PPvoIWjRz5kzk5ubaN3F81HhiYvtDVZ9ovth+GoWlFUqHRESkGqUVZszbmij3HxjWGY5se6H+BOi2227Dr7/+KvfFyMrVV18tk6B//vOfeOmll5oksKCgIPk1IyOj2v3itu17l/Lz84PJZLqs5wiurq7w8vKqtlHTuLpHEDr6ecjGfrbuxkREBCyNOyOnBwR7u9lXzpLKE6BDhw5hwIABcn/RokXo1asXtm3bhgULFmD+/PlNEljHjh1l0rJu3Tr7fWJujlgNNnjw4Bqf4+LigtjY2GrPqayslLdrew41L9HI74Gqmhafyol+bJJKRCSantqWvt97ZUe4OKl2RopuNehfvLy8XI6aCGvXrsX1118v9yMjI5GWVv8mmAUFBdi3b5/cbBOfxX5ycrK8fCIuq4lVYj/88AMOHjyIadOmyZpBkyZNsv+MUaNGYfbs2fbbYgn8xx9/jM8//xxHjx7Fww8/LJfbi1VhpAxR1dSvtSvSckvww76zSodDRKS4X45k4FR2IbzcnHDrAGv1fGpZTg15Us+ePeXy8vHjx2PNmjV4+eWX5f1iiXrbtm3r/XN2796NkSNHVktehOnTp8uRpKeeekomLw888ABycnJw5ZVXYtWqVbLAoU1CQoKc/Gxzyy23ICsrSxZQFJfnYmJi5HMunRhNLdsk9Z4rO+D1Vcdlo78b+obKBJeIyIhEmZcPqloFif6JrV0bdCqmRnKwNKBl94YNG2TtHnFJSiQrtho7//jHP3Ds2DEsWbIEWiaOS6wGExOiOR+oaeQWl+OKV9fL1Q7z7uqHqyKZkBKRMf126hxu+WiHvOy19emr4O9pvaJCLXv+blDaOWLECDnqIv5Hbdr8XrRJjNSIooNEl/Ju5YzbBraT17xFwz8mQERkVB9Wzf25KTaMyY/W5gAVFxfL4oG25CcpKUlWbT5+/DgCAljHgGp2zxUd4WxywM7E84hLvqB0OERELe54ej7WH8u0Nj0dysKHmkuARGsK0ZZCEHNzRHXmt956S05Onjt3blPHSDoR5O2GSTGhcv/DquvfRERGIuZBCuN6WUuEkMYSINFkdOjQoXL/u+++kxOMxSiQSIpE81Ki2thKvYsVEAlZBUqHQ0TUYs7kFNtXwj44jE1PNZkAFRUVwdPTU+7/8ssvuOGGG2TPrUGDBslEiKg2XQI8Mbp7IMTU+4+rroMTERml6WlFpQWDO7VFdLiP0uEYXoMSoC5dusiO7KJlxOrVqzFmzBh5f2ZmJldN0Z+ytcdYEncGmXklSodDRNTscovK7dXw2fRUwwmQqLHzxBNPoEOHDrIitK3KshgN6tOnT1PHSDrTr4Mv+rVvgzJzJeZtZZNUItK/L3ecRlGZGZFBnhjezV/pcKihCdBNN90kqzWLQoZiBOjiqszvvPNOU8ZHOvXgcOv17wU7kpBXUq50OEREzaak3Iz526wf9h4a3pmFYFWiwc1HRJ8uMdojqj/bOsOL0SDRDoPoz4yKDECXgNbIL63AF1VvDEREeiQufWUXlCHUpxXGRwUrHQ41JgESDUZF13dRbbF9+/Zy8/HxkS0xxPeI/oyjowP+elUXuf/JlkRZIZqISI+jP7a2F38Z2RnOJjY9VYsGvRL//Oc/ZQPSV199FXv37pXbf/7zH7z//vt49tlnmz5K0qXrokLQyd8DOUXl+JyjQESkQ9/uSkFGXilCvN0wJTZc6XCosQmQ6LT+ySefyE7rUVFRcvvLX/4iu7CLJqZE9WG6aBTo482nOApERLob/fnfhni5//DILrL3F6lHg16N8+fP1zjXR9wnvkdUXxOiQmQ1VDEK9MV2jgIRkX4s2m0d/Qn2dsPN/cKUDoeaIgGKjo6Wl8AuJe4To0FE9eVkcvx9FGjTKRRyFIiIdKC0woy5G6xzfx4e0RmuTialQ6Km6Ab/+uuvY/z48Vi7dq29BtD27dtlYcSffvqpIT+SDOz66BD8d91JnD5XhC93JMllokREWrZodyrScksQ5CVGfzj3RzcjQMOHD8eJEycwefJk2QxVbKIdxuHDh/Hll182fZSk+1GgR6/qKvc/2nQKRWUcBSIijY/+/Fo192dEZ7g5c/RHjRwsFtGVqWns378fffv2hdlshpbl5eXJJf65ubls7dFCKsyVGPX2RiSdK8LMcZH2QolERFrz1Y4k/GvZIQR6uWLjkyOZAKn0/M0p6aSeUaCR1rlAHAUiIq0qq6i0z/0Rl/OZ/KgXEyBSjcl9QtHO1x3nCsuwYIe1aSARkZZ8tycVZ3KKEeDpiqkD2ikdDtWBCRCpchTow00JKC7T9qVUIjLe6M+cqrk/HP3R2SowMdG5LmIyNFFjTO4bivd/PYmU88VY8FsS7hvaSemQiIjq5fs46+iPv6crbhvI0R9djQCJiUV1baIn2LRp05ovWtI954tGgT7YeIqjQESkCeXm30d/HhzWiaM/ehsB+uyzz5ovEqIqN/QNw/vr45F6oRhf70zGvVd2VDokIqI6LYlLle9Zfq1dcfvA9kqHQ/XAOUCkylGgR+yjQAmynw4RkZpHf2bb5/50QisXjv5oARMgUqUb+4Yh1KcVsvJL8fVvXBFGROq1NO6MnLfo19qFoz8awgSIVEl0TeYoEBFpafTngWEc/dESJkCkWjfFWkeBMvNLsXAnR4GISH2W7T2D5PNFaOvhgjsGcfRHS5gAkapHgf4y0toSYy5HgYhIhS18Lh79cXdpUH9xUggTIFK1KbHhCPF2Q0ZeKRbtTlE6HCIiu+X7zsr+hb4eLrhzMEd/tIYJEKl+FOjhqrlA//s1QXZZJiJSw+jP++tPyv37h3L0R4uYAJHq3dwvDMHebkjPK8HCnRwFIiJ1jP6cPleENu7OmMbRH01iAkSq5+pkwl+qRoFEgcTCUnaKJyLliJHot9eckPsPDOsMD1eO/mgREyDShFv7h6NDW3dkF5Ti0y2JSodDRAb21Y5k2fMr0MsVdw3poHQ41EBMgEgz1aH/PiZC7n+06RTOFZQqHRIRGVBeSTlmV839eXx0N9b90TDVJ0AdOnSAg4PDH7ZHHnmkxsfPnz//D491c3Nr8bip6Y3vHYzeod4oKK2wLz0lImpJH286hQtF5ejk74EpsWFKh0N6ToB27dqFtLQ0+7ZmzRp5/5QpU2p9jpeXV7XnJCUltWDE1FwcHR3w9DWRcv+rHUlIOV+kdEhEZCCZ+SX4ZLP1EvxTYyPgZFL9KZTqoPpXz9/fH0FBQfZt5cqV6Ny5M4YPH17rc8Soz8XPCQwMbNGYqflc2dUPV3bxQ7nZYp+ESETUEv677iSKy82ICffB2J5BSodDek+ALlZWVoavvvoK99xzj0xyalNQUID27dsjPDwcEydOxOHDh+v8uaWlpcjLy6u2kXrZRoGW7TuDo2l8rYio+Z3OLrSX4XhmXGSd5yDSBk0lQMuWLUNOTg7uuuuuWh8TERGBefPmYfny5TJZqqysxJAhQ5Camlrrc2bNmgVvb2/7JhInUq/eYd64LioYFgvw+qpjSodDRAbw5i/HUVFpwYgIfwzq1FbpcKgJOFgs4jSiDWPHjoWLiwtWrFhR7+eUl5eje/fumDp1Kl5++eVaR4DEZiNGgEQSlJubK+cTkTo/jY1+e6N8Q1r4wCC+IRFRszmYmosJs7dADPr8+Neh6BHC84JaifO3GMioz/lbMyNAYiLz2rVrcd99913W85ydndGnTx/Ex9e+asjV1VX+Q128kbp18PPA1AHt5P6rPx+DhvJ4ItKY16pGmifFhDL50RHNJECfffYZAgICMH78+Mt6ntlsxsGDBxEcHNxssZEy/jqqC1o5m7AvJQerD6crHQ4R6dDmk1nYEp8NZ5MDZlzdTelwyGgJkJjHIxKg6dOnw8mpesnxadOmYebMmfbbL730En755RecOnUKcXFxuOOOO+To0eWOHJH6BXi64f6hHeX+66uPy+aERERNpbLSYh/9uWNQe4T7uisdEhktARKXvpKTk+Xqr0uJ+0WtH5sLFy7g/vvvl/N+rr32Wnk9cNu2bejRo0cLR00t4f5hneDr4YJTWYVYvKf2ie5ERJdr5cE0HDqTh9auTni0qh8h6YemJkGrcRIVKW/elkS8tPKI7Muz4YmRLE1PRI1WVlGJq9/ZiKRzRfLS1/+N6qp0SGTUSdBEtbl9UDuEtWmFjLxSfLaNjVKJqPEW7kqWyY9fa1fce6X1UjvpCxMg0jxXJ5N9cuLcDQnIKSpTOiQi0rDC0gpZ9Vn4v1Fd4OFafe4p6QMTINKFiTGhiAzyRH5JhUyCiIga6tMticguKEP7tu64tb+13AbpDxMg0gXTRY1SP9t2GmdzipUOiYg06FxBKT7caP0Q9fcxEXBx4mlSr/jKkm6IEvUDO/rKyYvvrmWjVCK6fLN/jUdhmRm9Qr1wXW/Wj9MzJkCkG6I54dPjrKNA3+1JxcmMfKVDIiINSTlfhK92JMl9MaLs6MiGp3rGBIh0pW+7NrimZxAqLb+Xryciqm/D03KzBVd28cPQrv5Kh0PNjAkQ6c6T10TAydEBa49mYuOJLKXDISIN2H36PJbvOysbntrmE5K+MQEi3ens3xrTh3SQ+y+tOIxytsggojqYKy14YcVhuX9zbDh6h3krHRK1ACZApEuiamtbDxckZBXi822nlQ6HiFRs8e4U2fLC09VJjiCTMTABIl3ybuWMJ8da38jeW3sS2QWlSodERCqUW1yON1Yfl/uPje4qKz+TMTABIt2a0i8cvUO9kV9agTdWWd/giIguJj4gnSssQ2d/D0wbbL10TsbABIh0XRzxhet7yP1Fe1JwIDVH6ZCISEXiM/PxxXbrJfLnJvRk0UOD4atNuhbb3heT+4TCYgFe+OEwLGKHiAxPvBe8uOIIKiotGN09EMO7cdm70TABIt17Zlwk3F1MiEvOwbJ9Z5QOh4hUYM2RDGw+mQ0XkyOeva670uGQApgAke4FernhkZFd5P6sn46hoLRC6ZCISEEl5Wa88uNRuX/v0I5o39ZD6ZBIAUyAyBDuvVK8ybkjM78Uc36NVzocIlK423vy+SIEerni0aoPR2Q8TIDIENycTfjXeOuE6E83J+J0dqHSIRGRAtJzS+wfgsTlcQ9XJ6VDIoUwASLDGN09AMO6+aPMXGkf/iYiY3n156MoKjOjbzsfTIoJVTocUhATIDJUt/jnrutR1Scsg33CiAzY72tZVb+vF6/vJd8TyLiYAJGhdAn4vU/YiysOo6yCfcKIjID9vuhSTIDIcKzl7l1wKqvQXgSNiPSN/b7oUkyAyHC83Kr3CcvKZ58wIj1jvy+qCRMgMqQpseGICrP2CXuz6o2RiPTd7+viS+BETIDIkBwdHfD8hJ5yn33CiAzS7+u6HnA28bRHVvxNIMOKbd/G3ifs+R8Oo7KSfcKI9NzvS5TBILJhAkSGJguhuZiwNzkH3+xKVjocImpCP+w/y35fVCsmQASj9wl7ompC9Ks/HZNVYolI+84XlsnRH+GvV3Vhvy/6AyZAZHjTBndATLiPnBD9/A+HlA6HiJrAKz8ekUlQRKAnHhzeWelwSIWYAJHhmRwd8OqNvWWF6NWHM7DqUJrSIRFRI2w+mYUlcWdkxedZN/aGixNPdfRH/K0gAhAZ5IWHR1g/JT67/LCsG0JE2lNUVoF/LD0o96cP7oC+7dooHRKpFBMgoiqPjOyCTv4esjDiqz8fUzocImqAd9eeRMr5YoR4/z6/j6gmTICIqrg5mzBrcm+5/83OZOw4dU7pkIjoMhxMzcUnm0/J/Vcm90JrVyelQyIVYwJEdJGBndpi6oB2cv8fSw6ipNysdEhEVA/l5ko8/f0BiHJeE6JDcFVkoNIhkcqpOgF64YUX4ODgUG2LjIys8zmLFy+Wj3Fzc0Pv3r3x008/tVi8pJ/aQAGerjiVXYjZ6+OVDoeI6uHTLYk4kpYHH3dnPD+hh9LhkAaoOgESevbsibS0NPu2ZcuWWh+7bds2TJ06Fffeey/27t2LSZMmye3QIS5tpvrzbuWMlyZa22R8sDEBx9LzlA6JiOpwOrsQ76w5Iff/eW13NjulelF9AuTk5ISgoCD75ufnV+tj33vvPVxzzTV48skn0b17d7z88svo27cvZs+e3aIxk/Zd0ysYY3sGyhL6T39/EGa2ySBSbbsLseqrtKISV3Rpi5tiw5QOiTRC9QnQyZMnERISgk6dOuH2229HcnLt7Qq2b9+O0aNHV7tv7Nix8v66lJaWIi8vr9pG9NLEXvB0dcL+lBx8vs3aTJGI1GXxnlRsSzgHN2dH/GdybzlVgkjzCdDAgQMxf/58rFq1CnPnzkViYiKGDh2K/Pz8Gh+fnp6OwMDqE9/EbXF/XWbNmgVvb2/7Fh4e3qTHQdptk/HMtdY5Z2/+chypF4qUDomILiJKVvz7x6Ny/2+ju7HdBeknARo3bhymTJmCqKgoOZIjJjTn5ORg0aJFTfr/mTlzJnJzc+1bSkpKk/580q6p/dthQAdfFJWZ8a9lh+RwOxGpw4srrEVLe4Z44d4rOyodDmmMqhOgS/n4+KBbt26Ij695ZY6YI5SRkVHtPnFb3F8XV1dXeHl5VduIBEdHB/znht6ym/SG41myuzQRKW/tkQysPJAmW9m8dmMUnEyaOp2RCmjqN6agoAAJCQkIDg6u8fuDBw/GunXrqt23Zs0aeT9RQ3UJaC27SQsvrTiCC4VlSodEZGj5JeV4drl1de99V3ZEr1BvpUMiDVJ1AvTEE09g48aNOH36tFziPnnyZJhMJrnUXZg2bZq8fGXz2GOPyflCb731Fo4dOybrCO3evRuPPvqogkdBeiC6SYuu0ucKy/BK1ZwDIlLGm6uPIy23BO183fH46G5Kh0MapeoEKDU1VSY7ERERuPnmm9G2bVvs2LED/v7+8vtiRZioDWQzZMgQfP311/joo48QHR2N7777DsuWLUOvXr0UPArSA9FNWnSVFgtMvo9Lxa/HM5UOiciQdiaexxc7kuS+WPXVysWkdEikUQ4Wzur8A7EMXqwGExOiOR+ILiYugc3bmigLra1+fCjasuAaUYvJKynHuHc340xOMabEhuGNKdFKh0QaPn+regSISG2euiYC3QJbI7ugFM8sOchVYUQt6Pnlh2XyE+7bCs+x3QU1EhMgosvsGP/uLX3kqrA1RzKwcBdLJhC1BLECc+neM3B0AN69JQaebs5Kh0QaxwSI6DL1CPHCk2Mj7JfETmUVKB0Ska6JUZ9/Lj0o9x+9qiti2/sqHRLpABMgogYQRdeGdG6L4nIz/vbtPpSbK5UOiUiXRB++Gd/uQ35JBWLCffB/VSUpiBqLCRBRAwskvnVztOwcvz81F++tPal0SES69NGmU/gt8TzcXcTl5xgWPKQmw98kogYK9m4ll+EK/9sQj12nzysdEpGuHDqTi7fXHJf7L0zoiQ5+7PVFTYcJEFEjjI8Kxo19w1BpgbwUJpbpElHjFZeZ8djCvSg3WzC2ZyCm9AtTOiTSGSZARI30wvU95LLc1AvFeGH5YaXDIdKF//x0FAlZhQjwdMWrN0TBQVQhJWpCTICIGkksxxVzE8Ty3CV7z2AFG6YSNcr6Yxn4sqras5hr18bDRemQSIeYABE1AbEsVyzPFcRy3bM5xUqHRKRJosjoU98dkPv3XNERQ7taWx8RNTUmQERNRHSMjw73QV5JBWYs2odKMTGIiOpNVFZ/+rsDyC4oQ2SQp6y8TtRcmAARNRFnkyPeuyVGLtfdceo8Pt58SumQiDRlwW/JWHcsUzYffvfWGFl5nai5MAEiakJime7zVT2K3vzluFzGS0R/Lj6zAK/8eETuP31NJCKD2IiamhcTIKImdnO/cLlsVyzfffzbfXI5LxHVrqyiEo9/uxcl5ZUY2tUPdw/poHRIZABMgIiamFiuO+uGKLl8V3yqfWkll8YT1eX1Vcdw6EwefNyd8eaUaFlpnai5MQEiaga+Hi5y+a4oXfLNzhQs3JmsdEhEqiTKRnyyJVHuv3ZjFAK93JQOiQyCCRBRMxHLd58YY13F8tzyw9iXkqN0SESqcjw9377k/eERnTG2Z5DSIZGBMAEiakYPD++MMT0CUWauxMNf7ZE1TogIyC0ux4Nf7kZxuRlXdvGzf1ggailMgIhaoGt8Jz8PpOWW4K9f70WFuVLpsIgUJWpkzfh2H06fK0KoTyv8d2ofmDjvh1oYEyCiFmiV8eGdsfBwMWH7qXN4fbW1uzWRUb2/Pl7W+3F1cpR/G2LOHFFLYwJE1AK6BnrK1S3CR5tOYeUB9gsj4/b5enfdCbn/78m90SvUW+mQyKCYABG1kHG9g/HQ8M5yX0z8FBNAiYzkdHYhHl+4DxYLcOeg9rgpNkzpkMjAmAARtaAnxnSTEz6Lysx46Ks9ciIokREUlVXI33nRK69vOx88e521YjqRUpgAEbUgJ5OjnPApJn4mZhfKiaBsmkqGaHL6/UEcS8+HX2tXzL0jVvb7IlISfwOJWpiY8PlB1QlATAQVE0KJ9OzTLYmy4KGTowP+d3tfFjskVWACRKSA3mHe+PekXnJfTAgVE0OJ9Gh7wjnM+vmY3P/X+O4Y0NFX6ZCIJCZARAqZ0i8cdwxqJyeEiomhYoIokZ6k5Rbj0a/jYK60YHKfUExnk1NSESZARAp67rqe6NPOR04MFRNExURRIj0orRAT/eNwrrAM3YO98J/JvWWjYCK1YAJEpCAxD2ju7bFyYqiYIPrkdwc4KZp0Men52WWHsD8lB96tnPHhHbFo5WJSOiyiapgAESksyNtNTgwVE0R/PJCGWT8fVTokokZ5b91JLNqdCjHg896tMWjX1l3pkIj+gAkQkQqIiaGv3xQl9z/enIhPNp9SOiSiBvlmZzLeXXtS7r80sRdGRAQoHRJRjZgAEanEDX3D8PQ1kXL/lR+P4of9bJdB2rL2SAb+ufSg3H90ZBdZ7ZlIrZgAEanIQ8M74a6qlTJ/X7QP2+KzlQ6JqF7iki/g0W/iIKawTYkNw9/HdFM6JKI6MQEiUhGxSka0CLi2dxDKzRY88OUeHDmbp3RYRHVKyCrAvfN3oaS8EiMi/PGfG7jii9RP1QnQrFmz0L9/f3h6eiIgIACTJk3C8ePH63zO/Pnz5R/exZubG6uOknaYHB3w9s0xGNjRFwWlFbjrs51IOV+kdFhENcrMK8G0T3fiQlE5osO85YR+Z5OqTy1Ekqp/Szdu3IhHHnkEO3bswJo1a1BeXo4xY8agsLDugnFeXl5IS0uzb0lJSS0WM1FTcHM24aNp/RAR6InM/FJM/2wnLhSWKR0WUTX5JeWY/tkunMkpRoe27ph3V3+4uzgpHRZRvaj6N3XVqlV/GN0RI0F79uzBsGHDan2eGPUJCgpqgQiJmo+onzL/nv644X/bcCqrEPd+vgsL7hvEeiqkCmUVlbJ459G0PPi1dsEX9wxE29auSodFpI8RoEvl5ubKr76+dfeSKSgoQPv27REeHo6JEyfi8OHDdT6+tLQUeXl51TYiNQj2boXP7xkALzcnxCXn4K/f7EWFuVLpsMjgRLHOJxbvx9b4c/BwMeGzuwaw1g9pjmYSoMrKSjz++OO44oor0KuXtYlkTSIiIjBv3jwsX74cX331lXzekCFDkJqaWudcI29vb/smEicitegW6IlP7+ovq0avPZqBZ5cflpV2iZQiinWKMg2ieOfcO2Jlc18irXGwaOSd9OGHH8bPP/+MLVu2ICwsrN7PE/OGunfvjqlTp+Lll1+udQRIbDZiBEgkQWLEScwnIlKDVYfS8PCCONk89W+ju+Gx0V2VDokMSBTpFHWqhLdvjpb1q4jUQpy/xUBGfc7fmhgBevTRR7Fy5Ur8+uuvl5X8CM7OzujTpw/i4+NrfYyrq6v8h7p4I1Kba3oFy8q6wjtrT2DhzmSlQyKDEaM+tuTnmXGRTH5I01SdAInBKZH8LF26FOvXr0fHjh0v+2eYzWYcPHgQwcHBzRIjUUsSlXVFhV3hH0sPYune2i/tEjX1CKQozimIYp0PDuukdEhE+l0FJpbAf/3113I+j6gFlJ6eLu8Xw1utWrWS+9OmTUNoaKicxyO89NJLGDRoELp06YKcnBy88cYbchn8fffdp+ixEDUVUWH3XGEpvtmZghmL9qO0vBK3DmindFikY8v3nZG/a+ZKC66PDsFz1/VgoUPSPFUnQHPnzpVfR4wYUe3+zz77DHfddZfcT05OhqPj7wNZFy5cwP333y+TpTZt2iA2Nhbbtm1Djx49Wjh6ouYhTjz/ntQbTo6O+HJHEp5ZchBl5kpMG2xtoUHUlBbvTsFT3x+Qc89u7Bsmm/Y6OjL5Ie3TzCRotU6iIlKK+NMV8zE+3ZIob/9rfHfcN5SXJajpfP1bsrzUKkwdEC4TbyY/pGa6mwRNRDWPBImk55GRneVtkQzNXn9S6bBIJz7bmmhPfsScn/9MZvJD+qLqS2BE9OdJ0JNjI+HqZMLba07gzV9OoLSiEjOu7sY5GtRgH2xMwKs/H5P7YrKzWPHF3yfSGyZARDrwf6O6ykKJ4qT1/vp42aaAJy1qyGXV/66Ll2UWhP+7qgv+xmSadIoJEJFOPDS8M1ydHPHiiiP4cNMpORL0/ASu1qH6Jz9v/nIcc35NkLefGNMNj17FYpukX0yAiHTk7is6ysth/1x2EPO3nZZJ0L8n9eLcDbqsCfX/vLY77medH9I5JkBEOnPbwHbycthT3+3HNzuTUVphxhs3RcPEJIhqaWz63A+H8NUOa2Xxlyb2ZEkFMgQmQEQ6dFNsGJxNDrJ43ZK4M3JO0Du3xMDZxIWf9DtR2PAfSw7i290pEFdKZ03uzaKaZBhMgIh0amJMqJwT9Ndv9mLlgTScKyjD/27vizYeLkqHRiqQV1KOx77Zi1+PZ0EMDr45hY1NyVj4cZBI5w1UP7qzHzxcTNh+6hyun7MFx9PzlQ6LFJaYXYjJc7bK5EckybNv68vkhwyHCRCRzo2MDMCSv1yBcN9WSDlfjBv+txW/HLb21SPj2XQiCxNnb0FCViGCvNyw+KHBuLY3m0WT8TABIjKAiCBP/PDIlRjcqS0Ky8x44Ms9smo0O+EYh3itxSqvuz7bibySCvRp54MfHr0CUWE+SodGpAgmQEQGIeb+fHHvAEwf3F7eFlWjxfyg4jKz0qFRMxMrAZ/67gBeXnkElRbrJPmFDwxCgJeb0qERKYaToIkMRKwCe3FiL0QEeeG55Yfk5OjT5wrlPKEQn1ZKh0fNIDO/BA99uQdxyTlysvM/ru2Oe6/syAKZZHgcASIyaK2gBfcNhK+HCw6dycP1s7diT9J5pcOiJnboTC4mzt4qkx8vNyfMv3sA7hvaickPERMgIuMa2KmtnAMSGeSJ7IJSTP3oNyzanaJ0WNREVuw/i5s+2Ia03BJ08vfAskeuwLBu/kqHRaQaTICIDCysjTu+f3gIxvUKQpm5Us4TeWnFEVSYK5UOjRpR2fnN1cfl/K6S8kqMjPCXyU8n/9ZKh0akKkyAiAzOw9UJc27ri8dHWxtfztuaiFs/2oHT2YVKh0aXKfVCEe6c9xtm/xovbz84vBM+md4fXm7OSodGpDoOFq6D/YO8vDx4e3sjNzcXXl5eSodD1GJWHUrD3xftl0vl3Zwd8fQ1kZg+uAObqaqceBtfuCsF//7xKApKK+Rr95/JvVnckAwn7zLO30yAasAEiIws5XwRnv7+ALYlnJO3B3T0xZs3RaNdW3elQ6ManM0plq/X5pPZ8na/9m3wxpRodPTzUDo0ohbHBKiRmACR0Yl5JAt2JmPWT0dRVGZGK2cTZl4biTsGtudokEqIt24xaf2VlUeRX1ohW1o8OTYCd1/RESa+RmRQeUyAGocJENHvo0FPfrcfO05Zl8iLStKv3xSFcF+OBikpLbcYz3x/EBtPZMnboqqzaGbamROdyeDymAA1DhMgouqjQV/uSMKrPx9DcblZNladeW133D6wHevJtDDxdv193Bm8uOIw8ksq4OLkiL9f3U3W9uGoDxGYADUWEyCiP0o6V4gnFx/AztPW0aAru/jh1Rt7y6X01Pwy8kowc8lBrD+WKW9Hh/vgrSlR6BLgqXRoRKrBBKiRmAAR1T4aNH/baby++pisMdPa1QlPXROBqQPayTYb1PRETabv41LlCi/RxNTF5IjHr+6KB4Z2ghP/zYmqYQLUSEyAiOqWmC1Gg/Zjd9IFebt9W3f8bXQ3TIgO4aWYJkw2Vx1Ox1u/HEdClrUmU+9Qb7x1czS6BXLUh6gmTIAaiQkQ0Z8zi5VivyXhv+tOIrugTN4XEeiJGWO6YUyPQM4PaiDxlrzhRJas5nz4bJ68z8fdGY+M6IK7r+jAUR+iOjABaiQmQET1V1RWgc+2nsaHGxPkJRohOswbT4yNkPOEmAjV32+nzuGN1cftI2viEqPo3H7v0I6s5kxUD0yAGokJENHlyy0ux8ebTslWGqJ2kDCwo6+sTdOvg6/S4anagdQcmfjYihmKmj7Th3TAQ8M7w9fDRenwiDSDCVAjMQEiajjRWf5/vybgqx1JssGqcFVkAP4+pht6hngrHZ6qnMjIl3N8Vh/OkLedHB1w64Bw/PWqrgj0clM6PCLNYQLUSEyAiJqmRcP7609i0e5UOV9IEF3nbxvYDkM6+xl2srSY3Pxb4nl8szMZKw6chXgHFv8Uk/qE4vFR3dhyhKgRmAA1EhMgoqZdMfbu2hP4Yb/1ZC8Ee7thcp9Q3BgbZpjqxaKOkihiuCQuFakXiu33i6RwxtXd0JUru4gajQlQIzEBImp6x9Pz5WUxkQiJ+UI2oo3DjX3DMCEqBN7u+prom19Sjp8OpuG7PanYddo6sVnwdHXC+Khg3DGoPXqF8rIgUVNhAtRITICImk9phRnrjmbi+z2pcrm37fKYaOtwdY9A3NQ3DEO7+ml2ubc4nm0J2fL4RB0fUTBSEIvhxKq4m2LDMLZnENycTUqHSqQ7TIAaiQkQUcvIzC/BD/vOyhGSY+n59vv9PV0xKSYEV3b1R0y4D7xbOat+pGd/Si62xGdj+b4zSMstsX+vs78HbooNl5f8grw5sZmoOekuAZozZw7eeOMNpKenIzo6Gu+//z4GDBhQ6+MXL16MZ599FqdPn0bXrl3x2muv4dprr633/48JEFHLEm9DouifaPmwfN9ZnC+0Fla0jZx08W+Nvu3aILZ9G/Rt74NOfq3hqNAkahGrmNcUl5yDPUkXsDf5Ao5n5NvnNwkiYbs+OkTOcRI1kVgLiahl6CoB+vbbbzFt2jR88MEHGDhwIN59912Z4Bw/fhwBAQF/ePy2bdswbNgwzJo1C9dddx2+/vprmQDFxcWhV69e9fp/MgEiUk5ZRSU2HM/EqkPpiEu+gNPniv7wGC83J/SxJUTt2iA63BuezVQosLC0AvtTcxCXdEEmPSLhuVD0+xwmm3DfVjIWcXlrVPcAuDrxEhdRS9NVAiSSnv79+2P27NnydmVlJcLDw/HXv/4VzzzzzB8ef8stt6CwsBArV6603zdo0CDExMTIJKo+mAARqauu0N7kHJkMiSTkQGouisuthRZtxABLWw8X+Li7oI27M9rIry7w8bDtO1d9z7ovHi+SmAuFZcgRX4vK5O0c+bXM/j3x9XxhKaqmKdmJQoVRYd4y4RGJmBiVCvDk5S0ipV3O+dsJKlZWVoY9e/Zg5syZ9vscHR0xevRobN++vcbniPtnzJhR7b6xY8di2bJltf5/SktL5XbxPyARqYNfa1c5OVpsQrm5EsfS8q0JUdWWcr5Y9iOz9SRraqE+reRqNZHw9G3fBj2CveSkbSLSLlUnQNnZ2TCbzQgMtL7x2Yjbx44dq/E5Yp5QTY8X99dGXC578cUXmyhqImpOziZH9A7zlptoFyGcKyhFRl5p1QiOdUSn+n71r2Lc+/dRIesokW3fx+Pi+5zlyI6YlE1E+qLqBKiliBGmi0eNxAiQuMxGRNrQtrWr3IiIdJEA+fn5wWQyISPD2ifHRtwOCgqq8Tni/st5vODq6io3IiIiMgZVX8R2cXFBbGws1q1bZ79PTIIWtwcPHlzjc8T9Fz9eWLNmTa2PJyIiIuNR9QiQIC5NTZ8+Hf369ZO1f8QyeLHK6+6775bfF0vkQ0ND5Twe4bHHHsPw4cPx1ltvYfz48Vi4cCF2796Njz76SOEjISIiIrVQfQIklrVnZWXhueeekxOZxXL2VatW2Sc6Jycny5VhNkOGDJG1f/71r3/hH//4hyyEKFaA1bcGEBEREemf6usAKYF1gIiIiPR9/lb1HCAiIiKi5sAEiIiIiAyHCRAREREZDhMgIiIiMhwmQERERGQ4TICIiIjIcJgAERERkeEwASIiIiLDYQJEREREhqP6VhhKsBXHFhUliYiISBts5+36NLlgAlSD/Px8+TU8PFzpUIiIiKgB53HREqMu7AVWg8rKSpw9exaenp5wcHBo8uxUJFYpKSm67DPG49M+vR8jj0/79H6MPL6GEymNSH5CQkKqNUqvCUeAaiD+0cLCwpr1/yFedD3+Ytvw+LRP78fI49M+vR8jj69h/mzkx4aToImIiMhwmAARERGR4TABamGurq54/vnn5Vc94vFpn96PkcenfXo/Rh5fy+AkaCIiIjIcjgARERGR4TABIiIiIsNhAkRERESGwwSIiIiIDIcJUDOYM2cOOnToADc3NwwcOBA7d+6s8/GLFy9GZGSkfHzv3r3x008/QS/HN3/+fFlN++JNPE+tNm3ahAkTJsgqoiLWZcuW/elzNmzYgL59+8oVDV26dJHHrJfjE8d26esntvT0dKjRrFmz0L9/f1nFPSAgAJMmTcLx48f/9Hla+RtsyPFp7W9w7ty5iIqKshfJGzx4MH7++WddvH4NOT6tvX6XevXVV2XMjz/+ONT2GjIBamLffvstZsyYIZf4xcXFITo6GmPHjkVmZmaNj9+2bRumTp2Ke++9F3v37pVvaGI7dOgQ9HB8gvgjT0tLs29JSUlQq8LCQnlMIsmrj8TERIwfPx4jR47Evn375B/5fffdh9WrV0MPx2cjTrIXv4bi5KtGGzduxCOPPIIdO3ZgzZo1KC8vx5gxY+Rx10ZLf4MNOT6t/Q2KKvzipLlnzx7s3r0bV111FSZOnIjDhw9r/vVryPFp7fW72K5du/Dhhx/KhK8uir2GYhk8NZ0BAwZYHnnkEftts9lsCQkJscyaNavGx998882W8ePHV7tv4MCBlgcffNCih+P77LPPLN7e3hYtEn8eS5curfMxTz31lKVnz57V7rvlllssY8eOtejh+H799Vf5uAsXLli0KDMzU8a/cePGWh+jtb/Byz0+Lf8N2rRp08byySef6O71q8/xafX1y8/Pt3Tt2tWyZs0ay/Dhwy2PPfZYrY9V6jXkCFATKisrk1n96NGjq/UVE7e3b99e43PE/Rc/XhAjKrU9XmvHJxQUFKB9+/ay+d2ffdLRGi29fo0RExOD4OBgXH311di6dSu0Ijc3V3719fXV5WtYn+PT8t+g2WzGwoUL5QiXuFSkt9evPsen1dfvkUcekaPjl742anoNmQA1oezsbPkLHRgYWO1+cbu2ORPi/st5vNaOLyIiAvPmzcPy5cvx1VdfobKyEkOGDEFqair0oLbXT3Q7Li4uhtaJpOeDDz7A999/LzfxBjxixAh5+VPtxO+auCR5xRVXoFevXrU+Tkt/gw05Pi3+DR48eBCtW7eW8+oeeughLF26FD169NDN63c5x6fF12/hwoXyPULMWasPpV5DdoOnZiU+1Vz8yUb84Xbv3l1eF3755ZcVjY3+nHjzFdvFr19CQgLeeecdfPnll1D7J1Axh2DLli3Qo/oenxb/BsXvnJhTJ0a4vvvuO0yfPl3Of6otSdCayzk+rb1+KSkpeOyxx+QcNbVP1mYC1IT8/PxgMpmQkZFR7X5xOygoqMbniPsv5/FaO75LOTs7o0+fPoiPj4ce1Pb6iUmLrVq1gh4NGDBA9UnFo48+ipUrV8pVb2LSaV209DfYkOPT4t+gi4uLXFEpxMbGysm07733njzp6+H1u5zj09rrt2fPHrkoRqyMtRFXDsTv6uzZs1FaWirPI2p4DXkJrIl/qcUv87p16+z3ieFKcbu267vi/osfL4jMua7rwVo6vkuJPwQx/CsureiBll6/piI+uar19RNzu0VyIC4prF+/Hh07dtTVa9iQ49PD36B4nxEnTq2/fg05Pq29fqNGjZLxifcJ29avXz/cfvvtcv/S5EfR17BZp1gb0MKFCy2urq6W+fPnW44cOWJ54IEHLD4+Ppb09HT5/TvvvNPyzDPP2B+/detWi5OTk+XNN9+0HD161PL8889bnJ2dLQcPHrTo4fhefPFFy+rVqy0JCQmWPXv2WG699VaLm5ub5fDhwxa1rlzYu3ev3MSfx9tvvy33k5KS5PfFsYljtDl16pTF3d3d8uSTT8rXb86cORaTyWRZtWqVRQ/H984771iWLVtmOXnypPydFCs5HB0dLWvXrrWo0cMPPyxXzGzYsMGSlpZm34qKiuyP0fLfYEOOT2t/gyJ2saotMTHRcuDAAXnbwcHB8ssvv2j+9WvI8Wnt9avJpavA1PIaMgFqBu+//76lXbt2FhcXF7lsfMeOHdV+EaZPn17t8YsWLbJ069ZNPl4sqf7xxx8tejm+xx9/3P7YwMBAy7XXXmuJi4uzqJVt2felm+2YxFdxjJc+JyYmRh5jp06d5LJVvRzfa6+9ZuncubN8w/X19bWMGDHCsn79eota1XRsYrv4NdHy32BDjk9rf4P33HOPpX379jJef39/y6hRo+zJgdZfv4Ycn9Zev/okQGp5DR3Ef5p3jImIiIhIXTgHiIiIiAyHCRAREREZDhMgIiIiMhwmQERERGQ4TICIiIjIcJgAERERkeEwASIiIiLDYQJEREREhsMEiIiIiAyHCRAREREZDhMgIiIiMhwmQESkaytXroSPjw/MZrO8vW/fPjg4OOCZZ56xP+a+++7DHXfcoWCURNTSmAARka4NHToU+fn52Lt3r7y9ceNG+Pn5YcOGDfbHiPtGjBihYJRE1NKYABGRrnl7eyMmJsae8Iivf/vb32RCVFBQgDNnziA+Ph7Dhw9XOlQiakFMgIhI90RyIxIfi8WCzZs344YbbkD37t2xZcsWOfoTEhKCrl27Kh0mEbUgp5b8nxERKUFc3po3bx72798PZ2dnREZGyvtEUnThwgWO/hAZEEeAiMgw84Deeecde7JjS4DExvk/RMbDBIiIdK9NmzaIiorCggUL7MnOsGHDEBcXhxMnTnAEiMiAmAARkSGIJEcshbclQL6+vujRoweCgoIQERGhdHhE1MIcLGJWIBEREZGBcASIiIiIDIcJEBERERkOEyAiIiIyHCZAREREZDhMgIiIiMhwmAARERGR4TABIiIiIsNhAkRERESGwwSIiIiIDIcJEBERERkOEyAiIiKC0fw/lkXls9j0F8AAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "\n",
    "# our model for the forward pass\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# List of weights/Mean square Error (Mse) for each input\n",
    "w_list = []\n",
    "mse_list = []\n",
    "\n",
    "for w in np.arange(0.0, 4.1, 0.1):\n",
    "    # Print the weights and initialize the lost\n",
    "    print(\"w=\", w)\n",
    "    l_sum = 0\n",
    "\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        # For each input and output, calculate y_hat\n",
    "        # Compute the total loss and add to the total error\n",
    "        y_pred_val = forward(x_val)\n",
    "        l = loss(x_val, y_val)\n",
    "        l_sum += l\n",
    "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
    "    # Now compute the Mean squared error (mse) of each\n",
    "    # Aggregate the weight/mse from this run\n",
    "    print(\"MSE=\", l_sum / len(x_data))\n",
    "    w_list.append(w)\n",
    "    mse_list.append(l_sum / len(x_data))\n",
    "\n",
    "# Plot it all\n",
    "plt.plot(w_list, mse_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "944bc4b2-a165-4e46-9943-60242869c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (before training) 4 4.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.84\n",
      "\tgrad:  3.0 6.0 -16.23\n",
      "progress: 0 w= 1.26 loss= 4.92\n",
      "\tgrad:  1.0 2.0 -1.48\n",
      "\tgrad:  2.0 4.0 -5.8\n",
      "\tgrad:  3.0 6.0 -12.0\n",
      "progress: 1 w= 1.45 loss= 2.69\n",
      "\tgrad:  1.0 2.0 -1.09\n",
      "\tgrad:  2.0 4.0 -4.29\n",
      "\tgrad:  3.0 6.0 -8.87\n",
      "progress: 2 w= 1.6 loss= 1.47\n",
      "\tgrad:  1.0 2.0 -0.81\n",
      "\tgrad:  2.0 4.0 -3.17\n",
      "\tgrad:  3.0 6.0 -6.56\n",
      "progress: 3 w= 1.7 loss= 0.8\n",
      "\tgrad:  1.0 2.0 -0.6\n",
      "\tgrad:  2.0 4.0 -2.34\n",
      "\tgrad:  3.0 6.0 -4.85\n",
      "progress: 4 w= 1.78 loss= 0.44\n",
      "\tgrad:  1.0 2.0 -0.44\n",
      "\tgrad:  2.0 4.0 -1.73\n",
      "\tgrad:  3.0 6.0 -3.58\n",
      "progress: 5 w= 1.84 loss= 0.24\n",
      "\tgrad:  1.0 2.0 -0.33\n",
      "\tgrad:  2.0 4.0 -1.28\n",
      "\tgrad:  3.0 6.0 -2.65\n",
      "progress: 6 w= 1.88 loss= 0.13\n",
      "\tgrad:  1.0 2.0 -0.24\n",
      "\tgrad:  2.0 4.0 -0.95\n",
      "\tgrad:  3.0 6.0 -1.96\n",
      "progress: 7 w= 1.91 loss= 0.07\n",
      "\tgrad:  1.0 2.0 -0.18\n",
      "\tgrad:  2.0 4.0 -0.7\n",
      "\tgrad:  3.0 6.0 -1.45\n",
      "progress: 8 w= 1.93 loss= 0.04\n",
      "\tgrad:  1.0 2.0 -0.13\n",
      "\tgrad:  2.0 4.0 -0.52\n",
      "\tgrad:  3.0 6.0 -1.07\n",
      "progress: 9 w= 1.95 loss= 0.02\n",
      "Predicted score (after training) 4 hours of studying:  7.804863933862125\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = 1.0  # a random guess: random value\n",
    "\n",
    "\n",
    "# our model forward pass\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "\n",
    "# compute gradient\n",
    "def gradient(x, y):  # d_loss/d_w\n",
    "    return 2 * x * (x * w - y)\n",
    "\n",
    "\n",
    "# Before training\n",
    "print(\"Prediction (before training)\",  4, forward(4))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        # Compute derivative w.r.t to the learned weights\n",
    "        # Update the weights\n",
    "        # Compute the loss and print progress\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - 0.01 * grad\n",
    "        print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n",
    "        l = loss(x_val, y_val)\n",
    "    print(\"progress:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2))\n",
    "\n",
    "# After training\n",
    "print(\"Predicted score (after training)\",  \"4 hours of studying: \", forward(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06ed8d14-5849-4c26-a0af-1627ca2dda32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (before training) 4 4.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.840000152587891\n",
      "\tgrad:  3.0 6.0 -16.228801727294922\n",
      "Epoch: 0 | Loss: 7.315943717956543\n",
      "\tgrad:  1.0 2.0 -1.478623867034912\n",
      "\tgrad:  2.0 4.0 -5.796205520629883\n",
      "\tgrad:  3.0 6.0 -11.998146057128906\n",
      "Epoch: 1 | Loss: 3.9987640380859375\n",
      "\tgrad:  1.0 2.0 -1.0931644439697266\n",
      "\tgrad:  2.0 4.0 -4.285204887390137\n",
      "\tgrad:  3.0 6.0 -8.870372772216797\n",
      "Epoch: 2 | Loss: 2.1856532096862793\n",
      "\tgrad:  1.0 2.0 -0.8081896305084229\n",
      "\tgrad:  2.0 4.0 -3.1681032180786133\n",
      "\tgrad:  3.0 6.0 -6.557973861694336\n",
      "Epoch: 3 | Loss: 1.1946394443511963\n",
      "\tgrad:  1.0 2.0 -0.5975041389465332\n",
      "\tgrad:  2.0 4.0 -2.3422164916992188\n",
      "\tgrad:  3.0 6.0 -4.848389625549316\n",
      "Epoch: 4 | Loss: 0.6529689431190491\n",
      "\tgrad:  1.0 2.0 -0.4417421817779541\n",
      "\tgrad:  2.0 4.0 -1.7316293716430664\n",
      "\tgrad:  3.0 6.0 -3.58447265625\n",
      "Epoch: 5 | Loss: 0.35690122842788696\n",
      "\tgrad:  1.0 2.0 -0.3265852928161621\n",
      "\tgrad:  2.0 4.0 -1.2802143096923828\n",
      "\tgrad:  3.0 6.0 -2.650045394897461\n",
      "Epoch: 6 | Loss: 0.195076122879982\n",
      "\tgrad:  1.0 2.0 -0.24144840240478516\n",
      "\tgrad:  2.0 4.0 -0.9464778900146484\n",
      "\tgrad:  3.0 6.0 -1.9592113494873047\n",
      "Epoch: 7 | Loss: 0.10662525147199631\n",
      "\tgrad:  1.0 2.0 -0.17850565910339355\n",
      "\tgrad:  2.0 4.0 -0.699742317199707\n",
      "\tgrad:  3.0 6.0 -1.4484672546386719\n",
      "Epoch: 8 | Loss: 0.0582793727517128\n",
      "\tgrad:  1.0 2.0 -0.1319713592529297\n",
      "\tgrad:  2.0 4.0 -0.5173273086547852\n",
      "\tgrad:  3.0 6.0 -1.070866584777832\n",
      "Epoch: 9 | Loss: 0.03185431286692619\n",
      "Prediction (after training) 4 7.804864406585693\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pdb\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "w = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# our model forward pass\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# Loss function\n",
    "def loss(y_pred, y_val):\n",
    "    return (y_pred - y_val) ** 2\n",
    "\n",
    "# Before training\n",
    "print(\"Prediction (before training)\",  4, forward(4).item())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        y_pred = forward(x_val) # 1) Forward pass\n",
    "        l = loss(y_pred, y_val) # 2) Compute loss\n",
    "        l.backward() # 3) Back propagation to update weights\n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.item())\n",
    "        w.data = w.data - 0.01 * w.grad.item()\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print(f\"Epoch: {epoch} | Loss: {l.item()}\")\n",
    "\n",
    "# After training\n",
    "print(\"Prediction (after training)\",  4, forward(4).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e163445-4edf-45c0-b7c0-2d4727e2168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 35.59011459350586 \n",
      "Epoch: 1 | Loss: 16.140460968017578 \n",
      "Epoch: 2 | Loss: 7.477764129638672 \n",
      "Epoch: 3 | Loss: 3.6171751022338867 \n",
      "Epoch: 4 | Loss: 1.8944075107574463 \n",
      "Epoch: 5 | Loss: 1.1233971118927002 \n",
      "Epoch: 6 | Loss: 0.7761385440826416 \n",
      "Epoch: 7 | Loss: 0.617583155632019 \n",
      "Epoch: 8 | Loss: 0.5430889129638672 \n",
      "Epoch: 9 | Loss: 0.506071925163269 \n",
      "Epoch: 10 | Loss: 0.4857944846153259 \n",
      "Epoch: 11 | Loss: 0.4730234742164612 \n",
      "Epoch: 12 | Loss: 0.4636485278606415 \n",
      "Epoch: 13 | Loss: 0.4558376669883728 \n",
      "Epoch: 14 | Loss: 0.4487760663032532 \n",
      "Epoch: 15 | Loss: 0.44209909439086914 \n",
      "Epoch: 16 | Loss: 0.4356441795825958 \n",
      "Epoch: 17 | Loss: 0.4293383061885834 \n",
      "Epoch: 18 | Loss: 0.42314788699150085 \n",
      "Epoch: 19 | Loss: 0.417057603597641 \n",
      "Epoch: 20 | Loss: 0.41106006503105164 \n",
      "Epoch: 21 | Loss: 0.40515071153640747 \n",
      "Epoch: 22 | Loss: 0.3993271589279175 \n",
      "Epoch: 23 | Loss: 0.3935878276824951 \n",
      "Epoch: 24 | Loss: 0.387931227684021 \n",
      "Epoch: 25 | Loss: 0.38235577940940857 \n",
      "Epoch: 26 | Loss: 0.3768606185913086 \n",
      "Epoch: 27 | Loss: 0.3714447021484375 \n",
      "Epoch: 28 | Loss: 0.3661067485809326 \n",
      "Epoch: 29 | Loss: 0.36084502935409546 \n",
      "Epoch: 30 | Loss: 0.3556588292121887 \n",
      "Epoch: 31 | Loss: 0.3505479097366333 \n",
      "Epoch: 32 | Loss: 0.3455098271369934 \n",
      "Epoch: 33 | Loss: 0.34054404497146606 \n",
      "Epoch: 34 | Loss: 0.3356500267982483 \n",
      "Epoch: 35 | Loss: 0.33082619309425354 \n",
      "Epoch: 36 | Loss: 0.32607167959213257 \n",
      "Epoch: 37 | Loss: 0.3213854432106018 \n",
      "Epoch: 38 | Loss: 0.3167667090892792 \n",
      "Epoch: 39 | Loss: 0.3122141659259796 \n",
      "Epoch: 40 | Loss: 0.30772700905799866 \n",
      "Epoch: 41 | Loss: 0.3033047914505005 \n",
      "Epoch: 42 | Loss: 0.2989458739757538 \n",
      "Epoch: 43 | Loss: 0.2946493625640869 \n",
      "Epoch: 44 | Loss: 0.29041483998298645 \n",
      "Epoch: 45 | Loss: 0.2862410247325897 \n",
      "Epoch: 46 | Loss: 0.2821272909641266 \n",
      "Epoch: 47 | Loss: 0.2780727744102478 \n",
      "Epoch: 48 | Loss: 0.2740764617919922 \n",
      "Epoch: 49 | Loss: 0.27013757824897766 \n",
      "Epoch: 50 | Loss: 0.26625531911849976 \n",
      "Epoch: 51 | Loss: 0.2624289095401764 \n",
      "Epoch: 52 | Loss: 0.2586572766304016 \n",
      "Epoch: 53 | Loss: 0.25493988394737244 \n",
      "Epoch: 54 | Loss: 0.25127583742141724 \n",
      "Epoch: 55 | Loss: 0.2476646602153778 \n",
      "Epoch: 56 | Loss: 0.24410542845726013 \n",
      "Epoch: 57 | Loss: 0.24059727787971497 \n",
      "Epoch: 58 | Loss: 0.23713919520378113 \n",
      "Epoch: 59 | Loss: 0.23373126983642578 \n",
      "Epoch: 60 | Loss: 0.23037222027778625 \n",
      "Epoch: 61 | Loss: 0.2270614206790924 \n",
      "Epoch: 62 | Loss: 0.22379818558692932 \n",
      "Epoch: 63 | Loss: 0.22058187425136566 \n",
      "Epoch: 64 | Loss: 0.21741198003292084 \n",
      "Epoch: 65 | Loss: 0.21428731083869934 \n",
      "Epoch: 66 | Loss: 0.21120762825012207 \n",
      "Epoch: 67 | Loss: 0.20817239582538605 \n",
      "Epoch: 68 | Loss: 0.20518045127391815 \n",
      "Epoch: 69 | Loss: 0.20223182439804077 \n",
      "Epoch: 70 | Loss: 0.1993253231048584 \n",
      "Epoch: 71 | Loss: 0.19646084308624268 \n",
      "Epoch: 72 | Loss: 0.19363737106323242 \n",
      "Epoch: 73 | Loss: 0.19085457921028137 \n",
      "Epoch: 74 | Loss: 0.18811140954494476 \n",
      "Epoch: 75 | Loss: 0.1854080855846405 \n",
      "Epoch: 76 | Loss: 0.1827434003353119 \n",
      "Epoch: 77 | Loss: 0.18011707067489624 \n",
      "Epoch: 78 | Loss: 0.17752860486507416 \n",
      "Epoch: 79 | Loss: 0.17497718334197998 \n",
      "Epoch: 80 | Loss: 0.17246249318122864 \n",
      "Epoch: 81 | Loss: 0.1699838936328888 \n",
      "Epoch: 82 | Loss: 0.16754105687141418 \n",
      "Epoch: 83 | Loss: 0.16513299942016602 \n",
      "Epoch: 84 | Loss: 0.16276009380817413 \n",
      "Epoch: 85 | Loss: 0.16042086482048035 \n",
      "Epoch: 86 | Loss: 0.15811532735824585 \n",
      "Epoch: 87 | Loss: 0.15584295988082886 \n",
      "Epoch: 88 | Loss: 0.15360337495803833 \n",
      "Epoch: 89 | Loss: 0.1513957381248474 \n",
      "Epoch: 90 | Loss: 0.14921985566616058 \n",
      "Epoch: 91 | Loss: 0.14707550406455994 \n",
      "Epoch: 92 | Loss: 0.14496158063411713 \n",
      "Epoch: 93 | Loss: 0.14287841320037842 \n",
      "Epoch: 94 | Loss: 0.1408250331878662 \n",
      "Epoch: 95 | Loss: 0.13880103826522827 \n",
      "Epoch: 96 | Loss: 0.13680635392665863 \n",
      "Epoch: 97 | Loss: 0.13484027981758118 \n",
      "Epoch: 98 | Loss: 0.13290250301361084 \n",
      "Epoch: 99 | Loss: 0.1309923231601715 \n",
      "Epoch: 100 | Loss: 0.12910966575145721 \n",
      "Epoch: 101 | Loss: 0.12725414335727692 \n",
      "Epoch: 102 | Loss: 0.12542545795440674 \n",
      "Epoch: 103 | Loss: 0.12362294644117355 \n",
      "Epoch: 104 | Loss: 0.12184611707925797 \n",
      "Epoch: 105 | Loss: 0.12009495496749878 \n",
      "Epoch: 106 | Loss: 0.11836911737918854 \n",
      "Epoch: 107 | Loss: 0.1166679635643959 \n",
      "Epoch: 108 | Loss: 0.114991195499897 \n",
      "Epoch: 109 | Loss: 0.11333876103162766 \n",
      "Epoch: 110 | Loss: 0.1117098480463028 \n",
      "Epoch: 111 | Loss: 0.11010456830263138 \n",
      "Epoch: 112 | Loss: 0.10852208733558655 \n",
      "Epoch: 113 | Loss: 0.10696239024400711 \n",
      "Epoch: 114 | Loss: 0.10542501509189606 \n",
      "Epoch: 115 | Loss: 0.10390995442867279 \n",
      "Epoch: 116 | Loss: 0.10241666436195374 \n",
      "Epoch: 117 | Loss: 0.10094485431909561 \n",
      "Epoch: 118 | Loss: 0.09949398785829544 \n",
      "Epoch: 119 | Loss: 0.09806409478187561 \n",
      "Epoch: 120 | Loss: 0.09665481001138687 \n",
      "Epoch: 121 | Loss: 0.09526579827070236 \n",
      "Epoch: 122 | Loss: 0.09389668703079224 \n",
      "Epoch: 123 | Loss: 0.09254713356494904 \n",
      "Epoch: 124 | Loss: 0.09121719002723694 \n",
      "Epoch: 125 | Loss: 0.0899062380194664 \n",
      "Epoch: 126 | Loss: 0.08861421048641205 \n",
      "Epoch: 127 | Loss: 0.08734052628278732 \n",
      "Epoch: 128 | Loss: 0.08608550578355789 \n",
      "Epoch: 129 | Loss: 0.08484818041324615 \n",
      "Epoch: 130 | Loss: 0.08362872898578644 \n",
      "Epoch: 131 | Loss: 0.08242692798376083 \n",
      "Epoch: 132 | Loss: 0.08124230802059174 \n",
      "Epoch: 133 | Loss: 0.08007469773292542 \n",
      "Epoch: 134 | Loss: 0.07892385870218277 \n",
      "Epoch: 135 | Loss: 0.0777895599603653 \n",
      "Epoch: 136 | Loss: 0.07667167484760284 \n",
      "Epoch: 137 | Loss: 0.07556977868080139 \n",
      "Epoch: 138 | Loss: 0.07448369264602661 \n",
      "Epoch: 139 | Loss: 0.07341329753398895 \n",
      "Epoch: 140 | Loss: 0.07235829532146454 \n",
      "Epoch: 141 | Loss: 0.07131833583116531 \n",
      "Epoch: 142 | Loss: 0.07029340416193008 \n",
      "Epoch: 143 | Loss: 0.06928320229053497 \n",
      "Epoch: 144 | Loss: 0.06828746944665909 \n",
      "Epoch: 145 | Loss: 0.06730592995882034 \n",
      "Epoch: 146 | Loss: 0.06633889675140381 \n",
      "Epoch: 147 | Loss: 0.0653853639960289 \n",
      "Epoch: 148 | Loss: 0.06444571167230606 \n",
      "Epoch: 149 | Loss: 0.0635194480419159 \n",
      "Epoch: 150 | Loss: 0.06260664016008377 \n",
      "Epoch: 151 | Loss: 0.061706751585006714 \n",
      "Epoch: 152 | Loss: 0.06081996113061905 \n",
      "Epoch: 153 | Loss: 0.05994604527950287 \n",
      "Epoch: 154 | Loss: 0.059084393084049225 \n",
      "Epoch: 155 | Loss: 0.058235280215740204 \n",
      "Epoch: 156 | Loss: 0.057398319244384766 \n",
      "Epoch: 157 | Loss: 0.056573428213596344 \n",
      "Epoch: 158 | Loss: 0.05576034635305405 \n",
      "Epoch: 159 | Loss: 0.05495898053050041 \n",
      "Epoch: 160 | Loss: 0.054169245064258575 \n",
      "Epoch: 161 | Loss: 0.053390659391880035 \n",
      "Epoch: 162 | Loss: 0.05262340232729912 \n",
      "Epoch: 163 | Loss: 0.051867127418518066 \n",
      "Epoch: 164 | Loss: 0.051121730357408524 \n",
      "Epoch: 165 | Loss: 0.05038697272539139 \n",
      "Epoch: 166 | Loss: 0.049662888050079346 \n",
      "Epoch: 167 | Loss: 0.04894905909895897 \n",
      "Epoch: 168 | Loss: 0.04824572056531906 \n",
      "Epoch: 169 | Loss: 0.04755229502916336 \n",
      "Epoch: 170 | Loss: 0.046868860721588135 \n",
      "Epoch: 171 | Loss: 0.046195369213819504 \n",
      "Epoch: 172 | Loss: 0.045531339943408966 \n",
      "Epoch: 173 | Loss: 0.04487704113125801 \n",
      "Epoch: 174 | Loss: 0.044232115149497986 \n",
      "Epoch: 175 | Loss: 0.043596379458904266 \n",
      "Epoch: 176 | Loss: 0.04296987131237984 \n",
      "Epoch: 177 | Loss: 0.04235224053263664 \n",
      "Epoch: 178 | Loss: 0.04174364358186722 \n",
      "Epoch: 179 | Loss: 0.041143812239170074 \n",
      "Epoch: 180 | Loss: 0.040552496910095215 \n",
      "Epoch: 181 | Loss: 0.03996959701180458 \n",
      "Epoch: 182 | Loss: 0.03939519450068474 \n",
      "Epoch: 183 | Loss: 0.03882904350757599 \n",
      "Epoch: 184 | Loss: 0.03827095031738281 \n",
      "Epoch: 185 | Loss: 0.0377209447324276 \n",
      "Epoch: 186 | Loss: 0.03717883676290512 \n",
      "Epoch: 187 | Loss: 0.036644503474235535 \n",
      "Epoch: 188 | Loss: 0.036117903888225555 \n",
      "Epoch: 189 | Loss: 0.035598836839199066 \n",
      "Epoch: 190 | Loss: 0.03508725017309189 \n",
      "Epoch: 191 | Loss: 0.03458291292190552 \n",
      "Epoch: 192 | Loss: 0.034085940569639206 \n",
      "Epoch: 193 | Loss: 0.03359612450003624 \n",
      "Epoch: 194 | Loss: 0.03311324864625931 \n",
      "Epoch: 195 | Loss: 0.032637350261211395 \n",
      "Epoch: 196 | Loss: 0.032168321311473846 \n",
      "Epoch: 197 | Loss: 0.031706005334854126 \n",
      "Epoch: 198 | Loss: 0.03125032037496567 \n",
      "Epoch: 199 | Loss: 0.03080122172832489 \n",
      "Epoch: 200 | Loss: 0.03035854361951351 \n",
      "Epoch: 201 | Loss: 0.029922189190983772 \n",
      "Epoch: 202 | Loss: 0.02949223853647709 \n",
      "Epoch: 203 | Loss: 0.029068291187286377 \n",
      "Epoch: 204 | Loss: 0.028650682419538498 \n",
      "Epoch: 205 | Loss: 0.028238840401172638 \n",
      "Epoch: 206 | Loss: 0.02783302776515484 \n",
      "Epoch: 207 | Loss: 0.027433037757873535 \n",
      "Epoch: 208 | Loss: 0.027038728818297386 \n",
      "Epoch: 209 | Loss: 0.02665017545223236 \n",
      "Epoch: 210 | Loss: 0.026267264038324356 \n",
      "Epoch: 211 | Loss: 0.025889629498124123 \n",
      "Epoch: 212 | Loss: 0.025517644360661507 \n",
      "Epoch: 213 | Loss: 0.025150852277874947 \n",
      "Epoch: 214 | Loss: 0.02478942461311817 \n",
      "Epoch: 215 | Loss: 0.024433182552456856 \n",
      "Epoch: 216 | Loss: 0.024081967771053314 \n",
      "Epoch: 217 | Loss: 0.023735908791422844 \n",
      "Epoch: 218 | Loss: 0.0233948715031147 \n",
      "Epoch: 219 | Loss: 0.023058634251356125 \n",
      "Epoch: 220 | Loss: 0.022727230563759804 \n",
      "Epoch: 221 | Loss: 0.02240055799484253 \n",
      "Epoch: 222 | Loss: 0.02207867242395878 \n",
      "Epoch: 223 | Loss: 0.021761324256658554 \n",
      "Epoch: 224 | Loss: 0.02144857868552208 \n",
      "Epoch: 225 | Loss: 0.021140366792678833 \n",
      "Epoch: 226 | Loss: 0.020836517214775085 \n",
      "Epoch: 227 | Loss: 0.020537011325359344 \n",
      "Epoch: 228 | Loss: 0.02024191990494728 \n",
      "Epoch: 229 | Loss: 0.01995101198554039 \n",
      "Epoch: 230 | Loss: 0.019664287567138672 \n",
      "Epoch: 231 | Loss: 0.01938173547387123 \n",
      "Epoch: 232 | Loss: 0.019103115424513817 \n",
      "Epoch: 233 | Loss: 0.018828580155968666 \n",
      "Epoch: 234 | Loss: 0.018557971343398094 \n",
      "Epoch: 235 | Loss: 0.0182912889868021 \n",
      "Epoch: 236 | Loss: 0.01802844926714897 \n",
      "Epoch: 237 | Loss: 0.017769306898117065 \n",
      "Epoch: 238 | Loss: 0.017513982951641083 \n",
      "Epoch: 239 | Loss: 0.01726224645972252 \n",
      "Epoch: 240 | Loss: 0.01701413094997406 \n",
      "Epoch: 241 | Loss: 0.01676960289478302 \n",
      "Epoch: 242 | Loss: 0.016528580337762833 \n",
      "Epoch: 243 | Loss: 0.016291089355945587 \n",
      "Epoch: 244 | Loss: 0.016056900843977928 \n",
      "Epoch: 245 | Loss: 0.01582622341811657 \n",
      "Epoch: 246 | Loss: 0.015598739497363567 \n",
      "Epoch: 247 | Loss: 0.015374545007944107 \n",
      "Epoch: 248 | Loss: 0.015153605490922928 \n",
      "Epoch: 249 | Loss: 0.014935794286429882 \n",
      "Epoch: 250 | Loss: 0.014721188694238663 \n",
      "Epoch: 251 | Loss: 0.01450960710644722 \n",
      "Epoch: 252 | Loss: 0.014301113784313202 \n",
      "Epoch: 253 | Loss: 0.014095510356128216 \n",
      "Epoch: 254 | Loss: 0.01389294397085905 \n",
      "Epoch: 255 | Loss: 0.01369333453476429 \n",
      "Epoch: 256 | Loss: 0.013496466912329197 \n",
      "Epoch: 257 | Loss: 0.013302579522132874 \n",
      "Epoch: 258 | Loss: 0.013111373409628868 \n",
      "Epoch: 259 | Loss: 0.01292295940220356 \n",
      "Epoch: 260 | Loss: 0.012737215496599674 \n",
      "Epoch: 261 | Loss: 0.01255413144826889 \n",
      "Epoch: 262 | Loss: 0.012373744510114193 \n",
      "Epoch: 263 | Loss: 0.012195946648716927 \n",
      "Epoch: 264 | Loss: 0.012020671740174294 \n",
      "Epoch: 265 | Loss: 0.011847893707454205 \n",
      "Epoch: 266 | Loss: 0.011677641421556473 \n",
      "Epoch: 267 | Loss: 0.01150977611541748 \n",
      "Epoch: 268 | Loss: 0.011344414204359055 \n",
      "Epoch: 269 | Loss: 0.011181319132447243 \n",
      "Epoch: 270 | Loss: 0.011020606383681297 \n",
      "Epoch: 271 | Loss: 0.010862245224416256 \n",
      "Epoch: 272 | Loss: 0.01070612482726574 \n",
      "Epoch: 273 | Loss: 0.010552257299423218 \n",
      "Epoch: 274 | Loss: 0.010400607250630856 \n",
      "Epoch: 275 | Loss: 0.010251159779727459 \n",
      "Epoch: 276 | Loss: 0.010103830136358738 \n",
      "Epoch: 277 | Loss: 0.009958605282008648 \n",
      "Epoch: 278 | Loss: 0.009815525263547897 \n",
      "Epoch: 279 | Loss: 0.009674401953816414 \n",
      "Epoch: 280 | Loss: 0.009535424411296844 \n",
      "Epoch: 281 | Loss: 0.009398359805345535 \n",
      "Epoch: 282 | Loss: 0.009263250976800919 \n",
      "Epoch: 283 | Loss: 0.009130174294114113 \n",
      "Epoch: 284 | Loss: 0.008998955599963665 \n",
      "Epoch: 285 | Loss: 0.008869627490639687 \n",
      "Epoch: 286 | Loss: 0.008742171339690685 \n",
      "Epoch: 287 | Loss: 0.008616527542471886 \n",
      "Epoch: 288 | Loss: 0.008492671884596348 \n",
      "Epoch: 289 | Loss: 0.008370617404580116 \n",
      "Epoch: 290 | Loss: 0.008250328712165356 \n",
      "Epoch: 291 | Loss: 0.008131780661642551 \n",
      "Epoch: 292 | Loss: 0.008014892227947712 \n",
      "Epoch: 293 | Loss: 0.007899702526628971 \n",
      "Epoch: 294 | Loss: 0.007786182686686516 \n",
      "Epoch: 295 | Loss: 0.007674294523894787 \n",
      "Epoch: 296 | Loss: 0.0075639598071575165 \n",
      "Epoch: 297 | Loss: 0.00745526235550642 \n",
      "Epoch: 298 | Loss: 0.007348128594458103 \n",
      "Epoch: 299 | Loss: 0.007242502644658089 \n",
      "Epoch: 300 | Loss: 0.007138418033719063 \n",
      "Epoch: 301 | Loss: 0.007035822607576847 \n",
      "Epoch: 302 | Loss: 0.006934715900570154 \n",
      "Epoch: 303 | Loss: 0.006835076026618481 \n",
      "Epoch: 304 | Loss: 0.00673686433583498 \n",
      "Epoch: 305 | Loss: 0.006639988161623478 \n",
      "Epoch: 306 | Loss: 0.006544608622789383 \n",
      "Epoch: 307 | Loss: 0.006450535263866186 \n",
      "Epoch: 308 | Loss: 0.006357833743095398 \n",
      "Epoch: 309 | Loss: 0.006266457028687 \n",
      "Epoch: 310 | Loss: 0.00617639534175396 \n",
      "Epoch: 311 | Loss: 0.006087624467909336 \n",
      "Epoch: 312 | Loss: 0.0060001276433467865 \n",
      "Epoch: 313 | Loss: 0.005913885310292244 \n",
      "Epoch: 314 | Loss: 0.005828888155519962 \n",
      "Epoch: 315 | Loss: 0.005745124537497759 \n",
      "Epoch: 316 | Loss: 0.005662566050887108 \n",
      "Epoch: 317 | Loss: 0.005581190809607506 \n",
      "Epoch: 318 | Loss: 0.005500969476997852 \n",
      "Epoch: 319 | Loss: 0.00542192067950964 \n",
      "Epoch: 320 | Loss: 0.005344012752175331 \n",
      "Epoch: 321 | Loss: 0.005267204716801643 \n",
      "Epoch: 322 | Loss: 0.0051914723590016365 \n",
      "Epoch: 323 | Loss: 0.00511689530685544 \n",
      "Epoch: 324 | Loss: 0.005043316632509232 \n",
      "Epoch: 325 | Loss: 0.0049708690494298935 \n",
      "Epoch: 326 | Loss: 0.004899450112134218 \n",
      "Epoch: 327 | Loss: 0.004829052835702896 \n",
      "Epoch: 328 | Loss: 0.004759622737765312 \n",
      "Epoch: 329 | Loss: 0.004691196605563164 \n",
      "Epoch: 330 | Loss: 0.004623765125870705 \n",
      "Epoch: 331 | Loss: 0.004557346925139427 \n",
      "Epoch: 332 | Loss: 0.004491849802434444 \n",
      "Epoch: 333 | Loss: 0.004427287261933088 \n",
      "Epoch: 334 | Loss: 0.004363660700619221 \n",
      "Epoch: 335 | Loss: 0.004300940316170454 \n",
      "Epoch: 336 | Loss: 0.004239128902554512 \n",
      "Epoch: 337 | Loss: 0.004178192932158709 \n",
      "Epoch: 338 | Loss: 0.004118186887353659 \n",
      "Epoch: 339 | Loss: 0.004058988764882088 \n",
      "Epoch: 340 | Loss: 0.0040006497874855995 \n",
      "Epoch: 341 | Loss: 0.003943146206438541 \n",
      "Epoch: 342 | Loss: 0.003886493155732751 \n",
      "Epoch: 343 | Loss: 0.0038306498900055885 \n",
      "Epoch: 344 | Loss: 0.003775589168071747 \n",
      "Epoch: 345 | Loss: 0.003721296088770032 \n",
      "Epoch: 346 | Loss: 0.0036678363103419542 \n",
      "Epoch: 347 | Loss: 0.003615105990320444 \n",
      "Epoch: 348 | Loss: 0.003563180798664689 \n",
      "Epoch: 349 | Loss: 0.0035119380336254835 \n",
      "Epoch: 350 | Loss: 0.0034615176264196634 \n",
      "Epoch: 351 | Loss: 0.0034117542672902346 \n",
      "Epoch: 352 | Loss: 0.0033627119846642017 \n",
      "Epoch: 353 | Loss: 0.003314389381557703 \n",
      "Epoch: 354 | Loss: 0.0032667554914951324 \n",
      "Epoch: 355 | Loss: 0.0032197930850088596 \n",
      "Epoch: 356 | Loss: 0.003173540811985731 \n",
      "Epoch: 357 | Loss: 0.003127909265458584 \n",
      "Epoch: 358 | Loss: 0.0030829699244350195 \n",
      "Epoch: 359 | Loss: 0.003038666443899274 \n",
      "Epoch: 360 | Loss: 0.0029949923045933247 \n",
      "Epoch: 361 | Loss: 0.0029519377276301384 \n",
      "Epoch: 362 | Loss: 0.0029095332138240337 \n",
      "Epoch: 363 | Loss: 0.002867703791707754 \n",
      "Epoch: 364 | Loss: 0.0028264799620956182 \n",
      "Epoch: 365 | Loss: 0.0027859043329954147 \n",
      "Epoch: 366 | Loss: 0.0027458230033516884 \n",
      "Epoch: 367 | Loss: 0.002706374041736126 \n",
      "Epoch: 368 | Loss: 0.002667481079697609 \n",
      "Epoch: 369 | Loss: 0.002629158552736044 \n",
      "Epoch: 370 | Loss: 0.002591376891359687 \n",
      "Epoch: 371 | Loss: 0.002554109552875161 \n",
      "Epoch: 372 | Loss: 0.0025173949543386698 \n",
      "Epoch: 373 | Loss: 0.00248122145421803 \n",
      "Epoch: 374 | Loss: 0.002445574151352048 \n",
      "Epoch: 375 | Loss: 0.0024104416370391846 \n",
      "Epoch: 376 | Loss: 0.0023757817689329386 \n",
      "Epoch: 377 | Loss: 0.0023416317999362946 \n",
      "Epoch: 378 | Loss: 0.0023079884704202414 \n",
      "Epoch: 379 | Loss: 0.0022748135961592197 \n",
      "Epoch: 380 | Loss: 0.002242129296064377 \n",
      "Epoch: 381 | Loss: 0.0022099055349826813 \n",
      "Epoch: 382 | Loss: 0.00217813765630126 \n",
      "Epoch: 383 | Loss: 0.0021468340419232845 \n",
      "Epoch: 384 | Loss: 0.0021159842144697905 \n",
      "Epoch: 385 | Loss: 0.002085599582642317 \n",
      "Epoch: 386 | Loss: 0.002055603079497814 \n",
      "Epoch: 387 | Loss: 0.0020260598976165056 \n",
      "Epoch: 388 | Loss: 0.001996951177716255 \n",
      "Epoch: 389 | Loss: 0.001968259224668145 \n",
      "Epoch: 390 | Loss: 0.0019399671582505107 \n",
      "Epoch: 391 | Loss: 0.0019120679935440421 \n",
      "Epoch: 392 | Loss: 0.0018846006132662296 \n",
      "Epoch: 393 | Loss: 0.0018575023859739304 \n",
      "Epoch: 394 | Loss: 0.0018308168509975076 \n",
      "Epoch: 395 | Loss: 0.0018044973257929087 \n",
      "Epoch: 396 | Loss: 0.0017785840900614858 \n",
      "Epoch: 397 | Loss: 0.0017530213808640838 \n",
      "Epoch: 398 | Loss: 0.0017278080340474844 \n",
      "Epoch: 399 | Loss: 0.0017030054004862905 \n",
      "Epoch: 400 | Loss: 0.0016785128973424435 \n",
      "Epoch: 401 | Loss: 0.0016544059617444873 \n",
      "Epoch: 402 | Loss: 0.0016306191682815552 \n",
      "Epoch: 403 | Loss: 0.0016071931459009647 \n",
      "Epoch: 404 | Loss: 0.0015840756241232157 \n",
      "Epoch: 405 | Loss: 0.0015613322611898184 \n",
      "Epoch: 406 | Loss: 0.0015388673637062311 \n",
      "Epoch: 407 | Loss: 0.0015167533420026302 \n",
      "Epoch: 408 | Loss: 0.0014949522446841002 \n",
      "Epoch: 409 | Loss: 0.0014734941069036722 \n",
      "Epoch: 410 | Loss: 0.001452308613806963 \n",
      "Epoch: 411 | Loss: 0.0014314173022285104 \n",
      "Epoch: 412 | Loss: 0.0014108497416600585 \n",
      "Epoch: 413 | Loss: 0.0013905721716582775 \n",
      "Epoch: 414 | Loss: 0.0013705812161788344 \n",
      "Epoch: 415 | Loss: 0.0013509038835763931 \n",
      "Epoch: 416 | Loss: 0.001331472652964294 \n",
      "Epoch: 417 | Loss: 0.0013123424723744392 \n",
      "Epoch: 418 | Loss: 0.001293488428927958 \n",
      "Epoch: 419 | Loss: 0.0012748955050483346 \n",
      "Epoch: 420 | Loss: 0.0012565834913402796 \n",
      "Epoch: 421 | Loss: 0.001238520722836256 \n",
      "Epoch: 422 | Loss: 0.001220708480104804 \n",
      "Epoch: 423 | Loss: 0.0012031765654683113 \n",
      "Epoch: 424 | Loss: 0.0011858774814754725 \n",
      "Epoch: 425 | Loss: 0.001168832997791469 \n",
      "Epoch: 426 | Loss: 0.0011520478874444962 \n",
      "Epoch: 427 | Loss: 0.0011354724410921335 \n",
      "Epoch: 428 | Loss: 0.0011191649828106165 \n",
      "Epoch: 429 | Loss: 0.0011030720779672265 \n",
      "Epoch: 430 | Loss: 0.0010872279526665807 \n",
      "Epoch: 431 | Loss: 0.0010716125834733248 \n",
      "Epoch: 432 | Loss: 0.0010562199167907238 \n",
      "Epoch: 433 | Loss: 0.0010410285321995616 \n",
      "Epoch: 434 | Loss: 0.0010260645067319274 \n",
      "Epoch: 435 | Loss: 0.0010113087482750416 \n",
      "Epoch: 436 | Loss: 0.0009967845398932695 \n",
      "Epoch: 437 | Loss: 0.0009824620792642236 \n",
      "Epoch: 438 | Loss: 0.0009683499811217189 \n",
      "Epoch: 439 | Loss: 0.0009544154745526612 \n",
      "Epoch: 440 | Loss: 0.0009406960452906787 \n",
      "Epoch: 441 | Loss: 0.0009272015886381269 \n",
      "Epoch: 442 | Loss: 0.0009138609748333693 \n",
      "Epoch: 443 | Loss: 0.0009007252519950271 \n",
      "Epoch: 444 | Loss: 0.0008877797517925501 \n",
      "Epoch: 445 | Loss: 0.0008750382112339139 \n",
      "Epoch: 446 | Loss: 0.0008624620968475938 \n",
      "Epoch: 447 | Loss: 0.0008500668918713927 \n",
      "Epoch: 448 | Loss: 0.0008378337370231748 \n",
      "Epoch: 449 | Loss: 0.0008258061134256423 \n",
      "Epoch: 450 | Loss: 0.0008139219717122614 \n",
      "Epoch: 451 | Loss: 0.0008022392867133021 \n",
      "Epoch: 452 | Loss: 0.0007907145190984011 \n",
      "Epoch: 453 | Loss: 0.0007793430122546852 \n",
      "Epoch: 454 | Loss: 0.0007681375718675554 \n",
      "Epoch: 455 | Loss: 0.0007570981979370117 \n",
      "Epoch: 456 | Loss: 0.0007462227949872613 \n",
      "Epoch: 457 | Loss: 0.0007354954723268747 \n",
      "Epoch: 458 | Loss: 0.0007249288028106093 \n",
      "Epoch: 459 | Loss: 0.0007145069539546967 \n",
      "Epoch: 460 | Loss: 0.0007042454672046006 \n",
      "Epoch: 461 | Loss: 0.0006941102328710258 \n",
      "Epoch: 462 | Loss: 0.0006841412978246808 \n",
      "Epoch: 463 | Loss: 0.0006743139238096774 \n",
      "Epoch: 464 | Loss: 0.0006646235706284642 \n",
      "Epoch: 465 | Loss: 0.0006550673278979957 \n",
      "Epoch: 466 | Loss: 0.0006456482806243002 \n",
      "Epoch: 467 | Loss: 0.0006363766733556986 \n",
      "Epoch: 468 | Loss: 0.0006272360915318131 \n",
      "Epoch: 469 | Loss: 0.0006182267679832876 \n",
      "Epoch: 470 | Loss: 0.0006093342090025544 \n",
      "Epoch: 471 | Loss: 0.0006005688919685781 \n",
      "Epoch: 472 | Loss: 0.0005919433897361159 \n",
      "Epoch: 473 | Loss: 0.0005834336625412107 \n",
      "Epoch: 474 | Loss: 0.00057505804579705 \n",
      "Epoch: 475 | Loss: 0.0005667955847457051 \n",
      "Epoch: 476 | Loss: 0.0005586423212662339 \n",
      "Epoch: 477 | Loss: 0.0005506178131327033 \n",
      "Epoch: 478 | Loss: 0.0005427041323855519 \n",
      "Epoch: 479 | Loss: 0.0005349002312868834 \n",
      "Epoch: 480 | Loss: 0.0005272208945825696 \n",
      "Epoch: 481 | Loss: 0.0005196355050429702 \n",
      "Epoch: 482 | Loss: 0.0005121748545207083 \n",
      "Epoch: 483 | Loss: 0.000504805997479707 \n",
      "Epoch: 484 | Loss: 0.0004975489573553205 \n",
      "Epoch: 485 | Loss: 0.0004903933149762452 \n",
      "Epoch: 486 | Loss: 0.00048334835446439683 \n",
      "Epoch: 487 | Loss: 0.0004764089244417846 \n",
      "Epoch: 488 | Loss: 0.000469556194730103 \n",
      "Epoch: 489 | Loss: 0.0004628119058907032 \n",
      "Epoch: 490 | Loss: 0.0004561636014841497 \n",
      "Epoch: 491 | Loss: 0.00044960531522519886 \n",
      "Epoch: 492 | Loss: 0.000443142227595672 \n",
      "Epoch: 493 | Loss: 0.0004367721267044544 \n",
      "Epoch: 494 | Loss: 0.0004304928006604314 \n",
      "Epoch: 495 | Loss: 0.00042431519250385463 \n",
      "Epoch: 496 | Loss: 0.00041821555350907147 \n",
      "Epoch: 497 | Loss: 0.000412204914027825 \n",
      "Epoch: 498 | Loss: 0.00040627882117405534 \n",
      "Epoch: 499 | Loss: 0.00040044100023806095 \n",
      "Prediction (after training) 4 7.976996898651123\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "x_data = tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # 2) Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# After training\n",
    "hour_var = tensor([[4.0]])\n",
    "y_pred = model(hour_var)\n",
    "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce972d61-38b5-47af-9e2f-39562068fd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 | Loss: 1.5629\n",
      "Epoch 2/1000 | Loss: 1.5427\n",
      "Epoch 3/1000 | Loss: 1.5229\n",
      "Epoch 4/1000 | Loss: 1.5033\n",
      "Epoch 5/1000 | Loss: 1.4841\n",
      "Epoch 6/1000 | Loss: 1.4651\n",
      "Epoch 7/1000 | Loss: 1.4465\n",
      "Epoch 8/1000 | Loss: 1.4282\n",
      "Epoch 9/1000 | Loss: 1.4102\n",
      "Epoch 10/1000 | Loss: 1.3925\n",
      "Epoch 11/1000 | Loss: 1.3751\n",
      "Epoch 12/1000 | Loss: 1.3581\n",
      "Epoch 13/1000 | Loss: 1.3413\n",
      "Epoch 14/1000 | Loss: 1.3249\n",
      "Epoch 15/1000 | Loss: 1.3089\n",
      "Epoch 16/1000 | Loss: 1.2931\n",
      "Epoch 17/1000 | Loss: 1.2777\n",
      "Epoch 18/1000 | Loss: 1.2627\n",
      "Epoch 19/1000 | Loss: 1.2479\n",
      "Epoch 20/1000 | Loss: 1.2335\n",
      "Epoch 21/1000 | Loss: 1.2195\n",
      "Epoch 22/1000 | Loss: 1.2057\n",
      "Epoch 23/1000 | Loss: 1.1923\n",
      "Epoch 24/1000 | Loss: 1.1793\n",
      "Epoch 25/1000 | Loss: 1.1665\n",
      "Epoch 26/1000 | Loss: 1.1541\n",
      "Epoch 27/1000 | Loss: 1.1420\n",
      "Epoch 28/1000 | Loss: 1.1302\n",
      "Epoch 29/1000 | Loss: 1.1188\n",
      "Epoch 30/1000 | Loss: 1.1076\n",
      "Epoch 31/1000 | Loss: 1.0968\n",
      "Epoch 32/1000 | Loss: 1.0863\n",
      "Epoch 33/1000 | Loss: 1.0761\n",
      "Epoch 34/1000 | Loss: 1.0662\n",
      "Epoch 35/1000 | Loss: 1.0566\n",
      "Epoch 36/1000 | Loss: 1.0473\n",
      "Epoch 37/1000 | Loss: 1.0382\n",
      "Epoch 38/1000 | Loss: 1.0295\n",
      "Epoch 39/1000 | Loss: 1.0210\n",
      "Epoch 40/1000 | Loss: 1.0128\n",
      "Epoch 41/1000 | Loss: 1.0048\n",
      "Epoch 42/1000 | Loss: 0.9971\n",
      "Epoch 43/1000 | Loss: 0.9897\n",
      "Epoch 44/1000 | Loss: 0.9825\n",
      "Epoch 45/1000 | Loss: 0.9755\n",
      "Epoch 46/1000 | Loss: 0.9688\n",
      "Epoch 47/1000 | Loss: 0.9623\n",
      "Epoch 48/1000 | Loss: 0.9560\n",
      "Epoch 49/1000 | Loss: 0.9500\n",
      "Epoch 50/1000 | Loss: 0.9441\n",
      "Epoch 51/1000 | Loss: 0.9385\n",
      "Epoch 52/1000 | Loss: 0.9330\n",
      "Epoch 53/1000 | Loss: 0.9278\n",
      "Epoch 54/1000 | Loss: 0.9227\n",
      "Epoch 55/1000 | Loss: 0.9178\n",
      "Epoch 56/1000 | Loss: 0.9131\n",
      "Epoch 57/1000 | Loss: 0.9086\n",
      "Epoch 58/1000 | Loss: 0.9042\n",
      "Epoch 59/1000 | Loss: 0.8999\n",
      "Epoch 60/1000 | Loss: 0.8958\n",
      "Epoch 61/1000 | Loss: 0.8919\n",
      "Epoch 62/1000 | Loss: 0.8881\n",
      "Epoch 63/1000 | Loss: 0.8844\n",
      "Epoch 64/1000 | Loss: 0.8809\n",
      "Epoch 65/1000 | Loss: 0.8775\n",
      "Epoch 66/1000 | Loss: 0.8742\n",
      "Epoch 67/1000 | Loss: 0.8711\n",
      "Epoch 68/1000 | Loss: 0.8680\n",
      "Epoch 69/1000 | Loss: 0.8651\n",
      "Epoch 70/1000 | Loss: 0.8622\n",
      "Epoch 71/1000 | Loss: 0.8595\n",
      "Epoch 72/1000 | Loss: 0.8568\n",
      "Epoch 73/1000 | Loss: 0.8543\n",
      "Epoch 74/1000 | Loss: 0.8518\n",
      "Epoch 75/1000 | Loss: 0.8494\n",
      "Epoch 76/1000 | Loss: 0.8471\n",
      "Epoch 77/1000 | Loss: 0.8449\n",
      "Epoch 78/1000 | Loss: 0.8428\n",
      "Epoch 79/1000 | Loss: 0.8407\n",
      "Epoch 80/1000 | Loss: 0.8387\n",
      "Epoch 81/1000 | Loss: 0.8367\n",
      "Epoch 82/1000 | Loss: 0.8349\n",
      "Epoch 83/1000 | Loss: 0.8331\n",
      "Epoch 84/1000 | Loss: 0.8313\n",
      "Epoch 85/1000 | Loss: 0.8296\n",
      "Epoch 86/1000 | Loss: 0.8280\n",
      "Epoch 87/1000 | Loss: 0.8264\n",
      "Epoch 88/1000 | Loss: 0.8248\n",
      "Epoch 89/1000 | Loss: 0.8233\n",
      "Epoch 90/1000 | Loss: 0.8219\n",
      "Epoch 91/1000 | Loss: 0.8205\n",
      "Epoch 92/1000 | Loss: 0.8191\n",
      "Epoch 93/1000 | Loss: 0.8178\n",
      "Epoch 94/1000 | Loss: 0.8165\n",
      "Epoch 95/1000 | Loss: 0.8153\n",
      "Epoch 96/1000 | Loss: 0.8141\n",
      "Epoch 97/1000 | Loss: 0.8129\n",
      "Epoch 98/1000 | Loss: 0.8118\n",
      "Epoch 99/1000 | Loss: 0.8107\n",
      "Epoch 100/1000 | Loss: 0.8096\n",
      "Epoch 101/1000 | Loss: 0.8085\n",
      "Epoch 102/1000 | Loss: 0.8075\n",
      "Epoch 103/1000 | Loss: 0.8065\n",
      "Epoch 104/1000 | Loss: 0.8056\n",
      "Epoch 105/1000 | Loss: 0.8046\n",
      "Epoch 106/1000 | Loss: 0.8037\n",
      "Epoch 107/1000 | Loss: 0.8028\n",
      "Epoch 108/1000 | Loss: 0.8019\n",
      "Epoch 109/1000 | Loss: 0.8011\n",
      "Epoch 110/1000 | Loss: 0.8002\n",
      "Epoch 111/1000 | Loss: 0.7994\n",
      "Epoch 112/1000 | Loss: 0.7986\n",
      "Epoch 113/1000 | Loss: 0.7978\n",
      "Epoch 114/1000 | Loss: 0.7971\n",
      "Epoch 115/1000 | Loss: 0.7963\n",
      "Epoch 116/1000 | Loss: 0.7956\n",
      "Epoch 117/1000 | Loss: 0.7949\n",
      "Epoch 118/1000 | Loss: 0.7942\n",
      "Epoch 119/1000 | Loss: 0.7935\n",
      "Epoch 120/1000 | Loss: 0.7928\n",
      "Epoch 121/1000 | Loss: 0.7921\n",
      "Epoch 122/1000 | Loss: 0.7915\n",
      "Epoch 123/1000 | Loss: 0.7908\n",
      "Epoch 124/1000 | Loss: 0.7902\n",
      "Epoch 125/1000 | Loss: 0.7896\n",
      "Epoch 126/1000 | Loss: 0.7890\n",
      "Epoch 127/1000 | Loss: 0.7884\n",
      "Epoch 128/1000 | Loss: 0.7878\n",
      "Epoch 129/1000 | Loss: 0.7872\n",
      "Epoch 130/1000 | Loss: 0.7867\n",
      "Epoch 131/1000 | Loss: 0.7861\n",
      "Epoch 132/1000 | Loss: 0.7855\n",
      "Epoch 133/1000 | Loss: 0.7850\n",
      "Epoch 134/1000 | Loss: 0.7845\n",
      "Epoch 135/1000 | Loss: 0.7839\n",
      "Epoch 136/1000 | Loss: 0.7834\n",
      "Epoch 137/1000 | Loss: 0.7829\n",
      "Epoch 138/1000 | Loss: 0.7824\n",
      "Epoch 139/1000 | Loss: 0.7819\n",
      "Epoch 140/1000 | Loss: 0.7814\n",
      "Epoch 141/1000 | Loss: 0.7809\n",
      "Epoch 142/1000 | Loss: 0.7804\n",
      "Epoch 143/1000 | Loss: 0.7799\n",
      "Epoch 144/1000 | Loss: 0.7794\n",
      "Epoch 145/1000 | Loss: 0.7790\n",
      "Epoch 146/1000 | Loss: 0.7785\n",
      "Epoch 147/1000 | Loss: 0.7780\n",
      "Epoch 148/1000 | Loss: 0.7776\n",
      "Epoch 149/1000 | Loss: 0.7771\n",
      "Epoch 150/1000 | Loss: 0.7767\n",
      "Epoch 151/1000 | Loss: 0.7762\n",
      "Epoch 152/1000 | Loss: 0.7758\n",
      "Epoch 153/1000 | Loss: 0.7753\n",
      "Epoch 154/1000 | Loss: 0.7749\n",
      "Epoch 155/1000 | Loss: 0.7745\n",
      "Epoch 156/1000 | Loss: 0.7740\n",
      "Epoch 157/1000 | Loss: 0.7736\n",
      "Epoch 158/1000 | Loss: 0.7732\n",
      "Epoch 159/1000 | Loss: 0.7728\n",
      "Epoch 160/1000 | Loss: 0.7723\n",
      "Epoch 161/1000 | Loss: 0.7719\n",
      "Epoch 162/1000 | Loss: 0.7715\n",
      "Epoch 163/1000 | Loss: 0.7711\n",
      "Epoch 164/1000 | Loss: 0.7707\n",
      "Epoch 165/1000 | Loss: 0.7703\n",
      "Epoch 166/1000 | Loss: 0.7699\n",
      "Epoch 167/1000 | Loss: 0.7695\n",
      "Epoch 168/1000 | Loss: 0.7691\n",
      "Epoch 169/1000 | Loss: 0.7687\n",
      "Epoch 170/1000 | Loss: 0.7683\n",
      "Epoch 171/1000 | Loss: 0.7679\n",
      "Epoch 172/1000 | Loss: 0.7675\n",
      "Epoch 173/1000 | Loss: 0.7671\n",
      "Epoch 174/1000 | Loss: 0.7667\n",
      "Epoch 175/1000 | Loss: 0.7664\n",
      "Epoch 176/1000 | Loss: 0.7660\n",
      "Epoch 177/1000 | Loss: 0.7656\n",
      "Epoch 178/1000 | Loss: 0.7652\n",
      "Epoch 179/1000 | Loss: 0.7648\n",
      "Epoch 180/1000 | Loss: 0.7644\n",
      "Epoch 181/1000 | Loss: 0.7641\n",
      "Epoch 182/1000 | Loss: 0.7637\n",
      "Epoch 183/1000 | Loss: 0.7633\n",
      "Epoch 184/1000 | Loss: 0.7629\n",
      "Epoch 185/1000 | Loss: 0.7626\n",
      "Epoch 186/1000 | Loss: 0.7622\n",
      "Epoch 187/1000 | Loss: 0.7618\n",
      "Epoch 188/1000 | Loss: 0.7615\n",
      "Epoch 189/1000 | Loss: 0.7611\n",
      "Epoch 190/1000 | Loss: 0.7607\n",
      "Epoch 191/1000 | Loss: 0.7604\n",
      "Epoch 192/1000 | Loss: 0.7600\n",
      "Epoch 193/1000 | Loss: 0.7596\n",
      "Epoch 194/1000 | Loss: 0.7593\n",
      "Epoch 195/1000 | Loss: 0.7589\n",
      "Epoch 196/1000 | Loss: 0.7585\n",
      "Epoch 197/1000 | Loss: 0.7582\n",
      "Epoch 198/1000 | Loss: 0.7578\n",
      "Epoch 199/1000 | Loss: 0.7575\n",
      "Epoch 200/1000 | Loss: 0.7571\n",
      "Epoch 201/1000 | Loss: 0.7568\n",
      "Epoch 202/1000 | Loss: 0.7564\n",
      "Epoch 203/1000 | Loss: 0.7560\n",
      "Epoch 204/1000 | Loss: 0.7557\n",
      "Epoch 205/1000 | Loss: 0.7553\n",
      "Epoch 206/1000 | Loss: 0.7550\n",
      "Epoch 207/1000 | Loss: 0.7546\n",
      "Epoch 208/1000 | Loss: 0.7543\n",
      "Epoch 209/1000 | Loss: 0.7539\n",
      "Epoch 210/1000 | Loss: 0.7536\n",
      "Epoch 211/1000 | Loss: 0.7532\n",
      "Epoch 212/1000 | Loss: 0.7529\n",
      "Epoch 213/1000 | Loss: 0.7525\n",
      "Epoch 214/1000 | Loss: 0.7522\n",
      "Epoch 215/1000 | Loss: 0.7518\n",
      "Epoch 216/1000 | Loss: 0.7515\n",
      "Epoch 217/1000 | Loss: 0.7511\n",
      "Epoch 218/1000 | Loss: 0.7508\n",
      "Epoch 219/1000 | Loss: 0.7504\n",
      "Epoch 220/1000 | Loss: 0.7501\n",
      "Epoch 221/1000 | Loss: 0.7497\n",
      "Epoch 222/1000 | Loss: 0.7494\n",
      "Epoch 223/1000 | Loss: 0.7490\n",
      "Epoch 224/1000 | Loss: 0.7487\n",
      "Epoch 225/1000 | Loss: 0.7484\n",
      "Epoch 226/1000 | Loss: 0.7480\n",
      "Epoch 227/1000 | Loss: 0.7477\n",
      "Epoch 228/1000 | Loss: 0.7473\n",
      "Epoch 229/1000 | Loss: 0.7470\n",
      "Epoch 230/1000 | Loss: 0.7466\n",
      "Epoch 231/1000 | Loss: 0.7463\n",
      "Epoch 232/1000 | Loss: 0.7460\n",
      "Epoch 233/1000 | Loss: 0.7456\n",
      "Epoch 234/1000 | Loss: 0.7453\n",
      "Epoch 235/1000 | Loss: 0.7449\n",
      "Epoch 236/1000 | Loss: 0.7446\n",
      "Epoch 237/1000 | Loss: 0.7443\n",
      "Epoch 238/1000 | Loss: 0.7439\n",
      "Epoch 239/1000 | Loss: 0.7436\n",
      "Epoch 240/1000 | Loss: 0.7432\n",
      "Epoch 241/1000 | Loss: 0.7429\n",
      "Epoch 242/1000 | Loss: 0.7426\n",
      "Epoch 243/1000 | Loss: 0.7422\n",
      "Epoch 244/1000 | Loss: 0.7419\n",
      "Epoch 245/1000 | Loss: 0.7416\n",
      "Epoch 246/1000 | Loss: 0.7412\n",
      "Epoch 247/1000 | Loss: 0.7409\n",
      "Epoch 248/1000 | Loss: 0.7406\n",
      "Epoch 249/1000 | Loss: 0.7402\n",
      "Epoch 250/1000 | Loss: 0.7399\n",
      "Epoch 251/1000 | Loss: 0.7395\n",
      "Epoch 252/1000 | Loss: 0.7392\n",
      "Epoch 253/1000 | Loss: 0.7389\n",
      "Epoch 254/1000 | Loss: 0.7385\n",
      "Epoch 255/1000 | Loss: 0.7382\n",
      "Epoch 256/1000 | Loss: 0.7379\n",
      "Epoch 257/1000 | Loss: 0.7375\n",
      "Epoch 258/1000 | Loss: 0.7372\n",
      "Epoch 259/1000 | Loss: 0.7369\n",
      "Epoch 260/1000 | Loss: 0.7365\n",
      "Epoch 261/1000 | Loss: 0.7362\n",
      "Epoch 262/1000 | Loss: 0.7359\n",
      "Epoch 263/1000 | Loss: 0.7355\n",
      "Epoch 264/1000 | Loss: 0.7352\n",
      "Epoch 265/1000 | Loss: 0.7349\n",
      "Epoch 266/1000 | Loss: 0.7346\n",
      "Epoch 267/1000 | Loss: 0.7342\n",
      "Epoch 268/1000 | Loss: 0.7339\n",
      "Epoch 269/1000 | Loss: 0.7336\n",
      "Epoch 270/1000 | Loss: 0.7332\n",
      "Epoch 271/1000 | Loss: 0.7329\n",
      "Epoch 272/1000 | Loss: 0.7326\n",
      "Epoch 273/1000 | Loss: 0.7322\n",
      "Epoch 274/1000 | Loss: 0.7319\n",
      "Epoch 275/1000 | Loss: 0.7316\n",
      "Epoch 276/1000 | Loss: 0.7313\n",
      "Epoch 277/1000 | Loss: 0.7309\n",
      "Epoch 278/1000 | Loss: 0.7306\n",
      "Epoch 279/1000 | Loss: 0.7303\n",
      "Epoch 280/1000 | Loss: 0.7300\n",
      "Epoch 281/1000 | Loss: 0.7296\n",
      "Epoch 282/1000 | Loss: 0.7293\n",
      "Epoch 283/1000 | Loss: 0.7290\n",
      "Epoch 284/1000 | Loss: 0.7286\n",
      "Epoch 285/1000 | Loss: 0.7283\n",
      "Epoch 286/1000 | Loss: 0.7280\n",
      "Epoch 287/1000 | Loss: 0.7277\n",
      "Epoch 288/1000 | Loss: 0.7273\n",
      "Epoch 289/1000 | Loss: 0.7270\n",
      "Epoch 290/1000 | Loss: 0.7267\n",
      "Epoch 291/1000 | Loss: 0.7264\n",
      "Epoch 292/1000 | Loss: 0.7260\n",
      "Epoch 293/1000 | Loss: 0.7257\n",
      "Epoch 294/1000 | Loss: 0.7254\n",
      "Epoch 295/1000 | Loss: 0.7251\n",
      "Epoch 296/1000 | Loss: 0.7248\n",
      "Epoch 297/1000 | Loss: 0.7244\n",
      "Epoch 298/1000 | Loss: 0.7241\n",
      "Epoch 299/1000 | Loss: 0.7238\n",
      "Epoch 300/1000 | Loss: 0.7235\n",
      "Epoch 301/1000 | Loss: 0.7231\n",
      "Epoch 302/1000 | Loss: 0.7228\n",
      "Epoch 303/1000 | Loss: 0.7225\n",
      "Epoch 304/1000 | Loss: 0.7222\n",
      "Epoch 305/1000 | Loss: 0.7219\n",
      "Epoch 306/1000 | Loss: 0.7215\n",
      "Epoch 307/1000 | Loss: 0.7212\n",
      "Epoch 308/1000 | Loss: 0.7209\n",
      "Epoch 309/1000 | Loss: 0.7206\n",
      "Epoch 310/1000 | Loss: 0.7202\n",
      "Epoch 311/1000 | Loss: 0.7199\n",
      "Epoch 312/1000 | Loss: 0.7196\n",
      "Epoch 313/1000 | Loss: 0.7193\n",
      "Epoch 314/1000 | Loss: 0.7190\n",
      "Epoch 315/1000 | Loss: 0.7187\n",
      "Epoch 316/1000 | Loss: 0.7183\n",
      "Epoch 317/1000 | Loss: 0.7180\n",
      "Epoch 318/1000 | Loss: 0.7177\n",
      "Epoch 319/1000 | Loss: 0.7174\n",
      "Epoch 320/1000 | Loss: 0.7171\n",
      "Epoch 321/1000 | Loss: 0.7167\n",
      "Epoch 322/1000 | Loss: 0.7164\n",
      "Epoch 323/1000 | Loss: 0.7161\n",
      "Epoch 324/1000 | Loss: 0.7158\n",
      "Epoch 325/1000 | Loss: 0.7155\n",
      "Epoch 326/1000 | Loss: 0.7152\n",
      "Epoch 327/1000 | Loss: 0.7148\n",
      "Epoch 328/1000 | Loss: 0.7145\n",
      "Epoch 329/1000 | Loss: 0.7142\n",
      "Epoch 330/1000 | Loss: 0.7139\n",
      "Epoch 331/1000 | Loss: 0.7136\n",
      "Epoch 332/1000 | Loss: 0.7133\n",
      "Epoch 333/1000 | Loss: 0.7129\n",
      "Epoch 334/1000 | Loss: 0.7126\n",
      "Epoch 335/1000 | Loss: 0.7123\n",
      "Epoch 336/1000 | Loss: 0.7120\n",
      "Epoch 337/1000 | Loss: 0.7117\n",
      "Epoch 338/1000 | Loss: 0.7114\n",
      "Epoch 339/1000 | Loss: 0.7111\n",
      "Epoch 340/1000 | Loss: 0.7107\n",
      "Epoch 341/1000 | Loss: 0.7104\n",
      "Epoch 342/1000 | Loss: 0.7101\n",
      "Epoch 343/1000 | Loss: 0.7098\n",
      "Epoch 344/1000 | Loss: 0.7095\n",
      "Epoch 345/1000 | Loss: 0.7092\n",
      "Epoch 346/1000 | Loss: 0.7089\n",
      "Epoch 347/1000 | Loss: 0.7086\n",
      "Epoch 348/1000 | Loss: 0.7082\n",
      "Epoch 349/1000 | Loss: 0.7079\n",
      "Epoch 350/1000 | Loss: 0.7076\n",
      "Epoch 351/1000 | Loss: 0.7073\n",
      "Epoch 352/1000 | Loss: 0.7070\n",
      "Epoch 353/1000 | Loss: 0.7067\n",
      "Epoch 354/1000 | Loss: 0.7064\n",
      "Epoch 355/1000 | Loss: 0.7061\n",
      "Epoch 356/1000 | Loss: 0.7058\n",
      "Epoch 357/1000 | Loss: 0.7054\n",
      "Epoch 358/1000 | Loss: 0.7051\n",
      "Epoch 359/1000 | Loss: 0.7048\n",
      "Epoch 360/1000 | Loss: 0.7045\n",
      "Epoch 361/1000 | Loss: 0.7042\n",
      "Epoch 362/1000 | Loss: 0.7039\n",
      "Epoch 363/1000 | Loss: 0.7036\n",
      "Epoch 364/1000 | Loss: 0.7033\n",
      "Epoch 365/1000 | Loss: 0.7030\n",
      "Epoch 366/1000 | Loss: 0.7027\n",
      "Epoch 367/1000 | Loss: 0.7024\n",
      "Epoch 368/1000 | Loss: 0.7021\n",
      "Epoch 369/1000 | Loss: 0.7017\n",
      "Epoch 370/1000 | Loss: 0.7014\n",
      "Epoch 371/1000 | Loss: 0.7011\n",
      "Epoch 372/1000 | Loss: 0.7008\n",
      "Epoch 373/1000 | Loss: 0.7005\n",
      "Epoch 374/1000 | Loss: 0.7002\n",
      "Epoch 375/1000 | Loss: 0.6999\n",
      "Epoch 376/1000 | Loss: 0.6996\n",
      "Epoch 377/1000 | Loss: 0.6993\n",
      "Epoch 378/1000 | Loss: 0.6990\n",
      "Epoch 379/1000 | Loss: 0.6987\n",
      "Epoch 380/1000 | Loss: 0.6984\n",
      "Epoch 381/1000 | Loss: 0.6981\n",
      "Epoch 382/1000 | Loss: 0.6978\n",
      "Epoch 383/1000 | Loss: 0.6975\n",
      "Epoch 384/1000 | Loss: 0.6972\n",
      "Epoch 385/1000 | Loss: 0.6969\n",
      "Epoch 386/1000 | Loss: 0.6965\n",
      "Epoch 387/1000 | Loss: 0.6962\n",
      "Epoch 388/1000 | Loss: 0.6959\n",
      "Epoch 389/1000 | Loss: 0.6956\n",
      "Epoch 390/1000 | Loss: 0.6953\n",
      "Epoch 391/1000 | Loss: 0.6950\n",
      "Epoch 392/1000 | Loss: 0.6947\n",
      "Epoch 393/1000 | Loss: 0.6944\n",
      "Epoch 394/1000 | Loss: 0.6941\n",
      "Epoch 395/1000 | Loss: 0.6938\n",
      "Epoch 396/1000 | Loss: 0.6935\n",
      "Epoch 397/1000 | Loss: 0.6932\n",
      "Epoch 398/1000 | Loss: 0.6929\n",
      "Epoch 399/1000 | Loss: 0.6926\n",
      "Epoch 400/1000 | Loss: 0.6923\n",
      "Epoch 401/1000 | Loss: 0.6920\n",
      "Epoch 402/1000 | Loss: 0.6917\n",
      "Epoch 403/1000 | Loss: 0.6914\n",
      "Epoch 404/1000 | Loss: 0.6911\n",
      "Epoch 405/1000 | Loss: 0.6908\n",
      "Epoch 406/1000 | Loss: 0.6905\n",
      "Epoch 407/1000 | Loss: 0.6902\n",
      "Epoch 408/1000 | Loss: 0.6899\n",
      "Epoch 409/1000 | Loss: 0.6896\n",
      "Epoch 410/1000 | Loss: 0.6893\n",
      "Epoch 411/1000 | Loss: 0.6890\n",
      "Epoch 412/1000 | Loss: 0.6887\n",
      "Epoch 413/1000 | Loss: 0.6884\n",
      "Epoch 414/1000 | Loss: 0.6881\n",
      "Epoch 415/1000 | Loss: 0.6878\n",
      "Epoch 416/1000 | Loss: 0.6875\n",
      "Epoch 417/1000 | Loss: 0.6872\n",
      "Epoch 418/1000 | Loss: 0.6869\n",
      "Epoch 419/1000 | Loss: 0.6866\n",
      "Epoch 420/1000 | Loss: 0.6863\n",
      "Epoch 421/1000 | Loss: 0.6860\n",
      "Epoch 422/1000 | Loss: 0.6857\n",
      "Epoch 423/1000 | Loss: 0.6854\n",
      "Epoch 424/1000 | Loss: 0.6852\n",
      "Epoch 425/1000 | Loss: 0.6849\n",
      "Epoch 426/1000 | Loss: 0.6846\n",
      "Epoch 427/1000 | Loss: 0.6843\n",
      "Epoch 428/1000 | Loss: 0.6840\n",
      "Epoch 429/1000 | Loss: 0.6837\n",
      "Epoch 430/1000 | Loss: 0.6834\n",
      "Epoch 431/1000 | Loss: 0.6831\n",
      "Epoch 432/1000 | Loss: 0.6828\n",
      "Epoch 433/1000 | Loss: 0.6825\n",
      "Epoch 434/1000 | Loss: 0.6822\n",
      "Epoch 435/1000 | Loss: 0.6819\n",
      "Epoch 436/1000 | Loss: 0.6816\n",
      "Epoch 437/1000 | Loss: 0.6813\n",
      "Epoch 438/1000 | Loss: 0.6810\n",
      "Epoch 439/1000 | Loss: 0.6807\n",
      "Epoch 440/1000 | Loss: 0.6804\n",
      "Epoch 441/1000 | Loss: 0.6801\n",
      "Epoch 442/1000 | Loss: 0.6799\n",
      "Epoch 443/1000 | Loss: 0.6796\n",
      "Epoch 444/1000 | Loss: 0.6793\n",
      "Epoch 445/1000 | Loss: 0.6790\n",
      "Epoch 446/1000 | Loss: 0.6787\n",
      "Epoch 447/1000 | Loss: 0.6784\n",
      "Epoch 448/1000 | Loss: 0.6781\n",
      "Epoch 449/1000 | Loss: 0.6778\n",
      "Epoch 450/1000 | Loss: 0.6775\n",
      "Epoch 451/1000 | Loss: 0.6772\n",
      "Epoch 452/1000 | Loss: 0.6769\n",
      "Epoch 453/1000 | Loss: 0.6767\n",
      "Epoch 454/1000 | Loss: 0.6764\n",
      "Epoch 455/1000 | Loss: 0.6761\n",
      "Epoch 456/1000 | Loss: 0.6758\n",
      "Epoch 457/1000 | Loss: 0.6755\n",
      "Epoch 458/1000 | Loss: 0.6752\n",
      "Epoch 459/1000 | Loss: 0.6749\n",
      "Epoch 460/1000 | Loss: 0.6746\n",
      "Epoch 461/1000 | Loss: 0.6743\n",
      "Epoch 462/1000 | Loss: 0.6740\n",
      "Epoch 463/1000 | Loss: 0.6738\n",
      "Epoch 464/1000 | Loss: 0.6735\n",
      "Epoch 465/1000 | Loss: 0.6732\n",
      "Epoch 466/1000 | Loss: 0.6729\n",
      "Epoch 467/1000 | Loss: 0.6726\n",
      "Epoch 468/1000 | Loss: 0.6723\n",
      "Epoch 469/1000 | Loss: 0.6720\n",
      "Epoch 470/1000 | Loss: 0.6717\n",
      "Epoch 471/1000 | Loss: 0.6715\n",
      "Epoch 472/1000 | Loss: 0.6712\n",
      "Epoch 473/1000 | Loss: 0.6709\n",
      "Epoch 474/1000 | Loss: 0.6706\n",
      "Epoch 475/1000 | Loss: 0.6703\n",
      "Epoch 476/1000 | Loss: 0.6700\n",
      "Epoch 477/1000 | Loss: 0.6697\n",
      "Epoch 478/1000 | Loss: 0.6695\n",
      "Epoch 479/1000 | Loss: 0.6692\n",
      "Epoch 480/1000 | Loss: 0.6689\n",
      "Epoch 481/1000 | Loss: 0.6686\n",
      "Epoch 482/1000 | Loss: 0.6683\n",
      "Epoch 483/1000 | Loss: 0.6680\n",
      "Epoch 484/1000 | Loss: 0.6678\n",
      "Epoch 485/1000 | Loss: 0.6675\n",
      "Epoch 486/1000 | Loss: 0.6672\n",
      "Epoch 487/1000 | Loss: 0.6669\n",
      "Epoch 488/1000 | Loss: 0.6666\n",
      "Epoch 489/1000 | Loss: 0.6663\n",
      "Epoch 490/1000 | Loss: 0.6661\n",
      "Epoch 491/1000 | Loss: 0.6658\n",
      "Epoch 492/1000 | Loss: 0.6655\n",
      "Epoch 493/1000 | Loss: 0.6652\n",
      "Epoch 494/1000 | Loss: 0.6649\n",
      "Epoch 495/1000 | Loss: 0.6646\n",
      "Epoch 496/1000 | Loss: 0.6644\n",
      "Epoch 497/1000 | Loss: 0.6641\n",
      "Epoch 498/1000 | Loss: 0.6638\n",
      "Epoch 499/1000 | Loss: 0.6635\n",
      "Epoch 500/1000 | Loss: 0.6632\n",
      "Epoch 501/1000 | Loss: 0.6630\n",
      "Epoch 502/1000 | Loss: 0.6627\n",
      "Epoch 503/1000 | Loss: 0.6624\n",
      "Epoch 504/1000 | Loss: 0.6621\n",
      "Epoch 505/1000 | Loss: 0.6618\n",
      "Epoch 506/1000 | Loss: 0.6616\n",
      "Epoch 507/1000 | Loss: 0.6613\n",
      "Epoch 508/1000 | Loss: 0.6610\n",
      "Epoch 509/1000 | Loss: 0.6607\n",
      "Epoch 510/1000 | Loss: 0.6604\n",
      "Epoch 511/1000 | Loss: 0.6602\n",
      "Epoch 512/1000 | Loss: 0.6599\n",
      "Epoch 513/1000 | Loss: 0.6596\n",
      "Epoch 514/1000 | Loss: 0.6593\n",
      "Epoch 515/1000 | Loss: 0.6590\n",
      "Epoch 516/1000 | Loss: 0.6588\n",
      "Epoch 517/1000 | Loss: 0.6585\n",
      "Epoch 518/1000 | Loss: 0.6582\n",
      "Epoch 519/1000 | Loss: 0.6579\n",
      "Epoch 520/1000 | Loss: 0.6577\n",
      "Epoch 521/1000 | Loss: 0.6574\n",
      "Epoch 522/1000 | Loss: 0.6571\n",
      "Epoch 523/1000 | Loss: 0.6568\n",
      "Epoch 524/1000 | Loss: 0.6565\n",
      "Epoch 525/1000 | Loss: 0.6563\n",
      "Epoch 526/1000 | Loss: 0.6560\n",
      "Epoch 527/1000 | Loss: 0.6557\n",
      "Epoch 528/1000 | Loss: 0.6554\n",
      "Epoch 529/1000 | Loss: 0.6552\n",
      "Epoch 530/1000 | Loss: 0.6549\n",
      "Epoch 531/1000 | Loss: 0.6546\n",
      "Epoch 532/1000 | Loss: 0.6543\n",
      "Epoch 533/1000 | Loss: 0.6541\n",
      "Epoch 534/1000 | Loss: 0.6538\n",
      "Epoch 535/1000 | Loss: 0.6535\n",
      "Epoch 536/1000 | Loss: 0.6532\n",
      "Epoch 537/1000 | Loss: 0.6530\n",
      "Epoch 538/1000 | Loss: 0.6527\n",
      "Epoch 539/1000 | Loss: 0.6524\n",
      "Epoch 540/1000 | Loss: 0.6522\n",
      "Epoch 541/1000 | Loss: 0.6519\n",
      "Epoch 542/1000 | Loss: 0.6516\n",
      "Epoch 543/1000 | Loss: 0.6513\n",
      "Epoch 544/1000 | Loss: 0.6511\n",
      "Epoch 545/1000 | Loss: 0.6508\n",
      "Epoch 546/1000 | Loss: 0.6505\n",
      "Epoch 547/1000 | Loss: 0.6502\n",
      "Epoch 548/1000 | Loss: 0.6500\n",
      "Epoch 549/1000 | Loss: 0.6497\n",
      "Epoch 550/1000 | Loss: 0.6494\n",
      "Epoch 551/1000 | Loss: 0.6492\n",
      "Epoch 552/1000 | Loss: 0.6489\n",
      "Epoch 553/1000 | Loss: 0.6486\n",
      "Epoch 554/1000 | Loss: 0.6483\n",
      "Epoch 555/1000 | Loss: 0.6481\n",
      "Epoch 556/1000 | Loss: 0.6478\n",
      "Epoch 557/1000 | Loss: 0.6475\n",
      "Epoch 558/1000 | Loss: 0.6473\n",
      "Epoch 559/1000 | Loss: 0.6470\n",
      "Epoch 560/1000 | Loss: 0.6467\n",
      "Epoch 561/1000 | Loss: 0.6465\n",
      "Epoch 562/1000 | Loss: 0.6462\n",
      "Epoch 563/1000 | Loss: 0.6459\n",
      "Epoch 564/1000 | Loss: 0.6456\n",
      "Epoch 565/1000 | Loss: 0.6454\n",
      "Epoch 566/1000 | Loss: 0.6451\n",
      "Epoch 567/1000 | Loss: 0.6448\n",
      "Epoch 568/1000 | Loss: 0.6446\n",
      "Epoch 569/1000 | Loss: 0.6443\n",
      "Epoch 570/1000 | Loss: 0.6440\n",
      "Epoch 571/1000 | Loss: 0.6438\n",
      "Epoch 572/1000 | Loss: 0.6435\n",
      "Epoch 573/1000 | Loss: 0.6432\n",
      "Epoch 574/1000 | Loss: 0.6430\n",
      "Epoch 575/1000 | Loss: 0.6427\n",
      "Epoch 576/1000 | Loss: 0.6424\n",
      "Epoch 577/1000 | Loss: 0.6422\n",
      "Epoch 578/1000 | Loss: 0.6419\n",
      "Epoch 579/1000 | Loss: 0.6416\n",
      "Epoch 580/1000 | Loss: 0.6414\n",
      "Epoch 581/1000 | Loss: 0.6411\n",
      "Epoch 582/1000 | Loss: 0.6408\n",
      "Epoch 583/1000 | Loss: 0.6406\n",
      "Epoch 584/1000 | Loss: 0.6403\n",
      "Epoch 585/1000 | Loss: 0.6400\n",
      "Epoch 586/1000 | Loss: 0.6398\n",
      "Epoch 587/1000 | Loss: 0.6395\n",
      "Epoch 588/1000 | Loss: 0.6393\n",
      "Epoch 589/1000 | Loss: 0.6390\n",
      "Epoch 590/1000 | Loss: 0.6387\n",
      "Epoch 591/1000 | Loss: 0.6385\n",
      "Epoch 592/1000 | Loss: 0.6382\n",
      "Epoch 593/1000 | Loss: 0.6379\n",
      "Epoch 594/1000 | Loss: 0.6377\n",
      "Epoch 595/1000 | Loss: 0.6374\n",
      "Epoch 596/1000 | Loss: 0.6371\n",
      "Epoch 597/1000 | Loss: 0.6369\n",
      "Epoch 598/1000 | Loss: 0.6366\n",
      "Epoch 599/1000 | Loss: 0.6364\n",
      "Epoch 600/1000 | Loss: 0.6361\n",
      "Epoch 601/1000 | Loss: 0.6358\n",
      "Epoch 602/1000 | Loss: 0.6356\n",
      "Epoch 603/1000 | Loss: 0.6353\n",
      "Epoch 604/1000 | Loss: 0.6350\n",
      "Epoch 605/1000 | Loss: 0.6348\n",
      "Epoch 606/1000 | Loss: 0.6345\n",
      "Epoch 607/1000 | Loss: 0.6343\n",
      "Epoch 608/1000 | Loss: 0.6340\n",
      "Epoch 609/1000 | Loss: 0.6337\n",
      "Epoch 610/1000 | Loss: 0.6335\n",
      "Epoch 611/1000 | Loss: 0.6332\n",
      "Epoch 612/1000 | Loss: 0.6330\n",
      "Epoch 613/1000 | Loss: 0.6327\n",
      "Epoch 614/1000 | Loss: 0.6324\n",
      "Epoch 615/1000 | Loss: 0.6322\n",
      "Epoch 616/1000 | Loss: 0.6319\n",
      "Epoch 617/1000 | Loss: 0.6317\n",
      "Epoch 618/1000 | Loss: 0.6314\n",
      "Epoch 619/1000 | Loss: 0.6311\n",
      "Epoch 620/1000 | Loss: 0.6309\n",
      "Epoch 621/1000 | Loss: 0.6306\n",
      "Epoch 622/1000 | Loss: 0.6304\n",
      "Epoch 623/1000 | Loss: 0.6301\n",
      "Epoch 624/1000 | Loss: 0.6299\n",
      "Epoch 625/1000 | Loss: 0.6296\n",
      "Epoch 626/1000 | Loss: 0.6293\n",
      "Epoch 627/1000 | Loss: 0.6291\n",
      "Epoch 628/1000 | Loss: 0.6288\n",
      "Epoch 629/1000 | Loss: 0.6286\n",
      "Epoch 630/1000 | Loss: 0.6283\n",
      "Epoch 631/1000 | Loss: 0.6281\n",
      "Epoch 632/1000 | Loss: 0.6278\n",
      "Epoch 633/1000 | Loss: 0.6275\n",
      "Epoch 634/1000 | Loss: 0.6273\n",
      "Epoch 635/1000 | Loss: 0.6270\n",
      "Epoch 636/1000 | Loss: 0.6268\n",
      "Epoch 637/1000 | Loss: 0.6265\n",
      "Epoch 638/1000 | Loss: 0.6263\n",
      "Epoch 639/1000 | Loss: 0.6260\n",
      "Epoch 640/1000 | Loss: 0.6258\n",
      "Epoch 641/1000 | Loss: 0.6255\n",
      "Epoch 642/1000 | Loss: 0.6252\n",
      "Epoch 643/1000 | Loss: 0.6250\n",
      "Epoch 644/1000 | Loss: 0.6247\n",
      "Epoch 645/1000 | Loss: 0.6245\n",
      "Epoch 646/1000 | Loss: 0.6242\n",
      "Epoch 647/1000 | Loss: 0.6240\n",
      "Epoch 648/1000 | Loss: 0.6237\n",
      "Epoch 649/1000 | Loss: 0.6235\n",
      "Epoch 650/1000 | Loss: 0.6232\n",
      "Epoch 651/1000 | Loss: 0.6230\n",
      "Epoch 652/1000 | Loss: 0.6227\n",
      "Epoch 653/1000 | Loss: 0.6225\n",
      "Epoch 654/1000 | Loss: 0.6222\n",
      "Epoch 655/1000 | Loss: 0.6220\n",
      "Epoch 656/1000 | Loss: 0.6217\n",
      "Epoch 657/1000 | Loss: 0.6215\n",
      "Epoch 658/1000 | Loss: 0.6212\n",
      "Epoch 659/1000 | Loss: 0.6209\n",
      "Epoch 660/1000 | Loss: 0.6207\n",
      "Epoch 661/1000 | Loss: 0.6204\n",
      "Epoch 662/1000 | Loss: 0.6202\n",
      "Epoch 663/1000 | Loss: 0.6199\n",
      "Epoch 664/1000 | Loss: 0.6197\n",
      "Epoch 665/1000 | Loss: 0.6194\n",
      "Epoch 666/1000 | Loss: 0.6192\n",
      "Epoch 667/1000 | Loss: 0.6189\n",
      "Epoch 668/1000 | Loss: 0.6187\n",
      "Epoch 669/1000 | Loss: 0.6184\n",
      "Epoch 670/1000 | Loss: 0.6182\n",
      "Epoch 671/1000 | Loss: 0.6179\n",
      "Epoch 672/1000 | Loss: 0.6177\n",
      "Epoch 673/1000 | Loss: 0.6174\n",
      "Epoch 674/1000 | Loss: 0.6172\n",
      "Epoch 675/1000 | Loss: 0.6169\n",
      "Epoch 676/1000 | Loss: 0.6167\n",
      "Epoch 677/1000 | Loss: 0.6165\n",
      "Epoch 678/1000 | Loss: 0.6162\n",
      "Epoch 679/1000 | Loss: 0.6160\n",
      "Epoch 680/1000 | Loss: 0.6157\n",
      "Epoch 681/1000 | Loss: 0.6155\n",
      "Epoch 682/1000 | Loss: 0.6152\n",
      "Epoch 683/1000 | Loss: 0.6150\n",
      "Epoch 684/1000 | Loss: 0.6147\n",
      "Epoch 685/1000 | Loss: 0.6145\n",
      "Epoch 686/1000 | Loss: 0.6142\n",
      "Epoch 687/1000 | Loss: 0.6140\n",
      "Epoch 688/1000 | Loss: 0.6137\n",
      "Epoch 689/1000 | Loss: 0.6135\n",
      "Epoch 690/1000 | Loss: 0.6132\n",
      "Epoch 691/1000 | Loss: 0.6130\n",
      "Epoch 692/1000 | Loss: 0.6127\n",
      "Epoch 693/1000 | Loss: 0.6125\n",
      "Epoch 694/1000 | Loss: 0.6123\n",
      "Epoch 695/1000 | Loss: 0.6120\n",
      "Epoch 696/1000 | Loss: 0.6118\n",
      "Epoch 697/1000 | Loss: 0.6115\n",
      "Epoch 698/1000 | Loss: 0.6113\n",
      "Epoch 699/1000 | Loss: 0.6110\n",
      "Epoch 700/1000 | Loss: 0.6108\n",
      "Epoch 701/1000 | Loss: 0.6105\n",
      "Epoch 702/1000 | Loss: 0.6103\n",
      "Epoch 703/1000 | Loss: 0.6101\n",
      "Epoch 704/1000 | Loss: 0.6098\n",
      "Epoch 705/1000 | Loss: 0.6096\n",
      "Epoch 706/1000 | Loss: 0.6093\n",
      "Epoch 707/1000 | Loss: 0.6091\n",
      "Epoch 708/1000 | Loss: 0.6088\n",
      "Epoch 709/1000 | Loss: 0.6086\n",
      "Epoch 710/1000 | Loss: 0.6084\n",
      "Epoch 711/1000 | Loss: 0.6081\n",
      "Epoch 712/1000 | Loss: 0.6079\n",
      "Epoch 713/1000 | Loss: 0.6076\n",
      "Epoch 714/1000 | Loss: 0.6074\n",
      "Epoch 715/1000 | Loss: 0.6071\n",
      "Epoch 716/1000 | Loss: 0.6069\n",
      "Epoch 717/1000 | Loss: 0.6067\n",
      "Epoch 718/1000 | Loss: 0.6064\n",
      "Epoch 719/1000 | Loss: 0.6062\n",
      "Epoch 720/1000 | Loss: 0.6059\n",
      "Epoch 721/1000 | Loss: 0.6057\n",
      "Epoch 722/1000 | Loss: 0.6055\n",
      "Epoch 723/1000 | Loss: 0.6052\n",
      "Epoch 724/1000 | Loss: 0.6050\n",
      "Epoch 725/1000 | Loss: 0.6047\n",
      "Epoch 726/1000 | Loss: 0.6045\n",
      "Epoch 727/1000 | Loss: 0.6042\n",
      "Epoch 728/1000 | Loss: 0.6040\n",
      "Epoch 729/1000 | Loss: 0.6038\n",
      "Epoch 730/1000 | Loss: 0.6035\n",
      "Epoch 731/1000 | Loss: 0.6033\n",
      "Epoch 732/1000 | Loss: 0.6031\n",
      "Epoch 733/1000 | Loss: 0.6028\n",
      "Epoch 734/1000 | Loss: 0.6026\n",
      "Epoch 735/1000 | Loss: 0.6023\n",
      "Epoch 736/1000 | Loss: 0.6021\n",
      "Epoch 737/1000 | Loss: 0.6019\n",
      "Epoch 738/1000 | Loss: 0.6016\n",
      "Epoch 739/1000 | Loss: 0.6014\n",
      "Epoch 740/1000 | Loss: 0.6011\n",
      "Epoch 741/1000 | Loss: 0.6009\n",
      "Epoch 742/1000 | Loss: 0.6007\n",
      "Epoch 743/1000 | Loss: 0.6004\n",
      "Epoch 744/1000 | Loss: 0.6002\n",
      "Epoch 745/1000 | Loss: 0.6000\n",
      "Epoch 746/1000 | Loss: 0.5997\n",
      "Epoch 747/1000 | Loss: 0.5995\n",
      "Epoch 748/1000 | Loss: 0.5993\n",
      "Epoch 749/1000 | Loss: 0.5990\n",
      "Epoch 750/1000 | Loss: 0.5988\n",
      "Epoch 751/1000 | Loss: 0.5985\n",
      "Epoch 752/1000 | Loss: 0.5983\n",
      "Epoch 753/1000 | Loss: 0.5981\n",
      "Epoch 754/1000 | Loss: 0.5978\n",
      "Epoch 755/1000 | Loss: 0.5976\n",
      "Epoch 756/1000 | Loss: 0.5974\n",
      "Epoch 757/1000 | Loss: 0.5971\n",
      "Epoch 758/1000 | Loss: 0.5969\n",
      "Epoch 759/1000 | Loss: 0.5967\n",
      "Epoch 760/1000 | Loss: 0.5964\n",
      "Epoch 761/1000 | Loss: 0.5962\n",
      "Epoch 762/1000 | Loss: 0.5960\n",
      "Epoch 763/1000 | Loss: 0.5957\n",
      "Epoch 764/1000 | Loss: 0.5955\n",
      "Epoch 765/1000 | Loss: 0.5953\n",
      "Epoch 766/1000 | Loss: 0.5950\n",
      "Epoch 767/1000 | Loss: 0.5948\n",
      "Epoch 768/1000 | Loss: 0.5946\n",
      "Epoch 769/1000 | Loss: 0.5943\n",
      "Epoch 770/1000 | Loss: 0.5941\n",
      "Epoch 771/1000 | Loss: 0.5939\n",
      "Epoch 772/1000 | Loss: 0.5936\n",
      "Epoch 773/1000 | Loss: 0.5934\n",
      "Epoch 774/1000 | Loss: 0.5932\n",
      "Epoch 775/1000 | Loss: 0.5929\n",
      "Epoch 776/1000 | Loss: 0.5927\n",
      "Epoch 777/1000 | Loss: 0.5925\n",
      "Epoch 778/1000 | Loss: 0.5922\n",
      "Epoch 779/1000 | Loss: 0.5920\n",
      "Epoch 780/1000 | Loss: 0.5918\n",
      "Epoch 781/1000 | Loss: 0.5915\n",
      "Epoch 782/1000 | Loss: 0.5913\n",
      "Epoch 783/1000 | Loss: 0.5911\n",
      "Epoch 784/1000 | Loss: 0.5909\n",
      "Epoch 785/1000 | Loss: 0.5906\n",
      "Epoch 786/1000 | Loss: 0.5904\n",
      "Epoch 787/1000 | Loss: 0.5902\n",
      "Epoch 788/1000 | Loss: 0.5899\n",
      "Epoch 789/1000 | Loss: 0.5897\n",
      "Epoch 790/1000 | Loss: 0.5895\n",
      "Epoch 791/1000 | Loss: 0.5892\n",
      "Epoch 792/1000 | Loss: 0.5890\n",
      "Epoch 793/1000 | Loss: 0.5888\n",
      "Epoch 794/1000 | Loss: 0.5886\n",
      "Epoch 795/1000 | Loss: 0.5883\n",
      "Epoch 796/1000 | Loss: 0.5881\n",
      "Epoch 797/1000 | Loss: 0.5879\n",
      "Epoch 798/1000 | Loss: 0.5876\n",
      "Epoch 799/1000 | Loss: 0.5874\n",
      "Epoch 800/1000 | Loss: 0.5872\n",
      "Epoch 801/1000 | Loss: 0.5870\n",
      "Epoch 802/1000 | Loss: 0.5867\n",
      "Epoch 803/1000 | Loss: 0.5865\n",
      "Epoch 804/1000 | Loss: 0.5863\n",
      "Epoch 805/1000 | Loss: 0.5860\n",
      "Epoch 806/1000 | Loss: 0.5858\n",
      "Epoch 807/1000 | Loss: 0.5856\n",
      "Epoch 808/1000 | Loss: 0.5854\n",
      "Epoch 809/1000 | Loss: 0.5851\n",
      "Epoch 810/1000 | Loss: 0.5849\n",
      "Epoch 811/1000 | Loss: 0.5847\n",
      "Epoch 812/1000 | Loss: 0.5845\n",
      "Epoch 813/1000 | Loss: 0.5842\n",
      "Epoch 814/1000 | Loss: 0.5840\n",
      "Epoch 815/1000 | Loss: 0.5838\n",
      "Epoch 816/1000 | Loss: 0.5836\n",
      "Epoch 817/1000 | Loss: 0.5833\n",
      "Epoch 818/1000 | Loss: 0.5831\n",
      "Epoch 819/1000 | Loss: 0.5829\n",
      "Epoch 820/1000 | Loss: 0.5827\n",
      "Epoch 821/1000 | Loss: 0.5824\n",
      "Epoch 822/1000 | Loss: 0.5822\n",
      "Epoch 823/1000 | Loss: 0.5820\n",
      "Epoch 824/1000 | Loss: 0.5818\n",
      "Epoch 825/1000 | Loss: 0.5815\n",
      "Epoch 826/1000 | Loss: 0.5813\n",
      "Epoch 827/1000 | Loss: 0.5811\n",
      "Epoch 828/1000 | Loss: 0.5809\n",
      "Epoch 829/1000 | Loss: 0.5806\n",
      "Epoch 830/1000 | Loss: 0.5804\n",
      "Epoch 831/1000 | Loss: 0.5802\n",
      "Epoch 832/1000 | Loss: 0.5800\n",
      "Epoch 833/1000 | Loss: 0.5798\n",
      "Epoch 834/1000 | Loss: 0.5795\n",
      "Epoch 835/1000 | Loss: 0.5793\n",
      "Epoch 836/1000 | Loss: 0.5791\n",
      "Epoch 837/1000 | Loss: 0.5789\n",
      "Epoch 838/1000 | Loss: 0.5786\n",
      "Epoch 839/1000 | Loss: 0.5784\n",
      "Epoch 840/1000 | Loss: 0.5782\n",
      "Epoch 841/1000 | Loss: 0.5780\n",
      "Epoch 842/1000 | Loss: 0.5778\n",
      "Epoch 843/1000 | Loss: 0.5775\n",
      "Epoch 844/1000 | Loss: 0.5773\n",
      "Epoch 845/1000 | Loss: 0.5771\n",
      "Epoch 846/1000 | Loss: 0.5769\n",
      "Epoch 847/1000 | Loss: 0.5767\n",
      "Epoch 848/1000 | Loss: 0.5764\n",
      "Epoch 849/1000 | Loss: 0.5762\n",
      "Epoch 850/1000 | Loss: 0.5760\n",
      "Epoch 851/1000 | Loss: 0.5758\n",
      "Epoch 852/1000 | Loss: 0.5756\n",
      "Epoch 853/1000 | Loss: 0.5753\n",
      "Epoch 854/1000 | Loss: 0.5751\n",
      "Epoch 855/1000 | Loss: 0.5749\n",
      "Epoch 856/1000 | Loss: 0.5747\n",
      "Epoch 857/1000 | Loss: 0.5745\n",
      "Epoch 858/1000 | Loss: 0.5742\n",
      "Epoch 859/1000 | Loss: 0.5740\n",
      "Epoch 860/1000 | Loss: 0.5738\n",
      "Epoch 861/1000 | Loss: 0.5736\n",
      "Epoch 862/1000 | Loss: 0.5734\n",
      "Epoch 863/1000 | Loss: 0.5731\n",
      "Epoch 864/1000 | Loss: 0.5729\n",
      "Epoch 865/1000 | Loss: 0.5727\n",
      "Epoch 866/1000 | Loss: 0.5725\n",
      "Epoch 867/1000 | Loss: 0.5723\n",
      "Epoch 868/1000 | Loss: 0.5721\n",
      "Epoch 869/1000 | Loss: 0.5718\n",
      "Epoch 870/1000 | Loss: 0.5716\n",
      "Epoch 871/1000 | Loss: 0.5714\n",
      "Epoch 872/1000 | Loss: 0.5712\n",
      "Epoch 873/1000 | Loss: 0.5710\n",
      "Epoch 874/1000 | Loss: 0.5708\n",
      "Epoch 875/1000 | Loss: 0.5705\n",
      "Epoch 876/1000 | Loss: 0.5703\n",
      "Epoch 877/1000 | Loss: 0.5701\n",
      "Epoch 878/1000 | Loss: 0.5699\n",
      "Epoch 879/1000 | Loss: 0.5697\n",
      "Epoch 880/1000 | Loss: 0.5695\n",
      "Epoch 881/1000 | Loss: 0.5692\n",
      "Epoch 882/1000 | Loss: 0.5690\n",
      "Epoch 883/1000 | Loss: 0.5688\n",
      "Epoch 884/1000 | Loss: 0.5686\n",
      "Epoch 885/1000 | Loss: 0.5684\n",
      "Epoch 886/1000 | Loss: 0.5682\n",
      "Epoch 887/1000 | Loss: 0.5680\n",
      "Epoch 888/1000 | Loss: 0.5677\n",
      "Epoch 889/1000 | Loss: 0.5675\n",
      "Epoch 890/1000 | Loss: 0.5673\n",
      "Epoch 891/1000 | Loss: 0.5671\n",
      "Epoch 892/1000 | Loss: 0.5669\n",
      "Epoch 893/1000 | Loss: 0.5667\n",
      "Epoch 894/1000 | Loss: 0.5665\n",
      "Epoch 895/1000 | Loss: 0.5662\n",
      "Epoch 896/1000 | Loss: 0.5660\n",
      "Epoch 897/1000 | Loss: 0.5658\n",
      "Epoch 898/1000 | Loss: 0.5656\n",
      "Epoch 899/1000 | Loss: 0.5654\n",
      "Epoch 900/1000 | Loss: 0.5652\n",
      "Epoch 901/1000 | Loss: 0.5650\n",
      "Epoch 902/1000 | Loss: 0.5648\n",
      "Epoch 903/1000 | Loss: 0.5645\n",
      "Epoch 904/1000 | Loss: 0.5643\n",
      "Epoch 905/1000 | Loss: 0.5641\n",
      "Epoch 906/1000 | Loss: 0.5639\n",
      "Epoch 907/1000 | Loss: 0.5637\n",
      "Epoch 908/1000 | Loss: 0.5635\n",
      "Epoch 909/1000 | Loss: 0.5633\n",
      "Epoch 910/1000 | Loss: 0.5631\n",
      "Epoch 911/1000 | Loss: 0.5629\n",
      "Epoch 912/1000 | Loss: 0.5626\n",
      "Epoch 913/1000 | Loss: 0.5624\n",
      "Epoch 914/1000 | Loss: 0.5622\n",
      "Epoch 915/1000 | Loss: 0.5620\n",
      "Epoch 916/1000 | Loss: 0.5618\n",
      "Epoch 917/1000 | Loss: 0.5616\n",
      "Epoch 918/1000 | Loss: 0.5614\n",
      "Epoch 919/1000 | Loss: 0.5612\n",
      "Epoch 920/1000 | Loss: 0.5610\n",
      "Epoch 921/1000 | Loss: 0.5608\n",
      "Epoch 922/1000 | Loss: 0.5605\n",
      "Epoch 923/1000 | Loss: 0.5603\n",
      "Epoch 924/1000 | Loss: 0.5601\n",
      "Epoch 925/1000 | Loss: 0.5599\n",
      "Epoch 926/1000 | Loss: 0.5597\n",
      "Epoch 927/1000 | Loss: 0.5595\n",
      "Epoch 928/1000 | Loss: 0.5593\n",
      "Epoch 929/1000 | Loss: 0.5591\n",
      "Epoch 930/1000 | Loss: 0.5589\n",
      "Epoch 931/1000 | Loss: 0.5587\n",
      "Epoch 932/1000 | Loss: 0.5585\n",
      "Epoch 933/1000 | Loss: 0.5583\n",
      "Epoch 934/1000 | Loss: 0.5580\n",
      "Epoch 935/1000 | Loss: 0.5578\n",
      "Epoch 936/1000 | Loss: 0.5576\n",
      "Epoch 937/1000 | Loss: 0.5574\n",
      "Epoch 938/1000 | Loss: 0.5572\n",
      "Epoch 939/1000 | Loss: 0.5570\n",
      "Epoch 940/1000 | Loss: 0.5568\n",
      "Epoch 941/1000 | Loss: 0.5566\n",
      "Epoch 942/1000 | Loss: 0.5564\n",
      "Epoch 943/1000 | Loss: 0.5562\n",
      "Epoch 944/1000 | Loss: 0.5560\n",
      "Epoch 945/1000 | Loss: 0.5558\n",
      "Epoch 946/1000 | Loss: 0.5556\n",
      "Epoch 947/1000 | Loss: 0.5554\n",
      "Epoch 948/1000 | Loss: 0.5552\n",
      "Epoch 949/1000 | Loss: 0.5549\n",
      "Epoch 950/1000 | Loss: 0.5547\n",
      "Epoch 951/1000 | Loss: 0.5545\n",
      "Epoch 952/1000 | Loss: 0.5543\n",
      "Epoch 953/1000 | Loss: 0.5541\n",
      "Epoch 954/1000 | Loss: 0.5539\n",
      "Epoch 955/1000 | Loss: 0.5537\n",
      "Epoch 956/1000 | Loss: 0.5535\n",
      "Epoch 957/1000 | Loss: 0.5533\n",
      "Epoch 958/1000 | Loss: 0.5531\n",
      "Epoch 959/1000 | Loss: 0.5529\n",
      "Epoch 960/1000 | Loss: 0.5527\n",
      "Epoch 961/1000 | Loss: 0.5525\n",
      "Epoch 962/1000 | Loss: 0.5523\n",
      "Epoch 963/1000 | Loss: 0.5521\n",
      "Epoch 964/1000 | Loss: 0.5519\n",
      "Epoch 965/1000 | Loss: 0.5517\n",
      "Epoch 966/1000 | Loss: 0.5515\n",
      "Epoch 967/1000 | Loss: 0.5513\n",
      "Epoch 968/1000 | Loss: 0.5511\n",
      "Epoch 969/1000 | Loss: 0.5509\n",
      "Epoch 970/1000 | Loss: 0.5507\n",
      "Epoch 971/1000 | Loss: 0.5505\n",
      "Epoch 972/1000 | Loss: 0.5503\n",
      "Epoch 973/1000 | Loss: 0.5501\n",
      "Epoch 974/1000 | Loss: 0.5499\n",
      "Epoch 975/1000 | Loss: 0.5497\n",
      "Epoch 976/1000 | Loss: 0.5495\n",
      "Epoch 977/1000 | Loss: 0.5493\n",
      "Epoch 978/1000 | Loss: 0.5491\n",
      "Epoch 979/1000 | Loss: 0.5489\n",
      "Epoch 980/1000 | Loss: 0.5487\n",
      "Epoch 981/1000 | Loss: 0.5484\n",
      "Epoch 982/1000 | Loss: 0.5482\n",
      "Epoch 983/1000 | Loss: 0.5480\n",
      "Epoch 984/1000 | Loss: 0.5478\n",
      "Epoch 985/1000 | Loss: 0.5476\n",
      "Epoch 986/1000 | Loss: 0.5474\n",
      "Epoch 987/1000 | Loss: 0.5472\n",
      "Epoch 988/1000 | Loss: 0.5470\n",
      "Epoch 989/1000 | Loss: 0.5468\n",
      "Epoch 990/1000 | Loss: 0.5466\n",
      "Epoch 991/1000 | Loss: 0.5464\n",
      "Epoch 992/1000 | Loss: 0.5462\n",
      "Epoch 993/1000 | Loss: 0.5460\n",
      "Epoch 994/1000 | Loss: 0.5459\n",
      "Epoch 995/1000 | Loss: 0.5457\n",
      "Epoch 996/1000 | Loss: 0.5455\n",
      "Epoch 997/1000 | Loss: 0.5453\n",
      "Epoch 998/1000 | Loss: 0.5451\n",
      "Epoch 999/1000 | Loss: 0.5449\n",
      "Epoch 1000/1000 | Loss: 0.5447\n",
      "\n",
      "Let's predict the hours need to score above 50%\n",
      "==================================================\n",
      "Prediction after 1 hour of training: 0.4816 | Above 50%: False\n",
      "Prediction after 7 hours of training: 0.9253 | Above 50%: True\n"
     ]
    }
   ],
   "source": [
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import sigmoid\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Training data and ground truth\n",
    "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_data = tensor([[0.], [0.], [1.], [1.]])\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data.\n",
    "        \"\"\"\n",
    "        y_pred = sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = nn.BCELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training\n",
    "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
    "hour_var = model(tensor([[1.0]]))\n",
    "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
    "hour_var = model(tensor([[7.0]]))\n",
    "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ff9d06f-0641-40f1-bfb8-5c363deec0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n",
      "Epoch: 1/100 | Loss: 1.0067\n",
      "Epoch: 2/100 | Loss: 0.9692\n",
      "Epoch: 3/100 | Loss: 0.9353\n",
      "Epoch: 4/100 | Loss: 0.9047\n",
      "Epoch: 5/100 | Loss: 0.8771\n",
      "Epoch: 6/100 | Loss: 0.8523\n",
      "Epoch: 7/100 | Loss: 0.8300\n",
      "Epoch: 8/100 | Loss: 0.8100\n",
      "Epoch: 9/100 | Loss: 0.7922\n",
      "Epoch: 10/100 | Loss: 0.7762\n",
      "Epoch: 11/100 | Loss: 0.7620\n",
      "Epoch: 12/100 | Loss: 0.7493\n",
      "Epoch: 13/100 | Loss: 0.7380\n",
      "Epoch: 14/100 | Loss: 0.7279\n",
      "Epoch: 15/100 | Loss: 0.7189\n",
      "Epoch: 16/100 | Loss: 0.7109\n",
      "Epoch: 17/100 | Loss: 0.7038\n",
      "Epoch: 18/100 | Loss: 0.6974\n",
      "Epoch: 19/100 | Loss: 0.6918\n",
      "Epoch: 20/100 | Loss: 0.6868\n",
      "Epoch: 21/100 | Loss: 0.6823\n",
      "Epoch: 22/100 | Loss: 0.6783\n",
      "Epoch: 23/100 | Loss: 0.6748\n",
      "Epoch: 24/100 | Loss: 0.6716\n",
      "Epoch: 25/100 | Loss: 0.6688\n",
      "Epoch: 26/100 | Loss: 0.6663\n",
      "Epoch: 27/100 | Loss: 0.6641\n",
      "Epoch: 28/100 | Loss: 0.6621\n",
      "Epoch: 29/100 | Loss: 0.6603\n",
      "Epoch: 30/100 | Loss: 0.6587\n",
      "Epoch: 31/100 | Loss: 0.6573\n",
      "Epoch: 32/100 | Loss: 0.6560\n",
      "Epoch: 33/100 | Loss: 0.6549\n",
      "Epoch: 34/100 | Loss: 0.6539\n",
      "Epoch: 35/100 | Loss: 0.6530\n",
      "Epoch: 36/100 | Loss: 0.6522\n",
      "Epoch: 37/100 | Loss: 0.6514\n",
      "Epoch: 38/100 | Loss: 0.6508\n",
      "Epoch: 39/100 | Loss: 0.6502\n",
      "Epoch: 40/100 | Loss: 0.6497\n",
      "Epoch: 41/100 | Loss: 0.6492\n",
      "Epoch: 42/100 | Loss: 0.6488\n",
      "Epoch: 43/100 | Loss: 0.6485\n",
      "Epoch: 44/100 | Loss: 0.6481\n",
      "Epoch: 45/100 | Loss: 0.6478\n",
      "Epoch: 46/100 | Loss: 0.6476\n",
      "Epoch: 47/100 | Loss: 0.6473\n",
      "Epoch: 48/100 | Loss: 0.6471\n",
      "Epoch: 49/100 | Loss: 0.6469\n",
      "Epoch: 50/100 | Loss: 0.6468\n",
      "Epoch: 51/100 | Loss: 0.6466\n",
      "Epoch: 52/100 | Loss: 0.6465\n",
      "Epoch: 53/100 | Loss: 0.6463\n",
      "Epoch: 54/100 | Loss: 0.6462\n",
      "Epoch: 55/100 | Loss: 0.6461\n",
      "Epoch: 56/100 | Loss: 0.6460\n",
      "Epoch: 57/100 | Loss: 0.6460\n",
      "Epoch: 58/100 | Loss: 0.6459\n",
      "Epoch: 59/100 | Loss: 0.6458\n",
      "Epoch: 60/100 | Loss: 0.6458\n",
      "Epoch: 61/100 | Loss: 0.6457\n",
      "Epoch: 62/100 | Loss: 0.6457\n",
      "Epoch: 63/100 | Loss: 0.6456\n",
      "Epoch: 64/100 | Loss: 0.6456\n",
      "Epoch: 65/100 | Loss: 0.6456\n",
      "Epoch: 66/100 | Loss: 0.6455\n",
      "Epoch: 67/100 | Loss: 0.6455\n",
      "Epoch: 68/100 | Loss: 0.6455\n",
      "Epoch: 69/100 | Loss: 0.6455\n",
      "Epoch: 70/100 | Loss: 0.6454\n",
      "Epoch: 71/100 | Loss: 0.6454\n",
      "Epoch: 72/100 | Loss: 0.6454\n",
      "Epoch: 73/100 | Loss: 0.6454\n",
      "Epoch: 74/100 | Loss: 0.6454\n",
      "Epoch: 75/100 | Loss: 0.6454\n",
      "Epoch: 76/100 | Loss: 0.6454\n",
      "Epoch: 77/100 | Loss: 0.6453\n",
      "Epoch: 78/100 | Loss: 0.6453\n",
      "Epoch: 79/100 | Loss: 0.6453\n",
      "Epoch: 80/100 | Loss: 0.6453\n",
      "Epoch: 81/100 | Loss: 0.6453\n",
      "Epoch: 82/100 | Loss: 0.6453\n",
      "Epoch: 83/100 | Loss: 0.6453\n",
      "Epoch: 84/100 | Loss: 0.6453\n",
      "Epoch: 85/100 | Loss: 0.6453\n",
      "Epoch: 86/100 | Loss: 0.6453\n",
      "Epoch: 87/100 | Loss: 0.6453\n",
      "Epoch: 88/100 | Loss: 0.6453\n",
      "Epoch: 89/100 | Loss: 0.6453\n",
      "Epoch: 90/100 | Loss: 0.6453\n",
      "Epoch: 91/100 | Loss: 0.6453\n",
      "Epoch: 92/100 | Loss: 0.6453\n",
      "Epoch: 93/100 | Loss: 0.6453\n",
      "Epoch: 94/100 | Loss: 0.6453\n",
      "Epoch: 95/100 | Loss: 0.6453\n",
      "Epoch: 96/100 | Loss: 0.6453\n",
      "Epoch: 97/100 | Loss: 0.6453\n",
      "Epoch: 98/100 | Loss: 0.6453\n",
      "Epoch: 99/100 | Loss: 0.6453\n",
      "Epoch: 100/100 | Loss: 0.6453\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim, from_numpy\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = from_numpy(xy[:, 0:-1])\n",
    "y_data = from_numpy(xy[:, [-1]])\n",
    "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Linear(8, 6)\n",
    "        self.l2 = nn.Linear(6, 4)\n",
    "        self.l3 = nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = nn.BCELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(f'Epoch: {epoch + 1}/100 | Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5927aa1-924b-42a5-bb7a-a621a78307e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 33260, 9708) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\facultate\\an3\\Sisteme Inteligente\\video1\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1283\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:114\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     27\u001b[39m train_loader = DataLoader(dataset=dataset,\n\u001b[32m     28\u001b[39m                           batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m     29\u001b[39m                           shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     30\u001b[39m                           num_workers=\u001b[32m2\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# get the inputs\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# wrap them in Variable\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\facultate\\an3\\Sisteme Inteligente\\video1\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\facultate\\an3\\Sisteme Inteligente\\video1\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\facultate\\an3\\Sisteme Inteligente\\video1\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\facultate\\an3\\Sisteme Inteligente\\video1\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1297\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1296\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1298\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1299\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 33260, 9708) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import from_numpy, tensor\n",
    "import numpy as np\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz',\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = tensor(inputs), tensor(labels)\n",
    "\n",
    "        # Run your training process\n",
    "        print(f'Epoch: {i} | Inputs {inputs.data} | Labels {labels.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7e7ba-007f-4412-8449-6d042ab86c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, from_numpy, optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz',\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Linear(8, 6)\n",
    "        self.l2 = nn.Linear(6, 4)\n",
    "        self.l3 = nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(f'Epoch {epoch + 1} | Batch: {i+1} | Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a9e6bc-2610-4e6f-9723-83dc04584488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, tensor, max\n",
    "import numpy as np\n",
    "\n",
    "# Cross entropy example\n",
    "# One hot\n",
    "# 0: 1 0 0\n",
    "# 1: 0 1 0\n",
    "# 2: 0 0 1\n",
    "Y = np.array([1, 0, 0])\n",
    "Y_pred1 = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.6])\n",
    "print(f'Loss1: {np.sum(-Y * np.log(Y_pred1)):.4f}')\n",
    "print(f'Loss2: {np.sum(-Y * np.log(Y_pred2)):.4f}')\n",
    "\n",
    "# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = tensor([0], requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 1 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = tensor([[2.0, 1.0, 0.1]])\n",
    "Y_pred2 = tensor([[0.5, 2.0, 0.3]])\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(f'PyTorch Loss1: {l1.item():.4f} \\nPyTorch Loss2: {l2.item():.4f}')\n",
    "print(f'Y_pred1: {max(Y_pred1.data, 1)[1].item()}')\n",
    "print(f'Y_pred2: {max(Y_pred2.data, 1)[1].item()}')\n",
    "\n",
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = tensor([2, 0, 1], requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 2 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = tensor([[0.1, 0.2, 0.9],\n",
    "                  [1.1, 0.1, 0.2],\n",
    "                  [0.2, 2.1, 0.1]])\n",
    "\n",
    "Y_pred2 = tensor([[0.8, 0.2, 0.3],\n",
    "                  [0.2, 0.3, 0.5],\n",
    "                  [0.2, 0.2, 0.5]])\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "print(f'Batch Loss1:  {l1.item():.4f} \\nBatch Loss2: {l2.data:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda094f9-156a-4a3c-922d-a80c04d54c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "from torch import nn, optim, cuda\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).item()\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1, 10):\n",
    "        epoch_start = time.time()\n",
    "        train(epoch)\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "        test()\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "\n",
    "    m, s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "434c66f4-0a83-4993-9865-a847a0c33499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298526\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.272545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miron\\AppData\\Local\\Temp\\ipykernel_16660\\3638697702.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.266974\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.194002\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.189214\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.036589\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.952286\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.797332\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.690192\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.385042\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.059539\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.703528\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.793500\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.635425\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.562065\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.671729\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.584850\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.675992\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.369513\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.300239\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.446272\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.426424\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.385947\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.614246\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.330944\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.460462\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.334917\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.341812\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.221291\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.475698\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.412427\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.322940\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.231258\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.302331\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.282923\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.325752\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.337325\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.458247\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.342032\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.223301\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.200523\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.343103\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.376520\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.315952\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.481503\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.404942\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.282255\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.139020\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.192269\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.183595\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.202123\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.256729\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.250500\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.276470\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.329274\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.153834\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.174447\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.276236\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.220339\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.295356\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.179823\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.260543\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.182626\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.264150\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.211343\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.327972\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.285948\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.158839\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.165131\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.116191\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.159959\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.182085\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.115073\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.132900\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.151089\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.154757\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.089173\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.261658\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.145250\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.183284\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.232987\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.160901\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.249002\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.121937\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.279420\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.238814\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.108091\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.134372\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.285213\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.255437\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.256344\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.297938\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.239776\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.177565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miron\\AppData\\Local\\Temp\\ipykernel_16660\\3638697702.py:76: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1603, Accuracy: 9547/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.136099\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.282304\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.228663\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.077800\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.144130\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.263158\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.246835\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.377591\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.173274\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.133034\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.053543\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.114869\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.060672\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.150046\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.055409\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.171510\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.132655\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.243819\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.428216\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.081514\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.288344\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.190475\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.120486\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.204937\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.193025\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.171497\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.124780\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.181738\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.154419\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.095493\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.309184\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.315026\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.217134\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.176275\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.143056\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.125094\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.094960\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.148643\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.106660\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.078737\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.148539\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.157995\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.079225\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.168352\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.068134\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.303374\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.163381\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.086199\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.040554\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.275892\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.069640\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.240767\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.172588\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.221413\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.074537\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.058631\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.053962\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.187794\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.076033\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.088901\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.049122\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.179001\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.131027\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.282764\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.129098\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.049000\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.135495\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.077229\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.050980\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.121978\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.123664\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.098676\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.209594\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.071000\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.181205\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.120863\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.088504\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.107690\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.204904\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.157574\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.181950\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.020725\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.080374\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.432461\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.101714\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.196496\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.245052\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.090781\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.177504\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.034168\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.141958\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.044961\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.079428\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.153710\n",
      "\n",
      "Test set: Average loss: 0.1103, Accuracy: 9678/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.094089\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.038076\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.057534\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.077643\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.205166\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.133022\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.055730\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.075432\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.074020\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.124494\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.034032\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.287534\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.109896\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.131036\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.155482\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.044311\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.137354\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.082151\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.316566\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.028014\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.170801\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.047577\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.090016\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.096968\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.045876\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.260932\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.163008\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.146232\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.115450\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.238267\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.115160\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.035552\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.056731\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.094332\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.077482\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.130360\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.031081\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.180342\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.173332\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.037724\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.145936\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.191341\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.116627\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.130460\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.033150\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.172672\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.115732\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.053069\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.169235\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.101622\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.082926\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.050838\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.081315\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.155485\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.098749\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.052134\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.203419\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.183238\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.038287\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.125401\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.275338\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.219525\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.139691\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.094165\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.058870\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.053434\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.065426\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.040227\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.057632\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.045808\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.157757\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.018299\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.104365\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.146790\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.073902\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.167908\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.129415\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.029607\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.023017\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.028854\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.050111\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.067061\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.124685\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.066770\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.073138\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.076654\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.042141\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.122130\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.124039\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.119428\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.176750\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.094734\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.083290\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.033285\n",
      "\n",
      "Test set: Average loss: 0.0896, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.026617\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.115712\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.109565\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.100896\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.065202\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.079368\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.168686\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.068251\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.095400\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.114791\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.112684\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.144658\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.048637\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.123574\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.081204\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.029109\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.045197\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.176161\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.033628\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.081658\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.037962\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.129415\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.080839\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.241509\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.128287\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.177099\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.019558\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.043737\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.041046\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.204097\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.288374\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.038894\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.108007\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.159089\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.074478\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.059478\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.039516\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.065437\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.257690\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.099517\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.019975\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.079442\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.042607\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.128144\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.158776\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.185279\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.163379\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.099533\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.066728\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.048186\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.031163\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.090792\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.117117\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.050337\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.024791\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.319749\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.161480\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.123074\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.063392\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.124711\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.146180\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.046771\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.127515\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.105709\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.114510\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.098330\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.095554\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.072839\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.056522\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.024883\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.107194\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.052527\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.028062\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.086953\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.043771\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.047736\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.028512\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.052069\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.028317\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.028797\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.136963\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.217236\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.303097\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.054548\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.093759\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.080706\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.178850\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.070802\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.087650\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.046724\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.064488\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.029938\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.085765\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.231351\n",
      "\n",
      "Test set: Average loss: 0.0747, Accuracy: 9772/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.034413\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.064634\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.030709\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.054002\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.060911\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.121141\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.038408\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.077454\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.023178\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.072118\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.031818\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.065632\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.046759\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.091878\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.072017\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.044906\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.256064\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.034752\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.051034\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.066641\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.009852\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.020673\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.101986\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.047648\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.084085\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.021949\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.066768\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.109936\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.202169\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.062634\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.116638\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.036296\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.105059\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.112890\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.091160\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.070528\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.411969\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.110879\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.227342\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.061637\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.034781\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.054879\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.060285\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.017240\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.013566\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.024422\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.080344\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.098296\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.049430\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.123238\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.132283\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.215180\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.077916\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.021058\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.075613\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.011438\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.105395\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.043023\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.141192\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.059179\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.096876\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.064641\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.113887\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.034333\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.207077\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.041396\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.080182\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.094307\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.437436\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.041270\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.051921\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.079299\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.027171\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.059586\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.132453\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.057338\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.067937\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.153318\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.030636\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.043910\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.151814\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.080941\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.029090\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.124780\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.103162\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.054900\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.028925\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.063469\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.062175\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.158580\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.124223\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.158794\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.014330\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.230314\n",
      "\n",
      "Test set: Average loss: 0.0929, Accuracy: 9693/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.109545\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.028984\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.103957\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.072866\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.102412\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.198016\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.055487\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.069400\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.111315\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.059552\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.016816\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.012001\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.024371\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.036212\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.125695\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.194968\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.086085\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.018251\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.053735\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.072369\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.150962\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.040261\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.064214\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.033093\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.064839\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.087019\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.018459\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.108508\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.020273\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.018988\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.107578\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.016287\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.061271\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.031618\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.037156\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.101425\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.146094\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.158850\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.029486\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.039836\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.027622\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.027485\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.073807\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.121040\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.083220\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.069023\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.013726\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.051439\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.012677\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.036552\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.015556\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.149704\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.108773\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.062651\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.072419\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.204319\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.138720\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.027460\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.050459\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.025210\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.019145\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.088061\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.079501\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.074397\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.033083\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.243569\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.019145\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.129004\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.025120\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.022874\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.166378\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.136340\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.073772\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.038784\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.053592\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.031349\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.046314\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.022153\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.060186\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.039686\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.062587\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.009692\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.161607\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.073918\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.039733\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.090901\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.070129\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.113903\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.053634\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.089226\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.013322\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.018193\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.098604\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.037097\n",
      "\n",
      "Test set: Average loss: 0.0580, Accuracy: 9817/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.063196\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.053656\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.096960\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.100668\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.050108\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.174399\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.059468\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.030568\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.083215\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.111075\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.045117\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.029349\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.004253\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.019455\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.056331\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.215965\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.168358\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.038232\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.235858\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.113770\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.176438\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.090168\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.024978\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.080127\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.064756\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.079569\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.018362\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.054088\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.027108\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.115693\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.031167\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.097107\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.040643\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.093555\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.021482\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.098591\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.081922\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.014124\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.159845\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.024339\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.038932\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.066883\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.026785\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.062783\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.024968\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.019118\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.014040\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.094118\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.019102\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.099945\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.031956\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.010197\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.046916\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.129939\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.049356\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.019260\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.034648\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.022855\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.033961\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.055447\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.063418\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.006714\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.013832\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.015468\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.038139\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.123673\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.029015\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.023442\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.120672\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.027578\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.046380\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.010295\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.090773\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.048950\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.037444\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.020737\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.061528\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.010727\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.027888\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.072012\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.016211\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.034298\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.043658\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.024727\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.078873\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.224157\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.119675\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.141179\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.111697\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.077304\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.026520\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.065401\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.031285\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.089575\n",
      "\n",
      "Test set: Average loss: 0.0585, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.160474\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.043273\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.056172\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.075828\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.097453\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.061814\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.050578\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.143737\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.086689\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.030793\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.111398\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.020050\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.060475\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.012274\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.039230\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.029128\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.045136\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.031342\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.028525\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.010615\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.069905\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.110019\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.059143\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.028403\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.149449\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.018695\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.075905\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.016249\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.098937\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.061406\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.044231\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.016200\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.221778\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.021952\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.095786\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.199702\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.019643\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.026656\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.028240\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.053926\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.023199\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.044877\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.092858\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.132224\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.019754\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.085362\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.029821\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.041953\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.033288\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.327856\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.104684\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.041020\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.010332\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.184810\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.014229\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.028386\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.044232\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.074178\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.020063\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.048892\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.051471\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.017628\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.103527\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.055763\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.121126\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.123104\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.016013\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.009886\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.032564\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.127186\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.008095\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.008990\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.073716\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.032339\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.031607\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.076983\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.050707\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.022645\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.007056\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.009667\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.151559\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.028734\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.088285\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.076045\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.121048\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.054784\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.029499\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.009050\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.083175\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.024565\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.030595\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.041090\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.092306\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.080835\n",
      "\n",
      "Test set: Average loss: 0.0517, Accuracy: 9843/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.028125\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.070048\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.018050\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.021712\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.018317\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.017387\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.036690\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.076590\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.139172\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.016842\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.075748\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.030224\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.078604\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.078396\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.055665\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.033433\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.134800\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.050402\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.019113\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.148445\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.055652\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.023914\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.061119\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.114589\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.042175\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.029039\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.031985\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.053811\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.017599\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.064514\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.134166\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.119241\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.079116\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.048001\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.151994\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.019145\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.016581\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.076601\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.072376\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.019208\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.056311\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.017895\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.055479\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.079492\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.022420\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.044622\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.042610\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.122795\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.006594\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.021709\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.034018\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.211298\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.015376\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.139187\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.133406\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.020236\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.190393\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.044731\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.055944\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.041805\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.043417\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.034823\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.101211\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.160937\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.196554\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.040340\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.042663\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.135600\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.069532\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.009171\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.018360\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.013140\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.022962\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.072146\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.030811\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.076653\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.003620\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.011518\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.099381\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.026508\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.052740\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.021810\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.033580\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.074558\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.029851\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.084435\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.034990\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.093892\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.130940\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.097840\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.040470\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.168843\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.087383\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.053449\n",
      "\n",
      "Test set: Average loss: 0.0529, Accuracy: 9825/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a147553d-dd82-4079-8c46-eb43a18ea052",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTest set: Average loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[33m, Accuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m    125\u001b[39m         test_loss, correct, \u001b[38;5;28mlen\u001b[39m(test_loader.dataset),\n\u001b[32m    126\u001b[39m         \u001b[32m100.\u001b[39m * correct / \u001b[38;5;28mlen\u001b[39m(test_loader.dataset)))\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     test()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(epoch)\u001b[39m\n\u001b[32m    103\u001b[39m optimizer.step()\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_idx % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTrain Epoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m)]\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mLoss: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m    106\u001b[39m         epoch, batch_idx * \u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;28mlen\u001b[39m(train_loader.dataset),\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m         \u001b[32m100.\u001b[39m * batch_idx / \u001b[38;5;28mlen\u001b[39m(train_loader), \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n",
      "\u001b[31mIndexError\u001b[39m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
    "\n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05afb1e-9dc0-4b76-a945-47cc52e08a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# One hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "\n",
    "# (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n",
    "hidden = Variable(torch.randn(1, 1, 2))\n",
    "\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = Variable(torch.Tensor([h, e, l, l, o]))\n",
    "for one in inputs:\n",
    "    one = one.view(1, 1, -1)\n",
    "    # Input: (batch, seq_len, input_size) when batch_first=True\n",
    "    out, hidden = cell(one, hidden)\n",
    "    print(\"one input size\", one.size(), \"out size\", out.size())\n",
    "\n",
    "# We can do the whole at once\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = inputs.view(1, 5, -1)\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"sequence input size\", inputs.size(), \"out size\", out.size())\n",
    "\n",
    "\n",
    "# hidden : (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n",
    "hidden = Variable(torch.randn(1, 3, 2))\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "# 3 batches 'hello', 'eolll', 'lleel'\n",
    "# rank = (3, 5, 4)\n",
    "inputs = Variable(torch.Tensor([[h, e, l, l, o],\n",
    "                                [e, o, l, l, l],\n",
    "                                [l, l, e, e, l]]))\n",
    "\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "# B x S x I\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"batch input size\", inputs.size(), \"out size\", out.size())\n",
    "\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2)\n",
    "cell = nn.RNN(input_size=4, hidden_size=2)\n",
    "\n",
    "# The given dimensions dim0 and dim1 are swapped.\n",
    "inputs = inputs.transpose(dim0=0, dim1=1)\n",
    "# Propagate input through RNN\n",
    "# Input: (seq_len, batch_size, input_size) when batch_first=False (default)\n",
    "# S x B x I\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"batch input size\", inputs.size(), \"out size\", out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6bdf0-1629-4b43-96c7-48fff291aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 12 RNN\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "#            0    1    2    3    4\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [0, 1, 0, 2, 3, 3]   # hihell\n",
    "one_hot_lookup = [[1, 0, 0, 0, 0],  # 0\n",
    "                  [0, 1, 0, 0, 0],  # 1\n",
    "                  [0, 0, 1, 0, 0],  # 2\n",
    "                  [0, 0, 0, 1, 0],  # 3\n",
    "                  [0, 0, 0, 0, 1]]  # 4\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the RNN. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 1  # One by one\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                          hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, hidden, x):\n",
    "        # Reshape input (batch first)\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # hidden: (num_layers * num_directions, batch, hidden_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        return hidden, out.view(-1, num_classes)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "\n",
    "\n",
    "# Instantiate RNN model\n",
    "model = Model()\n",
    "print(model)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    sys.stdout.write(\"predicted string: \")\n",
    "    for input, label in zip(inputs, labels):\n",
    "        # print(input.size(), label.size())\n",
    "        hidden, output = model(hidden, input)\n",
    "        val, idx = output.max(1)\n",
    "        sys.stdout.write(idx2char[idx.data[0]])\n",
    "        loss += criterion(output, torch.LongTensor([label]))\n",
    "\n",
    "    print(\", epoch: %d, loss: %1.3f\" % (epoch + 1, loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e619b3f-3a06-4ab2-b265-5701c8948856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 12 RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=5, hidden_size=5, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size) for batch_first=True\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "\n",
    "        # Reshape input\n",
    "        x.view(x.size(0), self.sequence_length, self.input_size)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "        out, _ = self.rnn(x, h_0)\n",
    "        return out.view(-1, num_classes)\n",
    "\n",
    "\n",
    "# Instantiate RNN model\n",
    "rnn = RNN(num_classes, input_size, hidden_size, num_layers)\n",
    "print(rnn)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    outputs = rnn(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd241855-189f-4692-a99a-e6141583b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 12 RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.LongTensor(x_data))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5\n",
    "embedding_size = 10  # embedding size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.RNN(input_size=embedding_size,\n",
    "                          hidden_size=5, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(batch_size, sequence_length, -1)\n",
    "\n",
    "        # Propagate embedding through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "        out, _ = self.rnn(emb, h_0)\n",
    "        return self.fc(out.view(-1, num_classes))\n",
    "\n",
    "\n",
    "# Instantiate RNN model\n",
    "model = Model(num_layers, hidden_size)\n",
    "print(model)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    outputs = model(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f682ae-69bb-4ae8-8697-034144c35da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code is from https://github.com/spro/practical-pytorch\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from name_dataset import NameDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Parameters and DataLoaders\n",
    "HIDDEN_SIZE = 100\n",
    "N_CHARS = 128  # ASCII\n",
    "N_CLASSES = 18\n",
    "\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "\n",
    "        # input = B x S . size(0) = B\n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        # input:  B x S  -- (transpose) --> S x B\n",
    "        input = input.t()\n",
    "\n",
    "        # Embedding S x B -> S x B x I (embedding size)\n",
    "        print(\"  input\", input.size())\n",
    "        embedded = self.embedding(input)\n",
    "        print(\"  embedding\", embedded.size())\n",
    "\n",
    "        # Make a hidden\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        print(\"  gru hidden output\", hidden.size())\n",
    "        # Use the last layer output as FC's input\n",
    "        # No need to unpack, since we are going to use hidden\n",
    "        fc_output = self.fc(hidden)\n",
    "        print(\"  fc output\", fc_output.size())\n",
    "        return fc_output\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return Variable(hidden)\n",
    "\n",
    "# Help functions\n",
    "\n",
    "\n",
    "def str2ascii_arr(msg):\n",
    "    arr = [ord(c) for c in msg]\n",
    "    return arr, len(arr)\n",
    "\n",
    "# pad sequences and sort the tensor\n",
    "def pad_sequences(vectorized_seqs, seq_lengths):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    return seq_tensor\n",
    "\n",
    "# Create necessary variables, lengths, and target\n",
    "def make_variables(names):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    names = ['adylov', 'solan', 'hard', 'san']\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_CLASSES)\n",
    "\n",
    "    for name in names:\n",
    "        arr, _ = str2ascii_arr(name)\n",
    "        inp = Variable(torch.LongTensor([arr]))\n",
    "        out = classifier(inp)\n",
    "        print(\"in\", inp.size(), \"out\", out.size())\n",
    "\n",
    "\n",
    "    inputs = make_variables(names)\n",
    "    out = classifier(inputs)\n",
    "    print(\"batch in\", inputs.size(), \"batch out\", out.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c057de7-3cad-4f52-8de4-864ae2a51d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code is from https://github.com/spro/practical-pytorch\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from name_dataset import NameDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Parameters and DataLoaders\n",
    "HIDDEN_SIZE = 100\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100\n",
    "\n",
    "test_dataset = NameDataset(is_train_set=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset = NameDataset(is_train_set=True)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "N_COUNTRIES = len(train_dataset.get_countries())\n",
    "print(N_COUNTRIES, \"countries\")\n",
    "N_CHARS = 128  # ASCII\n",
    "\n",
    "\n",
    "# Some utility functions\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def create_variable(tensor):\n",
    "    # Do cuda() before wrapping with variable\n",
    "    if torch.cuda.is_available():\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "\n",
    "# pad sequences and sort the tensor\n",
    "def pad_sequences(vectorized_seqs, seq_lengths, countries):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "\n",
    "    # Sort tensors by their length\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "    # Also sort the target (countries) in the same order\n",
    "    target = countries2tensor(countries)\n",
    "    if len(countries):\n",
    "        target = target[perm_idx]\n",
    "\n",
    "    # Return variables\n",
    "    # DataParallel requires everything to be a Variable\n",
    "    return create_variable(seq_tensor), \\\n",
    "        create_variable(seq_lengths), \\\n",
    "        create_variable(target)\n",
    "\n",
    "\n",
    "# Create necessary variables, lengths, and target\n",
    "def make_variables(names, countries):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths, countries)\n",
    "\n",
    "\n",
    "def str2ascii_arr(msg):\n",
    "    arr = [ord(c) for c in msg]\n",
    "    return arr, len(arr)\n",
    "\n",
    "\n",
    "def countries2tensor(countries):\n",
    "    country_ids = [train_dataset.get_country_id(\n",
    "        country) for country in countries]\n",
    "    return torch.LongTensor(country_ids)\n",
    "\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    # Our model\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = int(bidirectional) + 1\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        # input shape: B x S (input size)\n",
    "        # transpose to make S(sequence) x B (batch)\n",
    "        input = input.t()\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        # Make a hidden\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "\n",
    "        # Embedding S x B -> S x B x I (embedding size)\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        # Pack them up nicely\n",
    "        gru_input = pack_padded_sequence(\n",
    "            embedded, seq_lengths.data.cpu().numpy())\n",
    "\n",
    "        # To compact weights again call flatten_parameters().\n",
    "        self.gru.flatten_parameters()\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "\n",
    "        # Use the last layer output as FC's input\n",
    "        # No need to unpack, since we are going to use hidden\n",
    "        fc_output = self.fc(hidden[-1])\n",
    "        return fc_output\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
    "                             batch_size, self.hidden_size)\n",
    "        return create_variable(hidden)\n",
    "\n",
    "\n",
    "# Train cycle\n",
    "def train():\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (names, countries) in enumerate(train_loader, 1):\n",
    "        input, seq_lengths, target = make_variables(names, countries)\n",
    "        output = classifier(input, seq_lengths)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "        classifier.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n",
    "                time_since(start), epoch,  i *\n",
    "                len(names), len(train_loader.dataset),\n",
    "                100. * i * len(names) / len(train_loader.dataset),\n",
    "                total_loss / i * len(names)))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Testing cycle\n",
    "def test(name=None):\n",
    "    # Predict for a given name\n",
    "    if name:\n",
    "        input, seq_lengths, target = make_variables([name], [])\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        country_id = pred.cpu().numpy()[0][0]\n",
    "        print(name, \"is\", train_dataset.get_country(country_id))\n",
    "        return\n",
    "\n",
    "    print(\"evaluating trained model ...\")\n",
    "    correct = 0\n",
    "    train_data_size = len(test_loader.dataset)\n",
    "\n",
    "    for names, countries in test_loader:\n",
    "        input, seq_lengths, target = make_variables(names, countries)\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, train_data_size, 100. * correct / train_data_size))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        # dim = 0 [33, xxx] -> [11, ...], [11, ...], [11, ...] on 3 GPUs\n",
    "        classifier = nn.DataParallel(classifier)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        classifier.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        # Train cycle\n",
    "        train()\n",
    "\n",
    "        # Testing\n",
    "        test()\n",
    "\n",
    "        # Testing several samples\n",
    "        test(\"Sung\")\n",
    "        test(\"Jungwoo\")\n",
    "        test(\"Soojin\")\n",
    "        test(\"Nako\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a4f48-1061-4d90-a523-0cb015cba0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9aad08-07ab-4b98-a547-bdaf9c3f5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/spro/practical-pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from text_loader import TextDataset\n",
    "\n",
    "hidden_size = 100\n",
    "n_layers = 3\n",
    "batch_size = 1\n",
    "n_epochs = 100\n",
    "n_characters = 128  # ASCII\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # This runs this one step at a time\n",
    "    # It's extremely slow, and please do not use in practice.\n",
    "    # We need to use (1) batch and (2) data parallelism\n",
    "    def forward(self, input, hidden):\n",
    "        embed = self.embedding(input.view(1, -1))  # S(=1) x I\n",
    "        embed = embed.view(1, 1, -1)  # S(=1) x B(=1) x I (embedding size)\n",
    "        output, hidden = self.gru(embed, hidden)\n",
    "        output = self.linear(output.view(1, -1))  # S(=1) x I\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if torch.cuda.is_available():\n",
    "            hidden = torch.zeros(self.n_layers, 1, self.hidden_size).cuda()\n",
    "        else:\n",
    "            hidden = torch.zeros(self.n_layers, 1, self.hidden_size)\n",
    "\n",
    "        return Variable(hidden)\n",
    "\n",
    "\n",
    "def str2tensor(string):\n",
    "    tensor = [ord(c) for c in string]\n",
    "    tensor = torch.LongTensor(tensor)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.cuda()\n",
    "\n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = str2tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "\n",
    "    inp = prime_input[-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = chr(top_i)\n",
    "        predicted += predicted_char\n",
    "        inp = str2tensor(predicted_char)\n",
    "\n",
    "    return predicted\n",
    "\n",
    "# Train for a given src and target\n",
    "# It feeds single string to demonstrate seq2seq\n",
    "# It's extremely slow, and we need to use (1) batch and (2) data parallelism\n",
    "# http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.\n",
    "\n",
    "\n",
    "def train_teacher_forching(line):\n",
    "    input = str2tensor(line[:-1])\n",
    "    target = str2tensor(line[1:])\n",
    "\n",
    "    hidden = decoder.init_hidden()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(len(input)):\n",
    "        output, hidden = decoder(input[c], hidden)\n",
    "        loss += criterion(output, target[c])\n",
    "\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / len(input)\n",
    "\n",
    "\n",
    "def train(line):\n",
    "    input = str2tensor(line[:-1])\n",
    "    target = str2tensor(line[1:])\n",
    "\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder_in = input[0]\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(len(input)):\n",
    "        output, hidden = decoder(decoder_in, hidden)\n",
    "        loss += criterion(output, target[c])\n",
    "        decoder_in = output.max(1)[1]\n",
    "\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / len(input)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "    if torch.cuda.is_available():\n",
    "        decoder.cuda()\n",
    "\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(dataset=TextDataset(),\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "\n",
    "    print(\"Training for %d epochs...\" % n_epochs)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for i, (lines, _) in enumerate(train_loader):\n",
    "            loss = train(lines[0])  # Batch size is 1\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('[(%d %d%%) loss: %.4f]' %\n",
    "                      (epoch, epoch / n_epochs * 100, loss))\n",
    "                print(generate(decoder, 'Wh', 100), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a9b38-0e2f-425f-9188-83dc59b7d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original source from\n",
    "# https://gist.github.com/Tushar-N/dfca335e370a2bc3bc79876e6270099e\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "seqs = ['ghatmasala', 'nicela', 'chutpakodas']\n",
    "\n",
    "# make <pad> idx 0\n",
    "vocab = ['<pad>'] + sorted(list(set(flatten(seqs))))\n",
    "\n",
    "# make model\n",
    "embedding_size = 3\n",
    "embed = nn.Embedding(len(vocab), embedding_size)\n",
    "lstm = nn.LSTM(embedding_size, 5)\n",
    "\n",
    "vectorized_seqs = [[vocab.index(tok) for tok in seq]for seq in seqs]\n",
    "print(\"vectorized_seqs\", vectorized_seqs)\n",
    "\n",
    "print([x for x in map(len, vectorized_seqs)])\n",
    "# get the length of each seq in your batch\n",
    "seq_lengths = torch.LongTensor([x for x in map(len, vectorized_seqs)])\n",
    "\n",
    "# dump padding everywhere, and place seqs on the left.\n",
    "# NOTE: you only need a tensor as big as your longest sequence\n",
    "seq_tensor = Variable(torch.zeros(\n",
    "    (len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "print(\"seq_tensor\", seq_tensor)\n",
    "\n",
    "# SORT YOUR TENSORS BY LENGTH!\n",
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "print(\"seq_tensor after sorting\", seq_tensor)\n",
    "\n",
    "# utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n",
    "# Otherwise, give (L,B,D) tensors\n",
    "seq_tensor = seq_tensor.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n",
    "print(\"seq_tensor after transposing\", seq_tensor.size(), seq_tensor.data)\n",
    "\n",
    "# embed your sequences\n",
    "embeded_seq_tensor = embed(seq_tensor)\n",
    "print(\"seq_tensor after embeding\", embeded_seq_tensor.size(), seq_tensor.data)\n",
    "\n",
    "# pack them up nicely\n",
    "packed_input = pack_padded_sequence(\n",
    "    embeded_seq_tensor, seq_lengths.cpu().numpy())\n",
    "\n",
    "# throw them through your LSTM (remember to give batch_first=True here if\n",
    "# you packed with it)\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "\n",
    "# unpack your output if required\n",
    "output, _ = pad_packed_sequence(packed_output)\n",
    "print(\"Lstm output\", output.size(), output.data)\n",
    "\n",
    "# Or if you just want the final hidden state?\n",
    "print(\"Last output\", ht[-1].size(), ht[-1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e80cc-cdf1-4d4b-b797-82e18e31f676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6afd0e-b480-43e0-895c-ce62bd14dea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f9ac1-5ea5-438e-aa49-c221d2f9d381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
